
<title>niplav</title>
<link rel="shortcut icon" type="image/png" href="./favicon.png">
<link rel="stylesheet" type="text/css" href="main.css">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<!DOCTYPE HTML>

<style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
</style>

<script type="text/javascript" async
	src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	extensions: ["tex2jax.js"],
	jax: ["input/TeX", "output/HTML-CSS"],
	tex2jax: {
		inlineMath: [ ['$','$'], ["\\(","\\)"] ],
		displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
		processEscapes: true,
		skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
	},
	"HTML-CSS": { availableFonts: ["TeX"] }
	});
</script>

<script>
var anchorhash={}
function addAnchor(element) {
	var cnt=element.textContent
	if(cnt==="home")
		return;
	var ref=element.textContent.replace(/[^a-zA-Z0-9 ]/mg, "")
	ref=ref.replace(/ /mg, "-")
	var newref=ref;
	if(anchorhash[ref]===1)
		for(i=1, newref=ref+"_"+i;anchorhash[ref+"_"+i]===1;i++, newref=ref+"_"+i)
			;
	ref=newref
	element.setAttribute("id", `${ref}`)
	element.innerHTML=`<a href="#${ref}" class="hanchor">${cnt}</a>`
	anchorhash[ref]=1
}
document.addEventListener('DOMContentLoaded', function () {
	// Add anchor links to all headings
	var headers = document.querySelectorAll('h1, h2, h3, h4, h5, h6')
	if (headers) {
		headers.forEach(addAnchor)
	}
	// Change the title to the h1 header
	var title = document.querySelector('h1')
	if(title) {
		var title_elem = document.querySelector('title')
		title_elem.textContent=title.textContent + " – niplav"
	}
});
</script>

<h2><a href="./index.html">home</a></h2>

<p><em>author: niplav, created: 2021-01-21, modified: 2021-02-12, language: english, status: in progress, importance: 2, confidence: likely</em></p>

<blockquote>
  <p><strong><a href="https://en.wikipedia.org/wiki/Artificial_Intelligence:_A_Modern_Approach">“Artificial Intelligence: A Modern
  Approach”</a>,
  written by <a href="https://en.wikipedia.org/wiki/Stuart_J._Russell">Stuart
  Russell</a> and <a href="https://en.wikipedia.org/wiki/Peter_Norvig">Peter
  Norvig</a>, is probably the most
  well-known textbook on artificial intelligence. Here, I write down my
  solutions to exercises in that book. I use the 2010 edition, because the
  exercises for the 2020 edition were moved online.</strong></p>
</blockquote>

<h1>Solutions to “Artificial Intelligence: A Modern Approach”</h1>

<h2>Chapter 1</h2>

<h3>1.1</h3>

<blockquote>
  <p>Define in your own words: (a) intelligence, (b) artificial intelligence,
  (c) agent, (d) rationality, (e) logical reasoning</p>
</blockquote>

<h4>Intelligence</h4>

<p>The word “intelligence” is mostly used to describe a property of
systems. Roughly, it refers to the ability of a system to make decisions
that result in consequences are graded high according to some metric,
as opposed to decisions that result in consequences that are graded low
according to that metric.</p>

<h4>Artificial Intelligence</h4>

<p>“Artificial intelligence” refers to systems designed and implemented
by humans with the aim of these systems displaying intelligent behavior.</p>

<h4>Agent</h4>

<p>An “agent” is a part of the universe that carries out goal-directed
actions.</p>

<h4>Rationality</h4>

<p>The usage of the word “rationality” is difficult to untangle from
the usage of the word “intelligence”. For humans, “rationality”
usually refers to the ability to detect and correct cognitive errors
that hinder coming to correct conclusions about the state of the world
(epistemic rationality), as well as the ability to act on those beliefs
according to ones values (instrumental rationality). However, these
seem very related to “intelligence”, maybe only being separated by a
potentiality–intelligence being the potential, and rationality being
the ability to fulfill that potential. One could attempt to apply the
same definition to artificial intelligences, but it seems unclear how
a lawful process could be more intelligent, but is not.</p>

<h4>Logical Reasoning</h4>

<p>“Logical reasoning” refers to the act of deriving statements from
other statements according to pre-defined rules.</p>

<!--
### 1.2

> Read Turing’s original paper on AI (Turing, 1950). In the paper, he discusses several
objections to his proposed enterprise and his test for intelligence. Which objections still carry
weight? Are his refutations valid? Can you think of new objections arising from develop-
ments since he wrote the paper? In the paper, he predicts that, by the year 2000, a computer
will have a 30% chance of passing a five-minute Turing Test with an unskilled interrogator.
What chance do you think a computer would have today? In another 50 years?

TODO

-->

<h3>1.3</h3>

<p>A reflex action is not intelligent, as it is not the result of a
deliberate reasoning process. According to my personal definition above
(and also the definition given in the text), it is also not rational
(since the action is not guided by a belief).</p>

<p>Common usage of the term “rational” indicates that
people would describe this reflex as a rational action. I
believe this is fine, and words are just pointers to <a href="https://www.lesswrong.com/posts/jMTbQj9XB5ah2maup/similarity-clusters">clusters in
thing-space</a>
anyway.</p>

<h3>1.4</h3>

<blockquote>
  <p>Suppose we extend Evans’s ANALOGY program so that it can score 200
  on a standard IQ test. Would we then have a program more intelligent
  than a human? Explain.</p>
</blockquote>

<p>No. (At least not for any useful definition of intelligence). IQ
tests as they currently exist measure a proxy for the actual ability
to perform complex tasks in the real world. For humans, geometry
puzzles correlate (and predict) well with such tests (<a href="./doc/aima_solutions/the_predictive_value_of_iq_sternberg_2001.pdf" title="The Predictive Value of IQ">Sternberg et al.
2001</a>).</p>

<p>However, this proxy breaks down once we start optimising for it (as
in the case on extending ANALOGY). We can now not predict real-world
performance on arbitrary goals given the result of the IQ test performed
on ANALOGY anymore.</p>

<h3>1.5</h3>

<blockquote>
  <p>The neural structure of the sea slug Aplysia has been widely studied
  (first by Nobel Laureate Eric Kandel) because it has only about 20,000
  neurons, most of them large and easily manipulated. Assuming that the
  cycle time for an Aplysia neuron is roughly the same as for a human
  neuron, how does the computational power, in terms of memory updates
  per second, compare with the high-end computer described in Figure 1.3?</p>
</blockquote>

<!--TODO: un-fuck the dimensional analysis here-->

<p>Given the cycle time of <code>$10^{-3}$</code> seconds, we can expect</p>

<div>
    $$\frac{2*10^{4} \hbox{ neurons}}{10^{-3}\frac{\hbox{s}}{\hbox{update}}}=2*10^{7} \frac{\hbox{neuron updates}}{s}$$
</div>

<p>which is seven orders of magnitude lower than a supercomputer. Aplysia
won't be proving any important theorems soon.</p>

<!--
If Aplysia has 20k neurons, then it can be expected to have

`$2*10^{4}\hbox{ neurons }*\frac{10 \hbox{ to } 10^{5} \hbox{ synapses }}{\hbox{neuron}}=2*10^{5}\hbox{ to } 2*10^{9} \hbox{ neurons}$`
-->

<!--
TODO: do exercises 6 through 15



md5-f1e6fd471f676892bd9a20e0fa791b69





md5-012517055ad1c414d190500660956e47





md5-425a75dbb333f82f9cfe0706215607da





md5-9cbe60cd390f94f8a32372681bada0a1





md5-588ccaa22fe15e9734d0176f0e53e197





md5-e6fbd2961ae99d102c48fcfbd9fe030e





md5-63c3adf459550c4e3b0ff742104ffba3





md5-a10d77ddff0cdcbd8ffc98e31a4bac26





md5-331ff4ed64b484144d64669cfe9242e6





md5-a0a11a485a167f3743bd1975d9622c69





md5-c122b5fb8926c1006c6684ca7c5fe91f





md5-44e83f6f91c177c608d0febcae7dd41f





md5-d4b5f2a2e1a679d11553ba550422c69a





md5-3336515d80e7b8575dc1fd34f42c1c35





md5-2522be78cc8b93ea3cc8975ccdd7b1a4





md5-aa9963e9c41a3d3add0d548da0fff0ee





md5-87030a45eae8d8c798561d27a3017dfb





md5-e80651cd51d4a9095e1f45a6ba2c7431





md5-744861f6d70e47aa34ab225b59349679





md5-0dcf275281a5648cb840be593c35a8e6



-->

<h2>Chapter 2</h2>

<h3>2.1</h3>

<blockquote>
  <p>Suppose that the performance measure is concerned with just the first
  T time steps of the environment and ignores everything thereafter. Show
  that a rational agent’s action may depend not just on the state of
  the environment but also on the time step it has reached.</p>
</blockquote>

<p>Example: Let's say that we are in an environment with a button,
and pressing the button causes a light to go on in the next timestep.
The agent cares that the light is on (obtaining 1 util per timestep the
light is on for the first T timesteps).</p>

<p>However, pressing the button incurs a cost of ½ on the agent.</p>

<p>Then, at timestep T, the agent will not press the button, since it does
not care about the light being on at timestep T+1, and wants to avoid
the cost ½. At timesteps <code>$&lt;T$</code> it will press the button, with the light
currently being on, at timestep T it will not press the button, under
the same environmental conditions.</p>

<!--

### 2.2

> Let us examine the rationality of various vacuum-cleaner agent
functions.

> a. Show that the simple vacuum-cleaner agent function described in
Figure 2.3 is indeed rational under the assumptions listed on page 38.  
> b. Describe a rational agent function for the case in which each movement
costs one point. Does the corresponding agent program require internal
state?  
> c. Discuss possible agent designs for the cases in which clean squares
can become dirty and the geography of the environment is unknown. Does it
make sense for the agent to learn from its experience in these cases? If
so, what should it learn? If not, why not?

-->

<h3>2.3</h3>

<blockquote>
  <p>For each of the following assertions, say whether it is true or
  false and support your answer with examples or counterexamples where
  appropriate.</p>
  
  <p>a. An agent that senses only partial information about the state cannot
  be perfectly rational.  </p>
</blockquote>

<p>False. An agent that senses only partial information about the state could
infer missing information by making deductions (logical or statistical)
about the state of the environment, coming to full knowledge of the
environment, and making perfectly rational choices using that information.</p>

<p>For example, a chess-playing agent that can't see exactly one square
could infer the piece standing on that square by observing which piece
is missing from the rest of the board.</p>

<blockquote>
  <p>b. There exist task environments in which no pure reflex agent can
  behave rationally.  </p>
</blockquote>

<p>True. In an environment in which the next reward depends on the current
state and the previous state, a simple reflex agent will get outperformed
by agents with an internal world-model.</p>

<p>An example for this is a stock-trading agent: The future prices of stocks
doesn't just depend on the current prices, but on the history of prices.</p>

<blockquote>
  <p>c. There exists a task environment in which every agent is rational.  </p>
</blockquote>

<p>True. It is the environment where the agent has no options to act.</p>

<blockquote>
  <p>d. The input to an agent program is the same as the input to the
  agent function.  </p>
</blockquote>

<p>Not sure. Both the agent function and the agent program receive percepts,
but sometimes the agent program also needs information that is not a
percept (e.g. priors for bayesian agents). Is that counted as input,
or simply as program-specific data?</p>

<blockquote>
  <p>e. Every agent function is implementable by some program/machine
  combination.  </p>
</blockquote>

<p>False. An agent function could be uncomputable
(e. g. <a href="https://en.wikipedia.org/wiki/AIXI">AIXI</a>), and therefore not
be implementable on a real-world machine.</p>

<blockquote>
  <p>f. Suppose an agent selects its action uniformly at random from the
  set of possible actions. There exists a deterministic task environment
  in which this agent is rational.  </p>
</blockquote>

<p>True, that would be the environment in which every action scores equally
well on the performance measure.</p>

<blockquote>
  <p>g. It is possible for a given agent to be perfectly rational in two
  distinct task environments.  </p>
</blockquote>

<p>True. Given two agents <code>$A_X$</code> and <code>$A_Y$</code>, and two task environments
<code>$X$</code> (giving percepts from the set <code>$\{x_1, \dots, x_n\}$</code>) and <code>$Y$</code>
(giving percepts from the set <code>$\{y_1, \dots, y_n\}$</code>), with <code>$A_X$</code> being
perfectly rational in <code>$X$</code> and <code>$A_Y$</code> being perfectly rational in <code>$Y$</code>
an agent that is perfectly rational in two distinct task environments
could be implemented using the code:</p>

<pre><code>p=percept()
if p∈X
    A_X(p)
    while p=percept()
        A_X(p)
if p∈Y
    A_Y(p)
    while p=percept()
        A_Y(p)
</code></pre>

<blockquote>
  <p>h. Every agent is rational in an unobservable environment.  </p>
</blockquote>

<p>False. Given an unobservable environment in which moving results in
the performance measure going up (e.g. by knocking over ugly vases),
agents that move a lot are more rational than agents that do not move.</p>

<blockquote>
  <p>i. A perfectly rational poker-playing agent never loses.</p>
</blockquote>

<p>False. Given incomplete knowledge, a rational poker-playing agent can
only win in expectation.</p>

<!--

### 2.4

> For each of the following activities, give a PEAS description of the
task environment and characterize it in terms of the properties listed
in Section 2.3.2.

> * Playing soccer.
> * Exploring the subsurface oceans of Titan.
> * Shopping for used AI books on the Internet.
> * Playing a tennis match.
> * Practicing tennis against a wall.
> * Performing a high jump.
> * Knitting a sweater.
> * Bidding on an item at an auction.

### 2.5

> Define in your own words the following terms: agent, agent function,
agent program, rationality, autonomy, reflex agent, model-based agent,
goal-based agent, utility-based agent, learning agent.

### 2.6

> This exercise explores the differences between agent functions and
agent programs.

> a. Can there be more than one agent program that implements a given
agent function? Give an example, or show why one is not possible.  
> b. Are there agent functions that cannot be implemented by any agent
program?  
> c. Given a fixed machine architecture, does each agent program implement
exactly one agent function?  
> d. Given an architecture with n bits of storage, how many different
possible agent programs are there?  
> e. Suppose we keep the agent program fixed but speed up the machine
by a factor of two. Does that change the agent function?

### 2.7

> Write pseudocode agent programs for the goal-based and utility-based
agents. The following exercises all concern the implementation of
environments and agents for the vacuum-cleaner world.

### 2.8

> Implement a performance-measuring environment simulator for
the vacuum-cleaner world depicted in Figure 2.2 and specified on
page 38. Your implementation should be modular so that the sensors,
actuators, and environment characteristics (size, shape, dirt placement,
etc.) can be changed easily. (Note: for some choices of programming
language and operating system there are already implementations in the
online code repository.)

### 2.9

> Implement a simple reflex agent for the vacuum environment in Exercise
2.8. Run the environment with this agent for all possible initial dirt
configurations and agent locations. Record the performance score for
each configuration and the overall average score.

### 2.10

> Consider a modified version of the vacuum environment in Exercise 2.8,
in which the agent is penalized one point for each movement.

> a. Can a simple reflex agent be perfectly rational for this
environment? Explain.  
> b. What about a reflex agent with state? Design such an agent.  
> c. How do your answers to a and b change if the agent’s percepts
give it the clean/dirty status of every square in the environment?

### 2.11

> Consider a modified version of the vacuum environment in Exercise 2.8,
in which the geography of the environment—its extent, boundaries,
and obstacles—is unknown, as is the initial dirt configuration. (The
agent can go Up and Down as well as Left and Right.)

> a. Can a simple reflex agent be perfectly rational for this
environment? Explain.  
> b. Can a simple reflex agent with a randomized agent function outperform
a simple reflex agent? Design such an agent and measure its performance
on several environments.  
> c. Can you design an environment in which your randomized agent will
perform poorly? Show your results.  
> d. Can a reflex agent with state outperform a simple reflex
agent? Design such an agent and measure its performance on several
environments. Can you design a rational agent of this type?

### 2.12

> Repeat Exercise 2.11 for the case in which the location sensor
is replaced with a “bump” sensor that detects the agent’s
attempts to move into an obstacle or to cross the boundaries of the
environment. Suppose the bump sensor stops working; how should the
agent behave?

### 2.13

> The vacuum environments in the preceding exercises have all been
deterministic. Discuss possible agent programs for each of the following
stochastic versions:

> a. Murphy’s law: twenty-five percent of the time, the Suck action
fails to clean the floor if it is dirty and deposits dirt onto the floor
if the floor is clean. How is your agent program affected if the dirt
sensor gives the wrong answer 10% of the time?  
> b. Small children: At each time step, each clean square has a 10%
chance of becoming dirty. Can you come up with a rational agent design
for this case?  

-->

<h2>Chapter 13</h2>

<h3>13.1</h3>

<blockquote>
  <p>Show from first principile that <code>$P(a|b \land a) = 1$</code>.</p>
</blockquote>

<p>I'm not sure whether this counts as "from first principles", but</p>

<p><code>$P(a|b \land a)=\frac{P(a \land a \land b)}{P(a \land b)}=\frac{P(a \land b)}{P(a \land b)}=1$</code></p>

<p>is my solution.</p>

<h3>13.2</h3>

<blockquote>
  <p>Using the axioms of probability, prove that any probability distribution
  on a discrete random variable must sum to 1.</p>
</blockquote>

<p>We know that <code>$\sum_{\omega \in \Omega} P(\omega)=1$</code>.</p>

<p>Given a discrete random variable X (X is discrete (and therefore also
countable?)), and a probability distribution <code>$P: X \rightarrow [0;1]$</code>.</p>

<p>Then, setting <code>$\Omega=X$</code>, one can see that <code>$\sum_{x \in X} P(x)=1$</code>.</p>

<!--Possible problem: What about other variables & their distributions?
Conditional on those in joint, the result is still 1, but would be
worthwhile to write down.-->

<h3>13.3</h3>

<blockquote>
  <p>For each of the following statements, either prove it is true or give
  a counterexample.</p>
  
  <p>a. If <code>$P(a|b,c)=P(b|a,c)$</code>, then <code>$P(a|c)=P(b|c)$</code></p>
</blockquote>

<div>
    $$P(a|b,c)=P(b|a,c) \Leftrightarrow \\
    \frac{P(a,b,c)}{P(b,c)}=\frac{P(a,b,c)}{P(a,c)} \Leftrightarrow \\
    P(a,c)=P(b,c) \Leftrightarrow \\
    \frac{P(a,c)}{P(c)}=\frac{P(b,c)}{P(c)} \Leftrightarrow \\
    P(a|c)=P(b|c)$$
</div>

<p>True.</p>

<blockquote>
  <p>b. If <code>$P(a|b,c)=P(a)$</code>, then <code>$P(b|c)=P(b)$</code></p>
</blockquote>

<p>False: If
<code>$P(a)=P(a|b,c)=P(a|\lnot b,c)=P(a|b, \lnot c)=P(a|\lnot b,\lnot c)=0.1$</code>
(<code>$P(\lnot a)$</code> elided for brevity), then still can b be dependent on c,
for example <code>$P(b|c)=0.2$</code>, <code>$P(\lnot b|c)=0.8$</code>, <code>$P(b|\lnot c)=0.3$</code>,
<code>$P(\lnot b|\lnot c)=0.7$</code>, and <code>$P(c)=P(\lnot c)=0.5$</code> (which would
make <code>$P(b)=\sum_{c \in C} P(b|c)*P(c)=0.5*0.2+0.5*0.3=0.25$</code> and
<code>$P(\lnot b)=\sum_{c \in C} P(\lnot b|c)*P(c)=0.5*0.8+0.5*0.7=0.75$</code>).</p>

<blockquote>
  <p>c. If <code>$P(a|b)=P(a)$</code>, then <code>$P(a|b,c)=P(a|c)$</code></p>
</blockquote>

<p><code>$a$</code> and <code>$b$</code> are independent. However, this does not imply conditional
independence given <code>$c$</code>. E.g.:</p>

<p><code>$P(a)=0.5, P(b)=0.5, P(c|a, b)=1, P(c|\lnot a, \lnot b)=0, P(c|\lnot a, b)=1, P(c|a, \lnot b)=1$</code></p>

<p>So this is false.</p>

<!--


md5-5c1ce70e7a87abf4e6718f9dccfa3c26





md5-1429df1a52061286382ec3dc615c3531



It is rational for an agent to believe `$P(A)=0.4, P(B)=0.3$` and
`$P(A \lor B)=0.5$`, if
`$P(A \land B)=P(A)+P(B)-P(A \lor B)=0.4+0.3-0.5=0.2$`.



md5-ea875355101f5faeaeca5b93abaf18b1


-->

<h3>13.5</h3>

<blockquote>
  <p>This question deals with the properties of possible worlds, defined
  on page 488 as assignments to all random variables. We will work with
  propositions that correspond to exactly one possible world because they
  pin down the assignments of all the variables. In probability theory,
  such propositions are called <strong>atomic events</strong>. For example, with Boolean
  variables <code>$X_1, X_2, X_3$</code>, the proposition <code>$x_1 \land \lnot x_2 \land \lnot x_3$</code>
  fixes the assignment of the variables,; in the language of
  propositional logic, we would say it has exactly one model.</p>
  
  <p>a. Prove, for the case of <code>$n$</code> Boolean variables, that any two distinct
  atomic events are mutually exclusive; that is, their conjunction is
  equivalent to <em>false</em>.</p>
</blockquote>

<p>Let <code>$s_1, s_2$</code> be two distinct atomic events. That means there exists at
least one <code>$x_i$</code> so that <code>$x_i$</code> is part of the conjunction in <code>$s_1$</code>
and <code>$\lnot x_i$</code> is part of the conjunction in <code>$s_2$</code>.</p>

<p>Then:</p>

<div>
    $$s_1 \land s_2 = \\
    s_1(1) \land \dots \land s_1(i-1) \land x_i \land s_1(i+1) \land \dots \land s_1(n) \land s_2(1) \land \dots \land s_2(i-1) \land \lnot x_i \land s_2(i+1) \land \dots \land s_2(n)=\\
    s_1(1) \land \dots \land s_1(i-1) \land s_1(i+1) \land \dots \land s_1(n) \land s_2(1) \land \dots \land s_2(i-1) \land s_2(i+1) \land \dots \land s_2(n) \land x_i \land \lnot x_i=\\
    s_1(1) \land \dots \land s_1(i-1) \land s_1(i+1) \land \dots \land s_1(n) \land s_2(1) \land \dots \land s_2(i-1) \land s_2(i+1) \land \dots \land s_2(n) \land false=\\
    false$$
</div>

<blockquote>
  <p>b. Prove that the disjunction of all possible atomic events is logically
  equivalent to <em>true</em>.</p>
</blockquote>

<p>For every atomic event <code>$s$</code>, there is an atomic event
<code>$s'=\lnot s=\lnot s(1) \land \dots \lnot s(n)$</code>. Then the
disjunction of all atomic events contains <code>$s \lor s' \lor \dots=true$</code>.</p>

<blockquote>
  <p>c. Prove that any proposition is logically equivalent to the disjunction
  of the atomic events that entail its truth.</p>
</blockquote>

<p>Let <code>$\mathcal{A}$</code> be the set of <code>$n$</code> assignments that make the proposition
true. Then each assignment <code>$A_i \in \mathcal{A}$</code> corresponds to exactly
one atomic event <code>$a_i$</code> (e.g. assigning true to <code>$x_1$</code>, false to <code>$x_2$</code> and
false to <code>$x_3$</code> corresponds to <code>$x_1 \land \lnot x_2 \land \lnot x_2$</code>).
The set of these atomic events exactly entails the proposition.</p>

<p>One can then simply create the conjunction of sentences
<code>$\bigwedge_{i=1}^{n} a_i$</code> that is true only if we use an assignment
that makes the proposition true.</p>

<h2>Chapter 15</h2>

<h3>15.13</h3>

<blockquote>
  <p>A professor wants to know if students are getting enough sleep. Each
  day, the professor observes whether the students sleep in class, and
  whether they have red eyes. The professor has the following domain theory:</p>
  
  <ul>
  <li>The prior probability of getting enough sleep, with no observations, is 0.7.</li>
  <li>The probability of getting enough sleep on night t is 0.8 given
  that the student got enough sleep the previous night, and 0.3
  if not.</li>
  <li>The probability of having red eyes is 0.2 if the student got enough sleep, and 0.7 if not.</li>
  <li>The probability of sleeping in class is 0.1 if the student got enough sleep, and 0.3 if not.</li>
  </ul>
  
  <p>Formulate this information as a dynamic Bayesian network that
  the professor could use to filter or predict from a sequence of
  observations. Then reformulate it as a hidden Markov model that has only
  a single observation variable. Give the complete probability tables for
  the model.</p>
</blockquote>

<p>There are three variables: <code>$E_t$</code> for getting enough sleep in night t,
<code>$S_t$</code> for sleeping in class on day t, and <code>$R_t$</code> for having red eyes
on day t.</p>

<!--TODO: Make diagram of bayes network-->

<p>The conditional probabilities tables for the dynamic Bayesian network are:</p>

<p><code>$P(E_{t+1}|E_t)$</code>:</p>

<table>
<thead>
    <tr>
        <td>$E_t$</td>
        <td>$e_{t+1}$</td>
        <td>$\lnot e_{t+1}$</td>
    </tr>
</thead>
<tbody>
    <tr>
            <td>1</td>
            <td>0.8</td>
            <td>0.2</td>
    </tr>
    <tr>
            <td>0</td>
            <td>0.3</td>
            <td>0.7</td>
    </tr>
</tbody>
</table>

<p><code>$P(S_t|E_t)$</code>:</p>

<table>
<thead>
    <tr>
        <td>$E_t$</td>
        <td>$s_t$</td>
        <td>$\lnot s_t$</td>
    </tr>
</thead>
<tbody>
    <tr>
            <td>1</td>
            <td>0.1</td>
            <td>0.9</td>
    </tr>
    <tr>
            <td>0</td>
            <td>0.3</td>
            <td>0.7</td>
    </tr>
</tbody>
</table>

<p><code>$P(R_t|E_t)$</code>:</p>

<table>
<thead>
    <tr>
        <td>$E_t$</td>
        <td>$r_t$</td>
        <td>$\lnot r_t$</td>
    </tr>
</thead>
<tbody>
    <tr>
            <td>1</td>
            <td>0.2</td>
            <td>0.8</td>
    </tr>
    <tr>
            <td>0</td>
            <td>0.7</td>
            <td>0.3</td>
    </tr>
</tbody>
</table>

<p>For the hidden Markov model, the table for <code>$P(E_{t+1}|E_t)$</code> stays
the same. For <code>$P(S_t, R_t | E_t)$</code> we assume that <code>$S_t$</code> and <code>$R_t$</code>
are conditionally independent given <code>$E_t$</code>:</p>

<table>
<thead>
    <tr>
        <td>$E_t$</td>
        <td>$r_t, s_t$</td>
        <td>$r_t, \lnot s_t$</td>
        <td>$\lnot r_t, s_t$</td>
        <td>$\lnot r_t, \lnot s_t$</td>
    </tr>
</thead>
<tbody>
    <tr>
            <td>1</td>
            <td>0.02</td>
            <td>0.18</td>
            <td>0.08</td>
            <td>0.72</td>
    </tr>
    <tr>
            <td>0</td>
            <td>0.21</td>
            <td>0.49</td>
            <td>0.09</td>
            <td>0.21</td>
    </tr>
</tbody>
</table>

<h3>15.14</h3>

<blockquote>
  <p>For the DBN specified in Exercise 15.13 and for the evidence values</p>
  
  <ul>
  <li>e1 = not red eyes, not sleeping in class</li>
  <li>e2 = red eyes, not sleeping in class</li>
  <li>e3 = red eyes, sleeping in class</li>
  </ul>
  
  <p>perform the following computations:</p>
  
  <p>a. State estimation: Compute <code>$P(EnoughSleep_t|e_{1:t})$</code> for each of t = 1, 2, 3.</p>
</blockquote>

<p>Note: In the previous exercise, I used e as a symbol for getting enough
sleep. This collides with the abstract symbol for evidence variables,
but I'm too lazy to change it back (I will use <code>$ev$</code> for the evidence
variables instead). I will not mix abstract variables and concrete
variables (here R, S and E) to keep the confusion minimal.</p>

<p>For t=1:</p>

<div>
    $$P(E_1|e_{1:1})=\\
    E(E_1|\lnot r, \lnot s)=\\
    \alpha P(\lnot r, \lnot s| E_1)*(P(E_1|e_0)*P(e_0)+P(E_1|\lnot e_0)*P(\lnot e_0)=\\
    \alpha \langle 0.72, 0.21 \rangle * (\langle 0.8, 0.2 \rangle * 0.7 + \langle 0.2, 0.8 \rangle * 0.3)=\\
    \alpha \langle 0.4464, 0.0798 \rangle \approx \\
    \langle 0.8483, 0.151653 \rangle $$
</div>

<p>For t=2:</p>

<div>
    $$P(E_2|e_{1:2})=\\
    E(E_2|r, \lnot s)=\\
    \alpha P(r, \lnot s| E_2)*(P(E_2|e_1)*P(e_1)+P(E_2|\lnot e_1)*P(\lnot e_1)=\\
    \alpha \langle 0.18, 0.49 \rangle * (\langle 0.8, 0.2 \rangle * 0.8483 + \langle 0.3, 0.7 \rangle * 0.151653)=\\
    \alpha \langle 0.13034446, 0.13515 \rangle \approx \\
    \langle 0.490949, 0.50905 \rangle $$
</div>

<p>For t=3:</p>

<div>
    $$P(E_3|e_{1:3})=\\
    E(E_3|r, s)=\\
    \alpha P(r, s| E_3)*(P(E_3|e_2)*P(e_2)+P(E_3|\lnot e_2)*P(\lnot e_2)=\\
    \alpha \langle 0.02, 0.21 \rangle * (\langle 0.8, 0.2 \rangle * 0.490949 + \langle 0.3, 0.7 \rangle * 0.50905)=\\
    \alpha \langle 0.0109095, 0.09545 \rangle \approx \\
    \langle 0.1025715, 0.89742846\rangle $$
</div>

<blockquote>
  <p>b. Smoothing: Compute <code>$P(EnoughSleep_t|e_{1:3})$</code> for each of t = 1, 2, 3.</p>
</blockquote>

<p>I'll use k instead of t for the point of smoothing here, because, let's
be real, I don't need more double-usage of symbols:</p>

<p>For k=1:</p>

<div>
    $$P(E_1|ev_{1:t}=\alpha P(E_1|ev_{1:1})\times P(ev_{2:3}|E_1)=\alpha f_{1:1} \times b_{2:3}=\\
    \alpha \langle 0.8483, 0.151653 \rangle \times b_{2:3}=\\
    \alpha \langle 0.8483, 0.151653 \rangle \times P(ev_{2:3}|E_1)=\\
    \alpha \langle 0.8483, 0.151653 \rangle \times P(r, \lnot s | e_2)*P(ev_{3:3}|e_2)*P(e_2|E_1)+P(r, \lnot s| \lnot e_2)*P(ev_{3:3}|\lnot e_2) * P(\lnot e_2 | E_1)=\\
    \alpha \langle 0.8483, 0.151653 \rangle \times P(r, \lnot s | e_2)*P(r,s|e_2)*P(e_2|E_1)+P(r, \lnot s| \lnot e_2)*P(r,s|\lnot e_2) * P(\lnot e_2 | E_1)=\\
    \alpha \langle 0.8483, 0.151653 \rangle \times 0.18*0.02*\langle 0.8, 0.3 \rangle + 0.49*0.21*\langle 0.2, 0.7 \rangle=
    \alpha \langle 0.8483, 0.151653 \rangle \times \langle 0.02346, 0.07311 \rangle=\\
    \langle 0.64221, 0.3577896 \rangle $$
</div>

<p>For k=2:</p>

<div>
    $$P(E_2|ev_{1:t}=\alpha P(E_2|ev_{1:2})\times P(ev_{3:3}|E_2)=\alpha f_{1:2} \times b_{3:3}=\\
    \alpha  \langle 0.490949, 0.50905 \rangle \times \langle 0.490949, 0.50905\rangle \times b_{3:3}=\\
    \alpha  \langle 0.490949, 0.50905 \rangle \times \langle 0.490949, 0.50905\rangle \times P(ev_{3:3}|E_2)=\\
    \alpha  \langle 0.490949, 0.50905 \rangle \times P(r, s | e_3)*P(ev_{4:3}|e_3)*P(e_3|E_2)+P(r, s| \lnot e_3)*P(ev_{4:3}|\lnot e_3) * P(\lnot e_3 | E_2)=\\
    \alpha  \langle 0.490949, 0.50905 \rangle \times P(r, s | e_3)*P(e_3|E_2)+P(r, s| \lnot e_3) * P(\lnot e_3 | E_2)=\\
    \alpha  \langle 0.490949, 0.50905 \rangle \times 0.02*\langle 0.8, 0.3 \rangle + 0.21*\langle 0.2, 0.7 \rangle=
    \alpha  \langle 0.490949, 0.50905 \rangle \times \langle 0.058, 0.153\rangle=\\
    \langle 0.2677723998, 0.732276 \rangle $$
</div>

<p>Since I don't know <code>$e_{4:3}$</code> (I think nobody does), I assign it
probability 1. Should I assign it probability 0? I don't know!</p>

<p>For k=3:</p>

<p>The number is the same as for filtering, since k=t.</p>

<blockquote>
  <p>c. Compare the filtered and smoothed probabilities for t = 1 and t = 2.</p>
</blockquote>

<p>As a reminder,
<code>$P(E_1|ev_{1:1})=\langle 0.8483, 0.151653 \rangle, P(E_2|ev_{1:2})=\langle 0.490949, 0.50905 \rangle$</code>,
and
<code>$P(E_1|ev_{1:3})=\langle 0.64221, 0.3577896 \rangle, P(E_2|ev_{1:3})=\langle 0.2677723998, 0.732276 \rangle$</code>.</p>

<p>The probabilities don't disagree sharply at any point. Interestingly,
<code>$P(E_1|ev_{1:1})$</code> is more confident than <code>$P(E_1|ev_{1:3})$</code>, but
it's the other way around for <code>$E_2$</code>.</p>

<p>Otherwise, what's there to compare further?</p>
