
<title>niplav</title>
<link rel="shortcut icon" type="image/png" href="./favicon.png">
<link rel="stylesheet" type="text/css" href="main.css">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<!DOCTYPE HTML>

<style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
</style>

<script type="text/javascript" async
	src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	extensions: ["tex2jax.js"],
	jax: ["input/TeX", "output/HTML-CSS"],
	tex2jax: {
		inlineMath: [ ['$','$'], ["\\(","\\)"] ],
		displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
		processEscapes: true,
		skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
	},
	"HTML-CSS": { availableFonts: ["TeX"] }
	});
</script>

<script>
var anchorhash={}
function addAnchor(element) {
	var cnt=element.textContent
	if(cnt==="home")
		return;
	var ref=element.textContent.replace(/[^a-zA-Z0-9 ]/mg, "")
	ref=ref.replace(/ /mg, "-")
	var newref=ref;
	if(anchorhash[ref]===1)
		for(i=1, newref=ref+"_"+i;anchorhash[ref+"_"+i]===1;i++, newref=ref+"_"+i)
			;
	ref=newref
	element.setAttribute("id", `${ref}`)
	element.innerHTML=`<a href="#${ref}" class="hanchor">${cnt}</a>`
	anchorhash[ref]=1
}
document.addEventListener('DOMContentLoaded', function () {
	// Add anchor links to all headings
	var headers = document.querySelectorAll('h1, h2, h3, h4, h5, h6')
	if (headers) {
		headers.forEach(addAnchor)
	}
	// Change the title to the h1 header
	var title = document.querySelector('h1')
	if(title) {
		var title_elem = document.querySelector('title')
		title_elem.textContent=title.textContent + " – niplav"
	}
});
</script>

<h2><a href="./index.html">home</a></h2>

<p><em>author: niplav, created: 2021-01-21, modified: 2021-02-09, language: english, status: in progress, importance: 2, confidence: likely</em></p>

<blockquote>
  <p><strong><a href="https://en.wikipedia.org/wiki/Artificial_Intelligence:_A_Modern_Approach">“Artificial Intelligence: A Modern
  Approach”</a>,
  written by <a href="https://en.wikipedia.org/wiki/Stuart_J._Russell">Stuart
  Russell</a> and <a href="https://en.wikipedia.org/wiki/Peter_Norvig">Peter
  Norvig</a>, is probably the most
  well-known textbook on artificial intelligence. Here, I write down my
  solutions to exercises in that book. I use the 2010 edition, because the
  exercises for the 2020 edition were moved online.</strong></p>
</blockquote>

<h1>Solutions to “Artificial Intelligence: A Modern Approach”</h1>

<h2>Chapter 1</h2>

<h3>1.1</h3>

<blockquote>
  <p>Define in your own words: (a) intelligence, (b) artificial intelligence,
  (c) agent, (d) rationality, (e) logical reasoning</p>
</blockquote>

<h4>Intelligence</h4>

<p>The word “intelligence” is mostly used to describe a property of
systems. Roughly, it refers to the ability of a system to make decisions
that result in consequences are graded high according to some metric,
as opposed to decisions that result in consequences that are graded low
according to that metric.</p>

<h4>Artificial Intelligence</h4>

<p>“Artificial intelligence” refers to systems designed and implemented
by humans with the aim of these systems displaying intelligent behavior.</p>

<h4>Agent</h4>

<p>An “agent” is a part of the universe that carries out goal-directed
actions.</p>

<h4>Rationality</h4>

<p>The usage of the word “rationality” is difficult to untangle from
the usage of the word “intelligence”. For humans, “rationality”
usually refers to the ability to detect and correct cognitive errors
that hinder coming to correct conclusions about the state of the world
(epistemic rationality), as well as the ability to act on those beliefs
according to ones values (instrumental rationality). However, these
seem very related to “intelligence”, maybe only being separated by a
potentiality–intelligence being the potential, and rationality being
the ability to fulfill that potential. One could attempt to apply the
same definition to artificial intelligences, but it seems unclear how
a lawful process could be more intelligent, but is not.</p>

<h4>Logical Reasoning</h4>

<p>“Logical reasoning” refers to the act of deriving statements from
other statements according to pre-defined rules.</p>

<!--
### 1.2

> Read Turing’s original paper on AI (Turing, 1950). In the paper, he discusses several
objections to his proposed enterprise and his test for intelligence. Which objections still carry
weight? Are his refutations valid? Can you think of new objections arising from develop-
ments since he wrote the paper? In the paper, he predicts that, by the year 2000, a computer
will have a 30% chance of passing a five-minute Turing Test with an unskilled interrogator.
What chance do you think a computer would have today? In another 50 years?

TODO

-->

<h3>1.3</h3>

<p>A reflex action is not intelligent, as it is not the result of a
deliberate reasoning process. According to my personal definition above
(and also the definition given in the text), it is also not rational
(since the action is not guided by a belief).</p>

<p>Common usage of the term “rational” indicates that
people would describe this reflex as a rational action. I
believe this is fine, and words are just pointers to <a href="https://www.lesswrong.com/posts/jMTbQj9XB5ah2maup/similarity-clusters">clusters in
thing-space</a>
anyway.</p>

<h3>1.4</h3>

<blockquote>
  <p>Suppose we extend Evans’s ANALOGY program so that it can score 200
  on a standard IQ test. Would we then have a program more intelligent
  than a human? Explain.</p>
</blockquote>

<p>No. (At least not for any useful definition of intelligence). IQ
tests as they currently exist measure a proxy for the actual ability
to perform complex tasks in the real world. For humans, geometry
puzzles correlate (and predict) well with such tests (<a href="./doc/aima_solutions/the_predictive_value_of_iq_sternberg_2001.pdf" title="The Predictive Value of IQ">Sternberg et al.
2001</a>).</p>

<p>However, this proxy breaks down once we start optimising for it (as
in the case on extending ANALOGY). We can now not predict real-world
performance on arbitrary goals given the result of the IQ test performed
on ANALOGY anymore.</p>

<h3>1.5</h3>

<blockquote>
  <p>The neural structure of the sea slug Aplysia has been widely studied
  (first by Nobel Laureate Eric Kandel) because it has only about 20,000
  neurons, most of them large and easily manipulated. Assuming that the
  cycle time for an Aplysia neuron is roughly the same as for a human
  neuron, how does the computational power, in terms of memory updates
  per second, compare with the high-end computer described in Figure 1.3?</p>
</blockquote>

<!--TODO: un-fuck the dimensional analysis here-->

<p>Given the cycle time of <code>$10^{-3}$</code> seconds, we can expect</p>

<div>
    $$\frac{2*10^{4} \hbox{ neurons}}{10^{-3}\frac{\hbox{s}}{\hbox{update}}}=2*10^{7} \frac{\hbox{neuron updates}}{s}$$
</div>

<p>which is seven orders of magnitude lower than a supercomputer. Aplysia
won't be proving any important theorems soon.</p>

<!--
If Aplysia has 20k neurons, then it can be expected to have

`$2*10^{4}\hbox{ neurons }*\frac{10 \hbox{ to } 10^{5} \hbox{ synapses }}{\hbox{neuron}}=2*10^{5}\hbox{ to } 2*10^{9} \hbox{ neurons}$`
-->

<!--
TODO: do exercises 6 through 15



md5-90ba2bf42afbb78fd5cb3a0d7c80672f





md5-42b8d9b75bcbfe735de7f88fa9c38cc8





md5-c8fd8a424e8c6aaf7d1a19d8f6ed0954





md5-0be41844dfdbb5c30e02ddf3f203180a





md5-a40a8a6e33774780e5ce74c368a31ee3





md5-bc8ed33ea6eecde8d77cdaf6b0c00bc1





md5-8b24f90446132462b682b30c09a43b8b





md5-155e5c1a08dcd58f552a352db687b8c8





md5-07f1d5bd05de1bb01a1535541288b47c





md5-04d74d10fe5573b764e00005f3dc5286





md5-80dcc67ac872347ba53e2a1e7ae35f77





md5-ca24a766cfd8c027c65e850939c9bf8e





md5-fec497fa89d1ac3f75e50ac7457e2e4a





md5-ec1294541429ba5243b203e32877a440





md5-a618b3b310a6c2ac03cb7746c8542d8c





md5-ee42e0e36de3f407db1f6ae87ef0f461





md5-498560b128f3d0bfc2274ed19b1bdec3





md5-68abc62d06ca3b06586a41cf4f38a8c0





md5-2cfbfcddf6f55fed115b0b79296359b7





md5-be9931c534c2083eb71ae64616f5e8c2



-->

<h2>Chapter 2</h2>

<h3>2.1</h3>

<blockquote>
  <p>Suppose that the performance measure is concerned with just the first
  T time steps of the environment and ignores everything thereafter. Show
  that a rational agent’s action may depend not just on the state of
  the environment but also on the time step it has reached.</p>
</blockquote>

<p>Example: Let's say that we are in an environment with a button,
and pressing the button causes a light to go on in the next timestep.
The agent cares that the light is on (obtaining 1 util per timestep the
light is on for the first T timesteps).</p>

<p>However, pressing the button incurs a cost of ½ on the agent.</p>

<p>Then, at timestep T, the agent will not press the button, since it does
not care about the light being on at timestep T+1, and wants to avoid
the cost ½. At timesteps <code>$&lt;T$</code> it will press the button, with the light
currently being on, at timestep T it will not press the button, under
the same environmental conditions.</p>

<!--

### 2.2

> Let us examine the rationality of various vacuum-cleaner agent
functions.

> a. Show that the simple vacuum-cleaner agent function described in
Figure 2.3 is indeed rational under the assumptions listed on page 38.  
> b. Describe a rational agent function for the case in which each movement
costs one point. Does the corresponding agent program require internal
state?  
> c. Discuss possible agent designs for the cases in which clean squares
can become dirty and the geography of the environment is unknown. Does it
make sense for the agent to learn from its experience in these cases? If
so, what should it learn? If not, why not?

-->

<h3>2.3</h3>

<blockquote>
  <p>For each of the following assertions, say whether it is true or
  false and support your answer with examples or counterexamples where
  appropriate.</p>
  
  <p>a. An agent that senses only partial information about the state cannot
  be perfectly rational.  </p>
</blockquote>

<p>False. An agent that senses only partial information about the state could
infer missing information by making deductions (logical or statistical)
about the state of the environment, coming to full knowledge of the
environment, and making perfectly rational choices using that information.</p>

<p>For example, a chess-playing agent that can't see exactly one square
could infer the piece standing on that square by observing which piece
is missing from the rest of the board.</p>

<blockquote>
  <p>b. There exist task environments in which no pure reflex agent can
  behave rationally.  </p>
</blockquote>

<p>True. In an environment in which the next reward depends on the current
state and the previous state, a simple reflex agent will get outperformed
by agents with an internal world-model.</p>

<p>An example for this is a stock-trading agent: The future prices of stocks
doesn't just depend on the current prices, but on the history of prices.</p>

<blockquote>
  <p>c. There exists a task environment in which every agent is rational.  </p>
</blockquote>

<p>True. It is the environment where the agent has no options to act.</p>

<blockquote>
  <p>d. The input to an agent program is the same as the input to the
  agent function.  </p>
</blockquote>

<p>Not sure. Both the agent function and the agent program receive percepts,
but sometimes the agent program also needs information that is not a
percept (e.g. priors for bayesian agents). Is that counted as input,
or simply as program-specific data?</p>

<blockquote>
  <p>e. Every agent function is implementable by some program/machine
  combination.  </p>
</blockquote>

<p>False. An agent function could be uncomputable
(e. g. <a href="https://en.wikipedia.org/wiki/AIXI">AIXI</a>), and therefore not
be implementable on a real-world machine.</p>

<blockquote>
  <p>f. Suppose an agent selects its action uniformly at random from the
  set of possible actions. There exists a deterministic task environment
  in which this agent is rational.  </p>
</blockquote>

<p>True, that would be the environment in which every action scores equally
well on the performance measure.</p>

<blockquote>
  <p>g. It is possible for a given agent to be perfectly rational in two
  distinct task environments.  </p>
</blockquote>

<p>True. Given two agents <code>$A_X$</code> and <code>$A_Y$</code>, and two task environments
<code>$X$</code> (giving percepts from the set <code>$\{x_1, \dots, x_n\}$</code>) and <code>$Y$</code>
(giving percepts from the set <code>$\{y_1, \dots, y_n\}$</code>), with <code>$A_X$</code> being
perfectly rational in <code>$X$</code> and <code>$A_Y$</code> being perfectly rational in <code>$Y$</code>
an agent that is perfectly rational in two distinct task environments
could be implemented using the code:</p>

<pre><code>p=percept()
if p∈X
    A_X(p)
    while p=percept()
        A_X(p)
if p∈Y
    A_Y(p)
    while p=percept()
        A_Y(p)
</code></pre>

<blockquote>
  <p>h. Every agent is rational in an unobservable environment.  </p>
</blockquote>

<p>False. Given an unobservable environment in which moving results in
the performance measure going up (e.g. by knocking over ugly vases),
agents that move a lot are more rational than agents that do not move.</p>

<blockquote>
  <p>i. A perfectly rational poker-playing agent never loses.</p>
</blockquote>

<p>False. Given incomplete knowledge, a rational poker-playing agent can
only win in expectation.</p>

<!--

### 2.4

> For each of the following activities, give a PEAS description of the
task environment and characterize it in terms of the properties listed
in Section 2.3.2.

> * Playing soccer.
> * Exploring the subsurface oceans of Titan.
> * Shopping for used AI books on the Internet.
> * Playing a tennis match.
> * Practicing tennis against a wall.
> * Performing a high jump.
> * Knitting a sweater.
> * Bidding on an item at an auction.

### 2.5

> Define in your own words the following terms: agent, agent function,
agent program, rationality, autonomy, reflex agent, model-based agent,
goal-based agent, utility-based agent, learning agent.

### 2.6

> This exercise explores the differences between agent functions and
agent programs.

> a. Can there be more than one agent program that implements a given
agent function? Give an example, or show why one is not possible.  
> b. Are there agent functions that cannot be implemented by any agent
program?  
> c. Given a fixed machine architecture, does each agent program implement
exactly one agent function?  
> d. Given an architecture with n bits of storage, how many different
possible agent programs are there?  
> e. Suppose we keep the agent program fixed but speed up the machine
by a factor of two. Does that change the agent function?

### 2.7

> Write pseudocode agent programs for the goal-based and utility-based
agents. The following exercises all concern the implementation of
environments and agents for the vacuum-cleaner world.

### 2.8

> Implement a performance-measuring environment simulator for
the vacuum-cleaner world depicted in Figure 2.2 and specified on
page 38. Your implementation should be modular so that the sensors,
actuators, and environment characteristics (size, shape, dirt placement,
etc.) can be changed easily. (Note: for some choices of programming
language and operating system there are already implementations in the
online code repository.)

### 2.9

> Implement a simple reflex agent for the vacuum environment in Exercise
2.8. Run the environment with this agent for all possible initial dirt
configurations and agent locations. Record the performance score for
each configuration and the overall average score.

### 2.10

> Consider a modified version of the vacuum environment in Exercise 2.8,
in which the agent is penalized one point for each movement.

> a. Can a simple reflex agent be perfectly rational for this
environment? Explain.  
> b. What about a reflex agent with state? Design such an agent.  
> c. How do your answers to a and b change if the agent’s percepts
give it the clean/dirty status of every square in the environment?

### 2.11

> Consider a modified version of the vacuum environment in Exercise 2.8,
in which the geography of the environment—its extent, boundaries,
and obstacles—is unknown, as is the initial dirt configuration. (The
agent can go Up and Down as well as Left and Right.)

> a. Can a simple reflex agent be perfectly rational for this
environment? Explain.  
> b. Can a simple reflex agent with a randomized agent function outperform
a simple reflex agent? Design such an agent and measure its performance
on several environments.  
> c. Can you design an environment in which your randomized agent will
perform poorly? Show your results.  
> d. Can a reflex agent with state outperform a simple reflex
agent? Design such an agent and measure its performance on several
environments. Can you design a rational agent of this type?

### 2.12

> Repeat Exercise 2.11 for the case in which the location sensor
is replaced with a “bump” sensor that detects the agent’s
attempts to move into an obstacle or to cross the boundaries of the
environment. Suppose the bump sensor stops working; how should the
agent behave?

### 2.13

> The vacuum environments in the preceding exercises have all been
deterministic. Discuss possible agent programs for each of the following
stochastic versions:

> a. Murphy’s law: twenty-five percent of the time, the Suck action
fails to clean the floor if it is dirty and deposits dirt onto the floor
if the floor is clean. How is your agent program affected if the dirt
sensor gives the wrong answer 10% of the time?  
> b. Small children: At each time step, each clean square has a 10%
chance of becoming dirty. Can you come up with a rational agent design
for this case?  

-->

<h2>Chapter 13</h2>

<h3>13.1</h3>

<blockquote>
  <p>Show from first principile that <code>$P(a|b \land a) = 1$</code>.</p>
</blockquote>

<p>I'm not sure whether this counts as "from first principles", but</p>

<p><code>$P(a|b \land a)=\frac{P(a \land a \land b)}{P(a \land b)}=\frac{P(a \land b)}{P(a \land b)}=1$</code></p>

<p>is my solution.</p>

<h3>13.2</h3>

<blockquote>
  <p>Using the axioms of probability, prove that any probability distribution
  on a discrete random variable must sum to 1.</p>
</blockquote>

<p>We know that <code>$\sum_{\omega \in \Omega} P(\omega)=1$</code>.</p>

<p>Given a discrete random variable X (X is discrete (and therefore also
countable?)), and a probability distribution <code>$P: X \rightarrow [0;1]$</code>.</p>

<p>Then, setting <code>$\Omega=X$</code>, one can see that <code>$\sum_{x \in X} P(x)=1$</code>.</p>

<!--Possible problem: What about other variables & their distributions?
Conditional on those in joint, the result is still 1, but would be
worthwhile to write down.-->

<h3>13.3</h3>

<blockquote>
  <p>For each of the following statements, either prove it is true or give
  a counterexample.</p>
  
  <p>a. If <code>$P(a|b,c)=P(b|a,c)$</code>, then <code>$P(a|c)=P(b|c)$</code></p>
</blockquote>

<div>
    $$P(a|b,c)=P(b|a,c) \Leftrightarrow \\
    \frac{P(a,b,c)}{P(b,c)}=\frac{P(a,b,c)}{P(a,c)} \Leftrightarrow \\
    P(a,c)=P(b,c) \Leftrightarrow \\
    \frac{P(a,c)}{P(c)}=\frac{P(b,c)}{P(c)} \Leftrightarrow \\
    P(a|c)=P(b|c)$$
</div>

<p>True.</p>

<blockquote>
  <p>b. If <code>$P(a|b,c)=P(a)$</code>, then <code>$P(b|c)=P(b)$</code></p>
</blockquote>

<p>False: If
<code>$P(a)=P(a|b,c)=P(a|\lnot b,c)=P(a|b, \lnot c)=P(a|\lnot b,\lnot c)=0.1$</code>
(<code>$P(\lnot a)$</code> elided for brevity), then still can b be dependent on c,
for example <code>$P(b|c)=0.2$</code>, <code>$P(\lnot b|c)=0.8$</code>, <code>$P(b|\lnot c)=0.3$</code>,
<code>$P(\lnot b|\lnot c)=0.7$</code>, and <code>$P(c)=P(\lnot c)=0.5$</code> (which would
make <code>$P(b)=\sum_{c \in C} P(b|c)*P(c)=0.5*0.2+0.5*0.3=0.25$</code> and
<code>$P(\lnot b)=\sum_{c \in C} P(\lnot b|c)*P(c)=0.5*0.8+0.5*0.7=0.75$</code>).</p>

<blockquote>
  <p>c. If <code>$P(a|b)=P(a)$</code>, then <code>$P(a|b,c)=P(a|c)$</code></p>
</blockquote>

<p><code>$a$</code> and <code>$b$</code> are independent. However, this does not imply conditional
independence given <code>$c$</code>. E.g.:</p>

<p><code>$P(a)=0.5, P(b)=0.5, P(c|a, b)=1, P(c|\lnot a, \lnot b)=0, P(c|\lnot a, b)=1, P(c|a, \lnot b)=1$</code></p>

<p>So this is false.</p>

<!--


md5-9bbf015a046a1a1c2427476d1bdda87d





md5-9c98922ab6b7127e4275cec67dd9b5f2



It is rational for an agent to believe `$P(A)=0.4, P(B)=0.3$` and
`$P(A \lor B)=0.5$`, if
`$P(A \land B)=P(A)+P(B)-P(A \lor B)=0.4+0.3-0.5=0.2$`.



md5-a5820ebe77e408d7f7de9e047e10efc9


-->

<h3>13.5</h3>

<blockquote>
  <p>This question deals with the properties of possible worlds, defined
  on page 488 as assignments to all random variables. We will work with
  propositions that correspond to exactly one possible world because they
  pin down the assignments of all the variables. In probability theory,
  such propositions are called <strong>atomic events</strong>. For example, with Boolean
  variables <code>$X_1, X_2, X_3$</code>, the proposition <code>$x_1 \land \lnot x_2 \land \lnot x_3$</code>
  fixes the assignment of the variables,; in the language of
  propositional logic, we would say it has exactly one model.</p>
  
  <p>a. Prove, for the case of <code>$n$</code> Boolean variables, that any two distinct
  atomic events are mutually exclusive; that is, their conjunction is
  equivalent to <em>false</em>.</p>
</blockquote>

<p>Let <code>$s_1, s_2$</code> be two distinct atomic events. That means there exists at
least one <code>$x_i$</code> so that <code>$x_i$</code> is part of the conjunction in <code>$s_1$</code>
and <code>$\lnot x_i$</code> is part of the conjunction in <code>$s_2$</code>.</p>

<p>Then:</p>

<div>
    $$s_1 \land s_2 = \\
    s_1(1) \land \dots \land s_1(i-1) \land x_i \land s_1(i+1) \land \dots \land s_1(n) \land s_2(1) \land \dots \land s_2(i-1) \land \lnot x_i \land s_2(i+1) \land \dots \land s_2(n)=\\
    s_1(1) \land \dots \land s_1(i-1) \land s_1(i+1) \land \dots \land s_1(n) \land s_2(1) \land \dots \land s_2(i-1) \land s_2(i+1) \land \dots \land s_2(n) \land x_i \land \lnot x_i=\\
    s_1(1) \land \dots \land s_1(i-1) \land s_1(i+1) \land \dots \land s_1(n) \land s_2(1) \land \dots \land s_2(i-1) \land s_2(i+1) \land \dots \land s_2(n) \land false=\\
    false$$
</div>

<blockquote>
  <p>b. Prove that the disjunction of all possible atomic events is logically
  equivalent to <em>true</em>.</p>
</blockquote>

<p>For every atomic event <code>$s$</code>, there is an atomic event
<code>$s'=\lnot s=\lnot s(1) \land \dots \lnot s(n)$</code>. Then the
disjunction of all atomic events contains <code>$s \lor s' \lor \dots=true$</code>.</p>

<blockquote>
  <p>c. Prove that any proposition is logically equivalent to the disjunction
  of the atomic events that entail its truth.</p>
</blockquote>

<p>Let <code>$\mathcal{A}$</code> be the set of <code>$n$</code> assignments that make the proposition
true. Then each assignment <code>$A_i \in \mathcal{A}$</code> corresponds to exactly
one atomic event <code>$a_i$</code> (e.g. assigning true to <code>$x_1$</code>, false to <code>$x_2$</code> and
false to <code>$x_3$</code> corresponds to <code>$x_1 \land \lnot x_2 \land \lnot x_2$</code>).
The set of these atomic events exactly entails the proposition.</p>

<p>One can then simply create the conjunction of sentences
<code>$\bigvee_{i=1}^{n} a_i$</code> that is true only if we use an assignment
that makes the proposition true.</p>
