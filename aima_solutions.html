
<title>niplav</title>
<link rel="shortcut icon" type="image/png" href="./favicon.png">
<link rel="stylesheet" type="text/css" href="main.css">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<!DOCTYPE HTML>

<style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
</style>

<script type="text/javascript" async
	src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	extensions: ["tex2jax.js"],
	jax: ["input/TeX", "output/HTML-CSS"],
	tex2jax: {
		inlineMath: [ ['$','$'], ["\\(","\\)"] ],
		displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
		processEscapes: true,
		skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
	},
	"HTML-CSS": { availableFonts: ["TeX"] }
	});
</script>

<script>
var anchorhash={}
function addAnchor(element) {
	var cnt=element.textContent
	if(cnt==="home")
		return;
	var ref=element.textContent.replace(/[^a-zA-Z0-9 ]/mg, "")
	ref=ref.replace(/ /mg, "-")
	var newref=ref;
	if(anchorhash[ref]===1)
		for(i=1, newref=ref+"_"+i;anchorhash[ref+"_"+i]===1;i++, newref=ref+"_"+i)
			;
	ref=newref
	element.setAttribute("id", `${ref}`)
	element.innerHTML=`<a href="#${ref}" class="hanchor">${cnt}</a>`
	anchorhash[ref]=1
}
document.addEventListener('DOMContentLoaded', function () {
	// Add anchor links to all headings
	var headers = document.querySelectorAll('h1, h2, h3, h4, h5, h6')
	if (headers) {
		headers.forEach(addAnchor)
	}
	// Change the title to the h1 header
	var title = document.querySelector('h1')
	if(title) {
		var title_elem = document.querySelector('title')
		title_elem.textContent=title.textContent + " – niplav"
	}
});
</script>

<h2><a href="./index.html">home</a></h2>

<p><em>author: niplav, created: 2021-01-21, modified: 2021-01-29, language: english, status: in progress, importance: 2, confidence: likely</em></p>

<blockquote>
  <p><strong><a href="https://en.wikipedia.org/wiki/Artificial_Intelligence:_A_Modern_Approach">“Artificial Intelligence: A Modern
  Approach”</a>,
  written by <a href="https://en.wikipedia.org/wiki/Stuart_J._Russell">Stuart
  Russell</a> and <a href="https://en.wikipedia.org/wiki/Peter_Norvig">Peter
  Norvig</a>, is probably the most
  well-known textbook on artificial intelligence. Here, I write down my
  solutions to exercises in that book. I use the 2010 edition, because the
  exercises for the 2020 edition were moved online.</strong></p>
</blockquote>

<h1>Solutions to “Artificial Intelligence: A Modern Approach”</h1>

<h2>Chapter 1</h2>

<h3>1.1</h3>

<blockquote>
  <p>Define in your own words: (a) intelligence, (b) artificial intelligence,
  (c) agent, (d) rationality, (e) logical reasoning</p>
</blockquote>

<h4>Intelligence</h4>

<p>The word “intelligence” is mostly used to describe a property of
systems. Roughly, it refers to the ability of a system to make decisions
that result in consequences are graded high according to some metric,
as opposed to decisions that result in consequences that are graded low
according to that metric.</p>

<h4>Artificial Intelligence</h4>

<p>“Artificial intelligence” refers to systems designed and implemented
by humans with the aim of these systems displaying intelligent behavior.</p>

<h4>Agent</h4>

<p>An “agent” is a part of the universe that carries out goal-directed
actions.</p>

<h4>Rationality</h4>

<p>The usage of the word “rationality” is difficult to untangle from
the usage of the word “intelligence”. For humans, “rationality”
usually refers to the ability to detect and correct cognitive errors
that hinder coming to correct conclusions about the state of the world
(epistemic rationality), as well as the ability to act on those beliefs
according to ones values (instrumental rationality). However, these
seem very related to “intelligence”, maybe only being separated by a
potentiality–intelligence being the potential, and rationality being
the ability to fulfill that potential. One could attempt to apply the
same definition to artificial intelligences, but it seems unclear how
a lawful process could be more intelligent, but is not.</p>

<h4>Logical Reasoning</h4>

<p>“Logical reasoning” refers to the act of deriving statements from
other statements according to pre-defined rules.</p>

<!--
### 1.2

> Read Turing’s original paper on AI (Turing, 1950). In the paper, he discusses several
objections to his proposed enterprise and his test for intelligence. Which objections still carry
weight? Are his refutations valid? Can you think of new objections arising from develop-
ments since he wrote the paper? In the paper, he predicts that, by the year 2000, a computer
will have a 30% chance of passing a five-minute Turing Test with an unskilled interrogator.
What chance do you think a computer would have today? In another 50 years?

TODO

-->

<h3>1.3</h3>

<p>A reflex action is not intelligent, as it is not the result of a
deliberate reasoning process. According to my personal definition above
(and also the definition given in the text), it is also not rational
(since the action is not guided by a belief).</p>

<p>Common usage of the term “rational” indicates that
people would describe this reflex as a rational action. I
believe this is fine, and words are just pointers to <a href="https://www.lesswrong.com/posts/jMTbQj9XB5ah2maup/similarity-clusters">clusters in
thing-space</a>
anyway.</p>

<h3>1.4</h3>

<blockquote>
  <p>Suppose we extend Evans’s ANALOGY program so that it can score 200
  on a standard IQ test. Would we then have a program more intelligent
  than a human? Explain.</p>
</blockquote>

<p>No. (At least not for any useful definition of intelligence). IQ
tests as they currently exist measure a proxy for the actual ability
to perform complex tasks in the real world. For humans, geometry
puzzles correlate (and predict) well with such tests (<a href="./doc/aima_solutions/the_predictive_value_of_iq_sternberg_2001.pdf" title="The Predictive Value of IQ">Sternberg et al.
2001</a>).</p>

<p>However, this proxy breaks down once we start optimising for it (as
in the case on extending ANALOGY). We can now not predict real-world
performance on arbitrary goals given the result of the IQ test performed
on ANALOGY anymore.</p>

<h3>1.5</h3>

<blockquote>
  <p>The neural structure of the sea slug Aplysia has been widely studied
  (first by Nobel Laureate Eric Kandel) because it has only about 20,000
  neurons, most of them large and easily manipulated. Assuming that the
  cycle time for an Aplysia neuron is roughly the same as for a human
  neuron, how does the computational power, in terms of memory updates
  per second, compare with the high-end computer described in Figure 1.3?</p>
</blockquote>

<!--TODO: un-fuck the dimensional analysis here-->

<p>Given the cycle time of <code>$10^{-3}$</code> seconds, we can expect</p>

<div>
    $$\frac{2*10^{4} \hbox{ neurons}}{10^{-3}\frac{\hbox{s}}{\hbox{update}}}=2*10^{7} \frac{\hbox{neuron updates}}{s}$$
</div>

<p>which is seven orders of magnitude lower than a supercomputer. Aplysia
won't be proving any important theorems soon.</p>

<!--
If Aplysia has 20k neurons, then it can be expected to have

`$2*10^{4}\hbox{ neurons }*\frac{10 \hbox{ to } 10^{5} \hbox{ synapses }}{\hbox{neuron}}=2*10^{5}\hbox{ to } 2*10^{9} \hbox{ neurons}$`
-->

<!--
TODO: do exercises 6 through 15



md5-07ec45cd23cfa4100614e912c44ae1c8





md5-589b2ce00e2b07209c511cc07e4400f6





md5-62e0ec299b1ed08fd9d445986e644081





md5-92405e1c8c60e612a71f4724143434b3





md5-a32432d87ba32f5483d3c33ec0d37d80





md5-c9221977575e06b2ee4a5b9cee4f37ed





md5-8ada65958a70d4808b2f6512cda24090





md5-73396aa9308a22e440021faf136678ef





md5-13d6bfccac89b63fe47b0f997278c935





md5-1954846c53443d8aedb4df8ad8546040





md5-50605189481550d782fbb2c4d8dfaf43





md5-6757242ae51780b65bdafb41af4d2b24





md5-08cc303e32793bf1826b07f388af9516





md5-b567173de8b756e5cf235efcdb6abfe6





md5-34e86328ede730274827064658d0eec6





md5-dd2aeaf544fccb46e2a2dc740b1efbdc





md5-bee1f1983aae09a42ac63a93a63609d4





md5-a83168d04a17ba2028b269c889240c88





md5-dd82754d130e12a11de7f74dd55fca51





md5-7879d38f2e25c4482592fad4fba55ebf



-->

<h2>Chapter 2</h2>

<h3>2.1</h3>

<blockquote>
  <p>Suppose that the performance measure is concerned with just the first
  T time steps of the environment and ignores everything thereafter. Show
  that a rational agent’s action may depend not just on the state of
  the environment but also on the time step it has reached.</p>
</blockquote>

<p>Example: Let's say that we are in an environment with a button,
and pressing the button causes a light to go on in the next timestep.
The agent cares that the light is on (obtaining 1 util per timestep the
light is on for the first T timesteps).</p>

<p>However, pressing the button incurs a cost of ½ on the agent.</p>

<p>Then, at timestep T, the agent will not press the button, since it does
not care about the light being on at timestep T+1, and wants to avoid
the cost ½. At timesteps <T it will press the button, with the light
currently being on, at timestep T it will not press the button, under
the same environmental conditions.</p>

<!--

### 2.2

> Let us examine the rationality of various vacuum-cleaner agent
functions.

> a. Show that the simple vacuum-cleaner agent function described in
Figure 2.3 is indeed rational under the assumptions listed on page 38.  
> b. Describe a rational agent function for the case in which each movement
costs one point.  Does the corresponding agent program require internal
state?  
> c. Discuss possible agent designs for the cases in which clean squares
can become dirty and the geography of the environment is unknown. Does it
make sense for the agent to learn from its experience in these cases? If
so, what should it learn? If not, why not?

-->

<h3>2.3</h3>

<blockquote>
  <p>For each of the following assertions, say whether it is true or
  false and support your answer with examples or counterexamples where
  appropriate.</p>
  
  <p>a. An agent that senses only partial information about the state cannot
  be perfectly rational.  </p>
</blockquote>

<p>False. An agent that senses only partial information about the state could
infer missing information by making deductions (logical or statistical)
about the state of the environment, coming to full knowledge of the
environment, and making perfectly rational choices using that information.</p>

<p>For example, a chess-playing agent that can't see exactly one square
could infer the piece standing on that square by observing which piece
is missing from the rest of the board.</p>

<blockquote>
  <p>b. There exist task environments in which no pure reflex agent can
  behave rationally.  </p>
</blockquote>

<p>True. In an environment in which the next reward depends on the current
state and the previous state, a simple reflex agent will get outperformed
by agents with an internal world-model.</p>

<p>An example for this is a stock-trading agent: The future prices of stocks
doesn't just depend on the current prices, but on the history of prices.</p>

<blockquote>
  <p>c. There exists a task environment in which every agent is rational.  </p>
</blockquote>

<p>True. It is the environment where the agent has no options to act.</p>

<blockquote>
  <p>d. The input to an agent program is the same as the input to the
  agent function.  </p>
</blockquote>

<p>Not sure. Both the agent function and the agent program receive percepts,
but sometimes the agent program also needs information that is not a
percept (e.g. priors for bayesian agents). Is that counted as input,
or simply as program-specific data?</p>

<blockquote>
  <p>e. Every agent function is implementable by some program/machine
  combination.  </p>
</blockquote>

<p>False. An agent function could be uncomputable
(e. g. <a href="https://en.wikipedia.org/wiki/AIXI">AIXI</a>), and therefore not
be implementable on a real-world machine.</p>

<blockquote>
  <p>f. Suppose an agent selects its action uniformly at random from the
  set of possible actions. There exists a deterministic task environment
  in which this agent is rational.  </p>
</blockquote>

<p>True, that would be the environment in which every action scores equally
well on the performance measure.</p>

<blockquote>
  <p>g. It is possible for a given agent to be perfectly rational in two
  distinct task environments.  </p>
</blockquote>

<p>True. Given two agents <code>$A_X$</code> and <code>$A_Y$</code>, and two task environments
<code>$X$</code> (giving percepts from the set <code>$\{x_1, \dots, x_n\}$</code>) and <code>$Y$</code>
(giving percepts from the set <code>$\{y_1, \dots, y_n\}$</code>), with <code>$A_X$</code> being
perfectly rational in <code>$X$</code> and <code>$A_Y$</code> being perfectly rational in <code>$Y$</code>
an agent that is perfectly rational in two distinct task environments
could be implemented using the code:</p>

<pre><code>p=percept()
if p∈X
    A_X(p)
    while p=percept()
        A_X(p)
if p∈Y
    A_Y(p)
    while p=percept()
        A_Y(p)
</code></pre>

<blockquote>
  <p>h. Every agent is rational in an unobservable environment.  </p>
</blockquote>

<p>False. Given an unobservable environment in which moving results in
the performance measure going up (e.g. by knocking over ugly vases),
agents that move a lot are more rational than agents that do not move.</p>

<blockquote>
  <p>i. A perfectly rational poker-playing agent never loses.</p>
</blockquote>

<p>False. Given incomplete knowledge, a rational poker-playing agent can
only win in expectation.</p>

<!--

### 2.4

> For each of the following activities, give a PEAS description of the
task environment and characterize it in terms of the properties listed
in Section 2.3.2.

> * Playing soccer.
> * Exploring the subsurface oceans of Titan.
> * Shopping for used AI books on the Internet.
> * Playing a tennis match.
> * Practicing tennis against a wall.
> * Performing a high jump.
> * Knitting a sweater.
> * Bidding on an item at an auction.

### 2.5

> Define in your own words the following terms: agent, agent function,
agent program, rationality, autonomy, reflex agent, model-based agent,
goal-based agent, utility-based agent, learning agent.

### 2.6

> This exercise explores the differences between agent functions and
agent programs.

> a. Can there be more than one agent program that implements a given
agent function? Give an example, or show why one is not possible.  
> b. Are there agent functions that cannot be implemented by any agent
program?  
> c. Given a fixed machine architecture, does each agent program implement
exactly one agent function?  
> d. Given an architecture with n bits of storage, how many different
possible agent programs are there?  
> e. Suppose we keep the agent program fixed but speed up the machine
by a factor of two. Does that change the agent function?

### 2.7

> Write pseudocode agent programs for the goal-based and utility-based
agents.  The following exercises all concern the implementation of
environments and agents for the vacuum-cleaner world.

### 2.8

> Implement a performance-measuring environment simulator for
the vacuum-cleaner world depicted in Figure 2.2 and specified on
page 38. Your implementation should be modular so that the sensors,
actuators, and environment characteristics (size, shape, dirt placement,
etc.) can be changed easily. (Note: for some choices of programming
language and operating system there are already implementations in the
online code repository.)

### 2.9

> Implement a simple reflex agent for the vacuum environment in Exercise
2.8. Run the environment with this agent for all possible initial dirt
configurations and agent locations.  Record the performance score for
each configuration and the overall average score.

### 2.10

> Consider a modified version of the vacuum environment in Exercise 2.8,
in which the agent is penalized one point for each movement.

> a. Can a simple reflex agent be perfectly rational for this
environment? Explain.  
> b. What about a reflex agent with state? Design such an agent.  
> c. How do your answers to a and b change if the agent’s percepts
give it the clean/dirty status of every square in the environment?

### 2.11

> Consider a modified version of the vacuum environment in Exercise 2.8,
in which the geography of the environment—its extent, boundaries,
and obstacles—is unknown, as is the initial dirt configuration. (The
agent can go Up and Down as well as Left and Right.)

> a. Can a simple reflex agent be perfectly rational for this
environment? Explain.  
> b. Can a simple reflex agent with a randomized agent function outperform
a simple reflex agent? Design such an agent and measure its performance
on several environments.  
> c. Can you design an environment in which your randomized agent will
perform poorly? Show your results.  
> d. Can a reflex agent with state outperform a simple reflex
agent? Design such an agent and measure its performance on several
environments. Can you design a rational agent of this type?

### 2.12

> Repeat Exercise 2.11 for the case in which the location sensor
is replaced with a “bump” sensor that detects the agent’s
attempts to move into an obstacle or to cross the boundaries of the
environment. Suppose the bump sensor stops working; how should the
agent behave?

### 2.13

> The vacuum environments in the preceding exercises have all been
deterministic. Discuss possible agent programs for each of the following
stochastic versions:

> a. Murphy’s law: twenty-five percent of the time, the Suck action
fails to clean the floor if it is dirty and deposits dirt onto the floor
if the floor is clean. How is your agent program affected if the dirt
sensor gives the wrong answer 10% of the time?  
> b. Small children: At each time step, each clean square has a 10%
chance of becoming dirty. Can you come up with a rational agent design
for this case?  

-->

<h2>Chapter 13</h2>

<h3>13.1</h3>

<blockquote>
  <p>Show from first principile that <code>$P(a|b \land a) = 1$</code>.</p>
</blockquote>

<p>I'm not sure whether this counts as "from first principles", but</p>

<p><code>$P(a|b \land a)=\frac{P(a \land a \land b)}{P(a \land b)}=\frac{P(a \land b)}{P(a \land b)}=1$</code></p>

<p>is my solution.</p>

<h3>13.2</h3>

<blockquote>
  <p>Using the axioms of probability, prove that any probability distribution
  on a discrete random variable must sum to 1.</p>
</blockquote>

<p>We know that <code>$\sum_{\omega \in \Omega} P(\omega)=1$</code>.</p>

<p>Given a discrete random variable X (X is discrete (and therefore also
countable?)), and a probability distribution <code>$P: X \rightarrow [0;1]$</code>.</p>

<p>Then, setting <code>$\Omega=X$</code>, one can see that <code>$\sum_{x \in X} P(x)=1$</code>.</p>

<!--Possible problem: What about other variables & their distributions?
Conditional on those in joint, the result is still 1, but would be
worthwhile to write down.-->

<h3>13.3</h3>

<blockquote>
  <p>For each of the following statements, either prove it is true or give
  a counterexample.</p>
  
  <p>a. If <code>$P(a|b,c)=P(b|a,c)$</code>, then <code>$P(a|c)=P(b|c)$</code></p>
</blockquote>

<div>
    $$P(a|b,c)=P(b|a,c) \Leftrightarrow \\
    \frac{P(a,b,c)}{P(b,c)}=\frac{P(a,b,c)}{P(a,c)} \Leftrightarrow \\
    P(a,c)=P(b,c) \Leftrightarrow \\
    \frac{P(a,c)}{P(c)}=\frac{P(b,c)}{P(c)} \Leftrightarrow \\
    P(a|c)=P(b|c)$$
</div>

<p>True.</p>

<blockquote>
  <p>b. If <code>$P(a|b,c)=P(a)$</code>, then <code>$P(b|c)=P(b)$</code></p>
</blockquote>

<p>False: If
<code>$P(a)=P(a|b,c)=P(a|\lnot b,c)=P(a|b, \lnot c)=P(a|\lnot b,\lnot c)=0.1$</code>
(<code>$P(\lnot a)$</code> elided for brevity), then still can b be dependent on c,
for example <code>$P(b|c)=0.2$</code>, <code>$P(\lnot b|c)=0.8$</code>, <code>$P(b|\lnot c)=0.3$</code>,
<code>$P(\lnot b|\lnot c)=0.7$</code>, and <code>$P(c)=P(\lnot c)=0.5$</code> (which would
make <code>$P(b)=\sum_{c \in C} P(b|c)*P(c)=0.5*0.2+0.5*0.3=0.25$</code> and
<code>$P(\lnot b)=\sum_{c \in C} P(\lnot b|c)*P(c)=0.5*0.8+0.5*0.7=0.75$</code>).</p>

<blockquote>
  <p>c. If <code>$P(a|b)=P(a)$</code>, then <code>$P(a|b,c)=P(a|c)$</code></p>
</blockquote>

<p><code>$a$</code> and <code>$b$</code> are independent. However, this does not imply conditional
independence given <code>$c$</code>. E.g.:</p>

<p><code>$P(a)=0.5, P(b)=0.5, P(c|a, b)=1, P(c|\lnot a, \lnot b)=0, P(c|\lnot a, b)=1, P(c|a, \lnot b)=1$</code></p>

<p>So this is false.</p>

<!--


md5-387dbcc38fdea7cd2c5070a7eea8d92c





md5-11614195b274b64ffd24a2a8136f0c46



It is rational for an agent to believe `$P(A)=0.4, P(B)=0.3$` and
`$P(A \lor B)=0.5$`, if
`$P(A \land B)=P(A)+P(B)-P(A \lor B)=0.4+0.3-0.5=0.2$`.



md5-998ca0c2d6b4d1fce6253a8b52c3799a


-->
