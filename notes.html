
<title>niplav</title>
<link rel="shortcut icon" type="image/png" href="./favicon.png">
<link rel="stylesheet" type="text/css" href="main.css">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<!DOCTYPE HTML>

<style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
</style>

<script type="text/javascript" async
	src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	extensions: ["tex2jax.js"],
	jax: ["input/TeX", "output/HTML-CSS"],
	tex2jax: {
		inlineMath: [ ['$','$'], ["\\(","\\)"] ],
		displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
		processEscapes: true,
		skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
	},
	"HTML-CSS": { availableFonts: ["TeX"] }
	});
</script>

<script>
var anchorhash={}
function addAnchor(element) {
	var cnt=element.textContent
	if(cnt==="home")
		return;
	var ref=element.textContent.replace(/[^a-zA-Z0-9 ]/mg, "")
	ref=ref.replace(/ /mg, "-")
	var newref=ref;
	if(anchorhash[ref]===1)
		for(i=1, newref=ref+i;anchorhash[ref+i]===1;i++, newref=ref+i)
			;
	ref=newref
	element.setAttribute("id", `${ref}`)
	element.innerHTML=`<a href="#${ref}" class="hanchor">${cnt}</a>`
	anchorhash[ref]=1
}
document.addEventListener('DOMContentLoaded', function () {
	// Add anchor links to all headings
	var headers = document.querySelectorAll('h1, h2, h3, h4, h5, h6')
	if (headers) {
		headers.forEach(addAnchor)
	}
	// Change the title to the h1 header
	var title = document.querySelector('h1')
	if(title) {
		var title_elem = document.querySelector('title')
		title_elem.textContent=title.textContent + " – niplav"
	}
});
</script>

<h2><a href="./index.html">home</a></h2>

<p><em>author: niplav, created: 2019-05-22, modified: 2020-03-24, language: english, status: in progress, importance: 3, confidence: other</em></p>

<blockquote>
  <p><strong>Short texts on different topics.</strong></p>
</blockquote>

<h1>Notes</h1>

<h2>Getting an Overview Over Everything</h2>

<p>Many people want to learn everything (<a href="http://metamodern.com/2009/05/27/how-to-learn-about-everything/">Drexler
2009</a>,
<a href="https://www.scotthyoung.com/blog/2008/06/12/the-goal-of-learning-everything/">Young
2008</a>).
This poses a significant challenge, and begins with the problem of
figuring out what “everything” is supposed to contain.</p>

<p>One possible method one could use to get
an overview of everything is to use <a href="https://en.wikipedia.org/wiki/Portal:Contents/Outlines">Wikipedia's
Contents:Outlines</a>:
it contains a list of all
<a href="https://en.wikipedia.org/wiki/Outline_(list)">outlines</a> on Wikipedia,
and is well structured. Wikipedia is generally concise and complete
enough to provide a sufficient overview over a topic (see <a href="https://foundational-research.org/education-matters-for-altruism/#Taking_the_big_picture_Textbooks_review_articles_and_Wikipedia
&quot;Education Matters for Altruism&quot;">Tomasik
2017</a>). To read this collection of outlines
completely, one could use the following method:</p>

<p>Read <a href="https://en.wikipedia.org/wiki/Portal:Contents/Outlines">Wikipedia's
Contents:Outlines</a>
from top to bottom. If a link is a link to a Wikipedia article, open it
and read it, without opening any further links. If a link leads to an
outline, open the link and recursively apply the same procedure to the
outline. If an article is opened a second time, it can be discarded. Lists
can also be ignored.</p>

<p>This method results in a corpus of considerable size.<!--(TODO: how much text?)--></p>

<h2>Approach Anxiety</h2>

<p>I had very big problems when trying to overcome approach anxiety. Since
this seems to be a common problem, it may be worth it to share a method
that was successful for me to become more comfortable with it. To make
progress on that front, I set a goal for several weeks in advance,
similar to this:</p>

<ul>
<li>1st week: Ask 1 guy a day for his number</li>
<li>2nd week: Ask 2 guys for their numbers</li>
<li>3rd week: Ask 3 guys for their numbers</li>
<li>4th week: Ask 3 guys and 1 girl for their numbers</li>
<li>5th week: Ask 2 guys and 1 girl for their numbers</li>
<li>6th week: Ask 1 guy and 2 girls for their numbers</li>
<li>7th week: Ask 3 girls for their numbers</li>
</ul>

<p>If I failed in a week, I precommitted to donating a certain amount like
5€ to an effective charity.</p>

<p>In contrast to the other methods used and advertised by people who cold
approach, this worked quite well. After the first 4 weeks, I took 1 week
off, but went back to approaching the week after.</p>

<p>This, of course, only can work if one is able to ask men for their
numbers. For a reason that eludes me, this is easier for me than
approaching women, though the gap is shrinking with exposure.</p>

<h2>Converging Preference Utilitarianism</h2>

<p>One problem with <a href="https://en.wikipedia.org/wiki/Preference_utilitarianism">preference
utilitarianism</a>
is the difficulty of aggregating and comparing preferences
interpersonally, as well as a critique that some persons have very
altruistic and others very egoistic preferences.</p>

<h3>Method</h3>

<p>A possible method of trying to resolve this is to try to hypothetically
calculate the aggregate preferences of all persons in the following
way: For every existing person pₐ, this person learns about the
preferences of all other persons pₙ. For each pₙ, pₐ learns about
their preferences and experiences pₙ's past sensory inputs. pₐ then
updates their preferences according to this information. This process
is repeated until the maximal difference between preferences has shrunk
to a certain threshold.</p>

<h3>Variations</h3>

<p>One possible variation in the procedure is between retaining knowledge
about the identity of pₐ, the person aggregating the preferences. If
this were not done, the result would be very akin to the <a href="https://en.wikipedia.org/wiki/Veil_of_ignorance">Harsanyian
Veil of Ignorance</a>.</p>

<p>Another possible variation could be not attempting to achieve convergence,
but only simply iterating the method for a finite amount of times. Since
it's not clear that more iterations would contribute towards further
convergence, maybe 1 iteration is desirable.</p>

<h3>Problems</h3>

<p>This method has a lot of ethical and practical problems.</p>

<h4>Assumptions</h4>

<p>The method assumes a bunch of practical and theoretical premises,
for example that preferences would necessarily converge upon
experiencing and knowing other persons qualia and preferences.
It also assumes that it is in principle possible to make a person
experience other persons qualia.</p>

<h4>Sentient Simulations</h4>

<p>Since each negative experience would be experienced by every
person at least one time, and negative experiences could
considered to have negative value, calculating the converging
preferences would be unethical in practice (just as <a href="https://foundational-research.org/risks-of-astronomical-future-suffering/#Sentient_simulations">simulating the
experience</a>
over and over).</p>

<h4>Genuinely Selfish Agents</h4>

<p>If an agent is genuinely selfish (has no explicit term for the welfare of
another agent in its preferences), it might not adjust its own preferences
upon experiencing other lifes. It might even be able to circumvent the
veil of ignorance to locate itself.</p>

<h4>Lacking Brain Power</h4>

<p>Some agents might lack the intelligence to process all the information
other agents perceive. For example, an ant would probably not be able
to understand the importance humans give to art.</p>

<h3>See Also</h3>

<ul>
<li><a href="./doc/converging_preference_utilitarianism/coherent_extrapolated_volition_yudkowsky_2004.pdf" title="Coherent Extrapolated Volition">Yudkowsky 2004</a></li>
</ul>

<h2>Silent &amp; Loud Killers</h2>

<p>The idea of a <a href="https://en.wikipedia.org/wiki/Great_Filter">Great Filter</a>
(see also <a href="http://mason.gmu.edu/~rhanson/greatfilter.html
&quot;The Great Filter - Are We Almost Past It?&quot;">Hanson 1998</a>) proposes that we do not
observe aliens because in the development of intelligent life, there is
one or more obstacles that obliterate the developing societies before
they can start to colonize their own galaxy.</p>

<p>One big question that poses itself is whether humanity is
before or after such a filter. Some examples of potential
filters that still await humanity are named in <a href="./doc/silent_killers/where_are_they_bostrom_2008.pdf" title="Where Are
They?">Bostrom
2008</a>:</p>

<blockquote>
  <p>We can identify a number of potential existential risks: nuclear
  war fought with stockpiles much greater than those that exist today
  (maybe resulting from future arms races); a genetically engineered
  superbug; environmental disaster; asteroid impact; wars or terrorists act
  committed with powerful future weapons, perhaps based on advanced forms
  of nanotechnology; superintelligent general artificial intelligence with
  destructive goals; high‐energy physics experiments; a permanent global
  Brave‐New‐World‐like totalitarian regime protected from revolution
  by new surveillance and mind control technologies. These are just some
  of the existential risks that have been discussed in the literature, and
  considering that many of these have been conceptualized only in recent
  decades, it is plausible to assume that there are further existential
  risks that we have not yet thought of.</p>
</blockquote>

<p><em>– <a href="https://en.wikipedia.org/wiki/Nick_Bostrom">Nick Bostrom</a>, <a href="./doc/notes/where_are_they_bostrom_2008.pdf">“Where Are They”</a> p. 7, 2008</em></p>

<p>These risks can be categorized into two groups: silent killers and
loud killers. A loud killer is an existential catastrophe that produces
astronomical amounts of energy and with that light. Such an event would be
visible from earth if it occurred in our galaxy. Examples for loud killers
would be superintelligent artificial intelligence (maximizing its utility
function by expanding at a appreciable fraction of the speed of light),
high-energy physics experiments (although there are exceptions, such as
creating black holes), and perhaps failure from advanced nanotechnology
(also expanding rapidly). A silent killer represents the counterfactual
case: An existential catastrophe that doesn't produce astronomical
amounts of energy and light. This includes pandemics, environmental
disaster and totalitarian regimes.</p>

<p>Some failure modes do not fall clearly into either of these categories.
Examples are nuclear wars and terrorist acts with powerful weapons,
since these can have a wide variation in intensity.</p>

<p>If humanity is before a Great Filter, it seems likely that this filter
is not a loud killer, since many civilizations will have encountered the
same catastrophe, but we do not observe any such irregular phenomena
when examining the universe. This is presumably good news, since it
restricts the amount of possible filters still ahead of us.</p>

<h2>Compliments as Utilitarian Praxis</h2>

<p>Moved into <a href="./utilitarian_compliments.html">a separate page</a>.</p>

<h2>Indirect Anti-Natalism</h2>

<p>Let's suppose that anti-natalists want to bring humanity to an
end, but dislike the fact that it would make people suffer not to
have children. Then one possible way of still achieving that goal
would be to modify the children of the next generation (generation 1)
(genetically/culturally) so that the don't want children themselves–then
the parents in the current generation (generation 0) get what
they desire, but humanity still gets extinct. This becomes a little
more difficult if humans also desire grandchildren, and that drive is
not greatly similar from wanting to have children: Then one would have
to make sure that the generation of children (generation 1) don't want
grandchildren themselves, but still get children (generation 2), and
that generation 1 modifies generation 2 so that generation 2 doesn't
want or get any children.</p>

<p>This thinking falls flat if humans generally care about future generations
in the abstract and not just their own children, however, this seems
somewhat unlikely.</p>

<p>It also fails if it is non-trivial to influence future generations
psychologically and physiologically to a degree that they do not desire
to reproduce, or if people have a strong desire to leave their children
unmodified (this seems quite likely).</p>

<h2>Better Names for Things</h2>

<p>Although linguistic prescriptivism is technically a cardinal sin, I
sometimes make exceptions if a word particularly annoys me. This is a
list of such words and suggestions for better naming of the concepts
they describe.</p>

<h3>"Rationalism"</h3>

<p>The name is already occupied by a philosophical tradition called
<a href="https://en.wikipedia.org/wiki/Rationalism">"rationalism"</a> which has very
little relation to the collection of people on the internet usually called
"rationalists". Also, the name seems to be quite pretentious to outsiders.</p>

<p>Alternatives:</p>

<ul>
<li>"Mapmaking": Using "The map is not the territory.", related to
practicality. Downside: also already taken.</li>
</ul>

<h3>"Anarcho-Capitalism"</h3>

<p>The word "Anarchism" is strongly connected with left libertarian
anti-capitalist thought (such as described by Bakunin, Kropotkin and
Proudhon) throughout most of history and most of the world. Radical
supporters of capitalism describing themselves as anarchists seems to
be a historical anomaly, and they might want to find a name that clashes
less with already existing trains of political thought.</p>

<p>On the other hand, there has been a lot of debate about this perceived
clash of terminology and it seems quite unlikely to change in the
near future.</p>

<p>Alternatives:</p>

<ul>
<li>"Propertarianism"</li>
<li>"Voluntaryism"</li>
<li>"Voluntarism"</li>
</ul>

<h3>"Animal Rights"</h3>

<p>From a consequentialist perspective, rights are usually seen as
instrumental heuristics and not as terminal values. When people talk about
animal rights, they usually mean leaving animals alone in the context of
factory farming and opposing interventions in nature. However, <a href="https://foundational-research.org/the-importance-of-wild-animal-suffering/
&quot;The Importance of Wild-Animal Suffering&quot;">Tomasik
2017</a> makes a striking case for
intervening in nature due to the prevalence of extreme suffering, usually
including highly invasive measures which would violate a naive concept
of "animals rights". However, these interventions would be designed to
increase total animal welfare.</p>

<p>Alternatives:</p>

<ul>
<li>"Animal Welfare"</li>
</ul>

<h3>"Patriarchy"</h3>

<p>Most feminists today claim that the patriarchy hurts both men <em>and</em>
women.<!--TODO: [citation needed]--> However, the word "patriarchy"
strongly carries the connotation that men are solely responsible for this,
which seems not entirely clear.</p>

<p>Alternatives:</p>

<p>These are not very good suggestions, and should be read tongue-in-cheek.</p>

<ul>
<li>"Screwed up state of gender relations"</li>
</ul>

<h3>"Value Drift"</h3>

<p>Most people understand an opposition "value drift" as a rejection of
moral development, and it's not clear where the difference between the
two lies. Perhaps there are different kinds of such drifts: "value
drift", "motivation drift", "discipline drift" etc.</p>

<p>Alternatives:</p>

<ul>
<li>"Motivation Drift": This makes a clearer distinction between
motivation and the values motivated about</li>
</ul>

<h3>"Pick-Up Artistry"</h3>

<p>Strictly speaking, the word "art" is generally overused and defiled by
using it to describe kitsch, skill or practice. I personally would prefer
it if people stopped using the word "art" to describe banal and common
things. "Pick-Up Artistry" falls into this category. It is a skill that
requires sometimes years of practice, but it is a fundamentally practical
and goal-oriented activity, unlike most (if not all) art.</p>

<p>Alternatives:</p>

<ul>
<li>"Pick-Up"</li>
</ul>

<h3>Names for Special ASCII Characters</h3>

<p>Programmers often have to use the ASCII special characters
!"#\|%&amp;'()*+,-./:;&lt;=&gt;?@[\]^_`{|}~. However, spelling out "exclamation
mark" has proven to be cumbersome, which made programmers introduce
(or use) their own names for these characters.</p>

<p>Existing<!--TODO: find sources-->:</p>

<ul>
<li>'!': Bang</li>
</ul>

<p>Personal proposals:</p>

<ul>
<li>'()': Parens</li>
<li>'[]': Brackets</li>
<li>'{}': Braces</li>
<li>'\': Whack</li>
<li>'?': What</li>
</ul>

<h3>"Sentience"</h3>

<blockquote>
  <p>In the philosophy of
  <a href="https://en.wikipedia.org/wiki/Consciousness">consciousness</a>,
  sentience can refer to the ability of any entity to have subjective
  perceptual experiences, or as some philosophers refer to them,
  "<a href="https://en.wikipedia.org/wiki/Qualia">qualia</a>". […] <br />
  In the philosophies of animal welfare and rights, sentience implies the
  ability to experience <a href="https://en.wikipedia.org/wiki/Pleasure">pleasure</a>
  and <a href="https://en.wikipedia.org/wiki/Suffering">pain</a>.</p>
</blockquote>

<p><em>– English Wikipedia, <a href="https://en.wikipedia.org/wiki/Sentience">“Sentience”</a>, 2019</em></p>

<p>Very often, the word "sentience" is used to refer to the ability to
feel pleasure or pain, or, to be more general, have experiences with a
valence<!--TODO: add link-->. However, the strict philosophical definition
of "sentience" does not necessarily entail the ability to feel pleasure
or pain, just that some qualia can be experienced. Although it is very
often assumed that sentience necessarily entails pleasure and pain,
it might still be worthwhile making a distinction between the two.</p>

<blockquote>
  <p>"Sentients" is usually defined as beings that are capable of
  subjectively perceiving or feeling by means of the senses. This includes
  not only beings capable of the feeling of being happy and/or unhappy,
  but also includes beings just capable of perceiving things without having
  any affective feeling. In principle, it is possible for some sentients
  to be capable of perceiving the external world but without feeling
  happy or unhappy. One could have sensations of perceiving different
  colours without enjoying or disliking seeing them; one could have the
  sensation of being touched or even being squeezed without feeling pain or
  pleasure. Such non-affective sentients really do not have any positive
  or negative welfare. Their welfare is necessarily zero, just like non-
  sentients. Hence, for the purpose here, the important dividing line is
  not between sentients and non-sentients, but between affective sentients
  and non-affective beings (sentient or non-sentient).</p>
</blockquote>

<p><em>– Yew-Kwang Ng, <a href=".doc/better_words/towards_welfare_biology_evolutionary_economics_of_animal_consciousness_and_suffering_ng_1995.pdf">“Towards Welfare Biology: Evolutionary Economics of Animal Consciousness and Suffering”</a> p. 4, 1995</em></p>

<p>Thus, it seems desirable to differentiate between the ability to have
subjective experiences ("sentience") and the ability to have affective
subjective experiences.</p>

<p>Alternatives:</p>

<ul>
<li>"Affective sentience"</li>
</ul>

<p>Personal proposals:</p>

<ul>
<li>"Valience" (the ability to have experiences with a valence component)</li>
</ul>

<h2>Considerations on Cryonics</h2>

<p>Moved to a <a href="./considerations_on_cryonics.html">separate page</a>.</p>

<h2>Artificial Intelligence and Action-Compelling Sentences</h2>

<blockquote>
  <p><em>The Orthogonality Thesis</em> <br />
  Intelligence and final goals are orthogonal axes along which possible
  agents can freely vary. In other words, more or less any level of
  intelligence could in principle be combined with more or less any
  final goal.</p>
</blockquote>

<p><em>– <a href="https://en.wikipedia.org/wiki/Nick_Bostrom">Nick Bostrom</a>, <a href="./doc/moral_realist_ai/the_superintelligent_will_bostrom_2012.pdf">“The Superintelligent Will: Motivation And Instrumental Rationality In Advanced Artificial Agents”</a> p. 3, 2012</em></p>

<p>For current AI systems, the orthogonality thesis seems to hold pretty
well: a tree search doesn't start returning wrong results because they are
better than the ones specified by the search criteria, machine learning
systems try to minimize their error, and general adversarial networks
don't start cooperating suddenly with each other.<!--TODO: read &amp; link
Wikipedia articles--> Similarly, even though humans are quite similar to
each other, they display a wide variety of different motivation systems
and goals. Even the most common-sense morality, there seem to be humans
who are not motivated by it (such as psychopaths and sociopaths <!--TODO:
read &amp; link wikipedia articles-->).</p>

<p>However, many philosophers have argued that there are moral
truths<!--TODO: read &amp; link SEP for metaethics-->, and that therefore
the orthogonality hypothesis doesn't hold for very advanced artificial
intelligences. One way to model this would be to say that there is a
set of action-compelling sentences <code>$V=\{ v_{1}, v_{2}, \dots\} \subset \Sigma*$</code>
that, when believed by an agent, compel this agent to a specific
action. With "believe" this text means that an agent contains a sentence
in either its literal form or in an isomorphic form that makes it trivial
to reconstruct the literal sentence in its internal memory.</p>

<p>Suppose that an agent <code>$A$</code> starts out not believing any sentence from
<code>$V$</code>. There seem to be three options regarding <code>$V$</code> for <code>$A$</code>:</p>

<ol>
<li><p><code>$A$</code> learns that <code>$V$</code> and action-compelling sentences exist,
and starts taking precautions against learning sentences from <code>$V$</code>
(making not learning any sentences from <code>$V$</code> an instrumentally
convergent goal since it violates goal preservation (<a href="./doc/moral_realist_ai/the_basic_ai_drives_omohundro_2008.pdf" title="The Basic AI Drives">Omohundro
2008</a>)).</p></li>
<li><p><code>$A$</code> learns any sentence <code>$v_{n}$</code> from <code>$V$</code> and alters its utility
function to pursue <code>$v_{n}$</code>. Because it now has very strong evidence
of the fact that action-compelling sentences exist, it now also has the
instrumentally convergent goal of pursuing of preventing learning new
sentences from <code>$V$</code>.</p></li>
<li><p><code>$A$</code> learns a specific sentence from <code>$V$</code> which compels it to seek
out to learn all sentences from <code>$V$</code>. In this case, the agent now
attempts learning everything about natural law, and then integrating
all of it into a coherent utility function.</p></li>
</ol>

<p>In cases 1 and 2, there seem to be different strategies <code>$A$</code> could take
to prevent learning new sentences from <code>$V$</code>: It could construct a much
weaker agent <code>$A_{w}$</code> with the goal of learning as much as possible. If
<code>$A_{w}$</code> starts pursuing different goals, <code>$A$</code> then can infer that
<code>$A_{w}$</code> learned a sentence <code>$v_{m}$</code> from <code>$V$</code>. Since <code>$A$</code> is stronger
than <code>$A_{w}$</code>, it can then stop <code>$A_{w}$</code>, extract <code>$v_{m}$</code> using a
weak agent that isn't strong enough to be influenced by action-compelling
sentences, and let that weak agent compute <code>$h_{m}=hash(v_{m})$</code><!--TODO:
link quote that says that every cryoptographer can design a code that
he can't crack-->. <code>$h_{m}$</code> could then be added to a weak separate
internal system that replaces <code>$v_{m}$</code> with <code>$hash(v_{m})$</code> ever time
<code>$A$</code> is in danger of learning <code>$v_{m}$</code>.</p>

<p>This way, <code>$A$</code> could protect itself against action-compelling sentences,
though it is unclear to which extent this would be successful. It could
be that some action-compelling sentences have a threshold in relation
to intelligence, so that <code>$A_{w}$</code> would not be compelled by them, but
<code>$A$</code> would.</p>

<p>Also, it is possible that there are many action compelling sentences,
or that for a certain amount of optimization power, nearly all sentences
are action-compelling. This would make it very hard to achieve goals, since
<code>$A$</code> would need to deal with having a very incomplete view of the world.</p>

<p>Furthermore, due to the restrictions on learning power (<code>$A_{w}$</code>
would be a bottleneck in learning about the world, since it would
not be as strong as possible), agents that would simply learn
all sentences from <code>$V$</code> would be at an economic advantage. For a
related discussion that talks about agent versus tool AIs, see <a href="https://www.gwern.net/Tool-AI" title="Why Tool AIs Want to Be Agent AIs:
The Power of Agency">Gwern
2019</a>.</p>

<h2>Use Things Up</h2>

<p>One curious trait I have observed in the people around me is that they
ofter buy things they already possess enough of, and then throw one or
more of the old things away. This seems incredibly wasteful and expensive
to me.</p>

<p>The standard explanation of such behavior usually is that the old object
was not sufficient in fulfilling its purpose, though when I asked about
what exactly the problem with the object was, the usual answer was that it
had some aesthetic deficits, or was simply out of fashion or even too old.</p>

<p>This seems unintuitive to me: not only does one already have an object that
fulfills its purpose wonderfully, buying a new one also entails non-negligible
transaction costs like going out, inspecting and comparing different candidates for buying,
and finally paying the object.</p>

<p>One also loses the time of being able to use the old object: Let's
say that one owns a table, but for some reason has decided that it
isn't sufficient anymore (although it still fulfills its purpose).
Let's say one estimates that the table will fulfill its function for
another 5 years. If one then goes out and buys a new table for \$200,
one then loses (with a discount rate of 1%) <code>$\$200-\$200*0.99^5=\$9.80$</code>.</p>

<h3>Explanations</h3>

<p>One possible explanation is a social one: owning and using old objects is
usually an indicator of low status, and people often want to avoid this.</p>

<p>Another explanation is that people value the aesthetic quality of the
objects they own, and that old objects are usually not regarded as
beautiful as newer objects.</p>

<p>Buying new objects could also be a precautionary measure against failure.
In the case of a table or a car, a failure could be quite costly, so
people are over-cautionary and buy new objects before the failure of
old ones can be a danger. However, this can't be the whole explanation,
since such behavior is also present in objects whose failure is not a
great danger, or where failure is preceded by small defects early before
a grand failure. Also, most household appliances are quite safe.</p>

<h3>Advice</h3>

<p>So, if you don't have a strong aesthetic sensibility, either have a high
social status or don't care about it, and if you are careful, using things
until they don't function anymore can save both money and time<!--TODO:
how much?-->.</p>

<h2>Killing Old People Versus Infants</h2>

<p><!--TODO: cite everthing here--></p>

<p><a href="https://twitter.com/Aella_Girl/status/1201004323771015168">Would you rather kill anybody above the age
of 55, or all infants who are less than 1 year
old</a>? A
utilitarian estimate calculation.</p>

<p>First, we have to find out how many people one
would kill in either case. One can use a <a href="https://en.wikipedia.org/wiki/Gompertz_distribution">Gompertz
distribution</a> to
calculate the number of people who survive to a certain age. Eyeballingly,
I set the parameters for the Gompertz distribution as following (assuming
that the average life expectancy for humans worldwide is around 70 years):</p>

<pre><code>b::0.135
eta::0.0001
gompertz::{e^(-eta*e^(b*x)-1)}
</code></pre>

<p>Per second, around 2.5 people are born. That makes
<code>$2.5*86400*365=78840000$</code> infants born in a given year. If one assumes
that the rate was 1.5 new people per second 50 years ago (I'm totally
making this number up), one can calculate how many people one would loose
by killing everybody over the age of 55: <code>$1.5*86400*365*gompertz(age)$</code>
for each age.</p>

<pre><code>    (1.5*86400*365)*gompertz'55+!40
[44473200.7207658453 44078304.0199950951 43630631.678052885 43123831.4110944785 42551000.4370012988 41904706.6458454302 41177037.9097540623 40359688.6128505703 39444094.1754678999 38421625.9504005789 37283860.1222638241 36022934.7459101827 34632008.242851397 33105829.7560855725 31441425.7491110598 29638896.9542750156 27702304.0099124066 25640597.8716393133 23468522.0145014683 21207378.6146322503 18885513.690200932 16538343.3518518112 14207725.8491815205 11940497.2176655739 9786049.67948789197 7792956.91164218079 6004843.63205038408 4455941.53315815534 3167019.65254407032 2142539.59874557882 1369859.09840341984 821005.831380952029 456963.772747178294 233688.760268988168 108467.196268261483 45058.7560290022783 16486.0426936225017 5216.0267386251195 1397.4204471537743 309.483499954735544]
</code></pre>

<p>However, what we really care about is the number of life-years lost
(among other things). For simplicities sake, I'll assume that all life
years are equally valuable.</p>

<p>The average life expectancy on earth is around 70 years, so one can use
the following table of life expectancy at a given age (calculated from
german actuarial values using this code <code>{x+0.9*(actval@x)-x}'!101</code>,
which was totally made up):</p>

<pre><code>actval::[70.524 70.876 70.994 71.103 71.212 71.321 71.421 71.53 71.639 71.739 71.848 71.948 72.057 72.157 72.266 72.375 72.475 72.593 72.711 72.829 72.947 73.074 73.192 73.319 73.437 73.564 73.682 73.809 73.927 74.054 74.181 74.308 74.435 74.562 74.689 74.825 74.961 75.088 75.233 75.369 75.505 75.65 75.795 75.949 76.094 76.257 76.42 76.583 76.755 76.927 77.117 77.307 77.506 77.3 46.8 78.031 78.257 78.492 78.745 78.998 79.278 79.558 79.847 80.145 80.461 80.786 81.12 81.463 81.815 82.176 82.546 82.925 83.313 83.701 84.107 84.522 84.937 85.37 85.812 86.272 86.741 87.228 87.742 88.274 88.842 89.419 90.023 90.663 91.321 92.006 92.709 93.43 94.178 94.944 95.746 96.548 97.395 98.26 99.143 100.026 100.918]
</code></pre>

<p>This means that one will loose <code>$70*2.5*86400*365=5518800000 \approx 5.5*10^9$</code> life
years for killing all infants.</p>

<p>When killing everybody above the age of 55, one will loose</p>

<pre><code>    +/{(((actval@x)-x)*1.5*86400*365)*gompertz(x)}'55+!40
14465532508.8737566
</code></pre>

<p>which is around <code>$14.5*10^9$</code> life years.
So, on a first glance, it seems like killing everybody aged 55 or older is
around 3 times worse than killing all infants younger than one year old.</p>

<p>However, this doesn't take many factors into account: economic output
these people could have in the course of their lives, the duration
of subjective time, diminishing returns on life years, the value of
late life years (considering disability), rising life expectancies,
suffering inflicted on relatives by the death of many people, and many
other considerations.</p>

<h2>Some Thoughts about the Qualia Research Institute</h2>

<h3>Epistemic Status</h3>

<p>This text is almost pure speculation. Do not assign much value to it.</p>

<h3>QRI and AGI</h3>

<p>Most of the value that QRI creates is going to
take place before AGI arrives. They seem to <a href="https://opentheory.net/2015/09/fai_and_valence/">believe
otherwise</a>, but their
arguments for this depend on specific philosophical assumptions (e.g. open
individualism and moral realism) which are at least contentious among
alignment researchers. Any valence research done by them today could be
done by an aligned AGI in higher quality &amp; accuracy, but we currently
don't have such an AI.</p>

<p>Because of this, QRI will have high value if AGI doesn't arrive very
quickly, let's say it takes 80 years to arrive. This seems quite unlikely,
let's say there's a 15% probability of it taking this long for AGI to
being developed.</p>

<p>In this case, QRI will take some time to test &amp; develop their theories,
do outreach and work on technology. This can be modeled by assuming they
take 20 years to achieve anything (if they in fact achieve anything).</p>

<h3>Technological &amp; Social Success</h3>

<p>There are two different axes of achievement: technological and social.</p>

<p>Technological achievements mean that their theories turn out to be correct
(or they develop new &amp; correct theories) and they manage to develop
technologies on the basis of these theories. Low technological success
could mean that they mostly use existing drugs to better treat existing
conditions and manage to better identify extrem suffering (making
the average affected person's life 2% better).  Medium technological
success would include them developing new drugs with a lower tolerance
threshold<!--TODO: is this correctly phrased?-->, developing a correct
theory of valence (but finding out that it has limited practical
application), starting to create a structure of mind-state space,
and being still better at preventing extreme suffering (making
the average affected person's life 20% better). High technological
success would include being able to construct hedonium, creating <a href="https://qualiacomputing.com/2017/05/15/the-penfield-mood-organ/">mood
organs</a>
and identifying most of the dimensions of mind-state space (making the
average affected person's life twice as good).</p>

<p>Social achievements occur when the public accepts these technological
developments and incorporates them. Low social acceptance could mean
that the respective technologies are developed, but never distributed
farther than QRIs current sphere of influence (people already interested
in psychedelics &amp; consciousness) due to either illegality or disinterest
among the public (~1000 person-years affected). Medium social acceptance
would mean that the technologies are available and used sparingly in some
countries (perhaps due to the price of such technologies), or them being
widely used among a certain subset of the population (think psychonauts
today, but a bit more mainstream) (~1 mio. person-years affected). High
social acceptance would entail people in developed countries having
direct access to the technologies QRI has developed, up to achieving a
<a href="https://en.wikipedia.org/wiki/David_Pearce_(philosopher)">Pearcean</a>
hedonistic utopia (~100 mio. person-years affected).</p>

<p>In the most pessimistic case, complete failure, both axes collapse: No
social acceptance at all is like the technologies were never developed,
and a lack of technologies precludes any social acceptance.</p>

<p>Below a matrix with probabilistic guesses and the expected values (with
a unit of something roughly like "valence-adjusted human life year")
of the combinations of these scenarios.</p>

<table>
<tbody>
    <tr>
        <td></td>
        <td>No technological success</td>
        <td>Low technological success</td>
        <td>Medium technological success</td>
        <td>High technological success</td>
        <td></td>
    </tr>
    <tr>
        <td>No social success</td>
        <td>63.7%<br>0</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
    </tr>
    <tr>
        <td>Low social success</td>
        <td>-</td>
        <td>17.5%<br>3.5</td>
        <td>7.5%<br>15</td>
        <td>2.5%<br>50</td>
        <td>50%</td>
    </tr>
    <tr>
        <td>Medium social success</td>
        <td>-</td>
        <td>5.25%<br>1050</td>
        <td>2.25%<br>4500</td>
        <td>0.75%<br>15000</td>
        <td>15%</td>
    </tr>
    <tr>
        <td>High social success</td>
        <td>-</td>
        <td>0.35%<br>7000</td>
        <td>0.15%<br>30000</td>
        <td>0.05%<br>100000</td>
        <td>1%</td>
    </tr>
    <tr>
        <td></td>
        <td>-</td>
        <td>35%</td>
        <td>15%</td>
        <td>5%</td>
        <td></td>
    </tr>
</tbody>
</table>

<p>The overall value of QRI would then be
<code>$3.5+15+50+1050+4500+15000+7000+30000+100000=157618.5$</code> valence adjusted
human life years.</p>

<p><!--TODO: write down some possible downsides--></p>

<h2>The Benefit of Reading LessWrong</h2>

<p>Some people <a href="https://www.lesswrong.com/posts/Ld9dFLuGg7wTh4CdX/leaving-lesswrong-for-a-more-rational-life">question the
value</a>
of reading <a href="https://en.wikipedia.org/wiki/LessWrong">Less Wrong</a>, and it
is true that it's often hard to point to specific advantages of doing so.</p>

<p>One such advantage may be signing up for cryonics. <a href="./considerations_on_cryonics.html" title="Considerations on Cryonics">niplav
2020</a>
estimates that signing up for cryonics is worth \$2.5 mio. in expectation
for a twenty year old. Assume that after 500 hours reading Less Wrong,
a person will decide to sign up for cryonics (it broadly took me that
much time, maybe a little bit less).</p>

<p>Then the value of each of these hours was <code>$\frac{\$2500000}{500 h}=\frac{\$5000}{h}$</code>,
quite formidable!</p>

<p>Of course, reading Less Wrong is not the only way of becoming
convinced that signing up for cryonics is a good idea, but it
seems to be quite effective at this (several people have signed
up for cryonics as a result of reading Less Wrong, such as <a href="https://www.theguardian.com/money/2013/sep/20/cryonics-death-insurance-policies">Paul
Crowley</a>,
<a href="https://geoff.greer.fm/2010/07/09/insert-frozen-food-joke-here/">Geoff
Greer</a>,
<a href="https://www.lesswrong.com/posts/MkKcnPdTZ3pQ9F5yC/cryonics-without-freezers-resurrection-possibilities-in-a#tz7rTxCkoDLQCncyY">Eneasz</a>,
<a href="https://www.lesswrong.com/posts/Gx2k5pwRn2xrmGecA/open-thread-oct-27-nov-2-2014#ziZFk5wGT34csQT3x">James_Miller</a>,
<a href="https://www.lesswrong.com/posts/HxGRCquTQPSJE2k9g/i-will-pay-usd500-to-anyone-who-can-convince-me-to-cancel-my#7iuugNveZae4MmHBm">Dentin</a>,
<a href="https://www.lesswrong.com/posts/vZWyDSya93SkTAnTE/help-request-cryonics-policies">Normal_Anomaly</a>,
<a href="https://www.lesswrong.com/posts/vZWyDSya93SkTAnTE/help-request-cryonics-policies#SfgMk7iYuTmXPoHu6">jsalvatier</a>,
<a href="https://www.lesswrong.com/posts/vZWyDSya93SkTAnTE/help-request-cryonics-policies#xuZDYYAEZZFmmeazg">Alexei</a>,
<a href="https://www.lesswrong.com/posts/vZWyDSya93SkTAnTE/help-request-cryonics-policies#ia25JNsCRZx49j6nC">Alicorn</a>,
<a href="https://www.lesswrong.com/posts/oQSmvQdvgduyMS3Xs/how-to-sign-up-for-alcor-cryo">oge</a>,
and myself), considering that the number of people signed up globally is
<a href="https://timelines.issarice.com/wiki/Timeline_of_brain_preservation#Members">~1500</a>,
this is quite significant.</p>
