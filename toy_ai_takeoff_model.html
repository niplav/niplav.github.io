
<title>niplav</title>
<link rel="shortcut icon" type="image/png" href="./favicon.png">
<link rel="stylesheet" type="text/css" href="main.css">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<!DOCTYPE HTML>

<style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
</style>

<script type="text/javascript" async
	src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	extensions: ["tex2jax.js"],
	jax: ["input/TeX", "output/HTML-CSS"],
	tex2jax: {
		inlineMath: [ ['$','$'], ["\\(","\\)"] ],
		displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
		processEscapes: true,
		skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
	},
	"HTML-CSS": { availableFonts: ["TeX"] }
	});
</script>

<script>
var anchorhash={}
function addAnchor(element) {
	var cnt=element.textContent
	if(cnt==="home")
		return;
	var ref=element.textContent.replace(/[^a-zA-Z0-9 ]/mg, "")
	ref=ref.replace(/ /mg, "-")
	var newref=ref;
	if(anchorhash[ref]===1)
		for(i=1, newref=ref+"_"+i;anchorhash[ref+"_"+i]===1;i++, newref=ref+"_"+i)
			;
	ref=newref
	element.setAttribute("id", `${ref}`)
	element.innerHTML=`<a href="#${ref}" class="hanchor">${cnt}</a>`
	anchorhash[ref]=1
}
document.addEventListener('DOMContentLoaded', function () {
	// Add anchor links to all headings
	var headers = document.querySelectorAll('h1, h2, h3, h4, h5, h6')
	if (headers) {
		headers.forEach(addAnchor)
	}
	// Change the title to the h1 header
	var title = document.querySelector('h1')
	if(title) {
		var title_elem = document.querySelector('title')
		title_elem.textContent=title.textContent + " – niplav"
	}
});
</script>

<h2><a href="./index.html">home</a></h2>

<p><em>author: niplav, created: 2020-07-22, modified: 2020-12-29, language: english, status: in progress, importance: 7, confidence: highly unlikely</em></p>

<blockquote>
  <p><strong>In <a href="https://en.wikipedia.org/wiki/AI_control_problem">AI safety</a>,
  significant time has been spent on the question of
  the intelligence of AI systems over time, especially during
  <a href="https://en.wikipedia.org/wiki/Technological_singularity#Hard_vs._soft_takeoff">takeoff</a>.
  An underappreciated argument in the debate has been the idea that the more
  intelligent an AI system becomes, the better it can search the space of
  possible optimization algorithms. This post proposes a computational model
  of this process by creating an n-dimensional search space and then running
  a very simple <a href="https://en.wikipedia.org/wiki/Hill_climbing">hill-climbing</a>
  algorithm and brute-force search on that space. Possible further
  improvements to the model are suggested, and implications are discussed.</strong></p>
</blockquote>

<!--
* Another view on takeoff speeds in relation to the shape of optimization-space
    *   Maybe model with n-dimensional space with perlin noise,
        depth of current search determines size of subspace
        being searchable at the moment
-->

<h1>An Exploratory Toy AI Takeoff Model</h1>

<blockquote>
  <p>Libre de la metáfora y del mito <br />
  labra un arduo cristal: el infinito <br />
  mapa de Aquel que es todas Sus estrellas.</p>
</blockquote>

<p><em>– <a href="https://en.wikipedia.org/wiki/Jorge_Luis_Borges">Jorge Luis Borges</a>, <a href="https://thefunambulist.net/literature/litterature-spinoza-by-borges">“Spinoza”</a>, 1964</em></p>

<blockquote>
  <p>Paraphrasing Roache (2008) the state of play is such that nobody
  believes the result of a simulation, except the person who performed
  the simulation, and everybody believes the result of an experiment,
  except the person who ran the experiment.</p>
</blockquote>

<p><em>– Ryan G. McClarren, “Uncertainty Quantification and Predictive Computational Science“ p. 9, 2018</em></p>

<p>(Although the quote apparently goes back to Einstein, see “The
advancement of science, and its burdens” p. 13, only there with "theory"
instead of "simulation").</p>

<blockquote>
  <p>And let me just make an aside. There’s a lot of meta-debate that goes
  on in the AI safety community which I don’t understand why: it’s not
  as if we haven’t got enough real work to do. So now we have meta-debates
  about whether you should focus on short-term or long-term, or whether
  we should try to reduce the conflict between the short-termers and the
  long-termers and it’s like, there doesn’t need to be a conflict.</p>
</blockquote>

<p><em>– <a href="https://en.wikipedia.org/wiki/Stuart_J._Russell">Stuart J. Russell</a>, <a href="https://80000hours.org/podcast/episodes/stuart-russell-human-compatible-ai/#what-most-needs-to-be-done-015014">“The flaws that make today’s AI architecture unsafe and a new approach that could fix it”</a> (Episode 80 of the <a href="https://80000hours.org/podcast/">80,000 hours podcast</a>), 2020</em></p>

<h2>Introduction</h2>

<p>Many regard the trajectory of future AI development as
crucial<!--TODO: citation needed-->: when will AGI first be
developed? Will the development be slow, moderate, or fast (in
economic doublings, or in wallclock time)? Will one AGI system become a
<a href="https://en.wikipedia.org/wiki/Singleton_(global_governance)">singleton</a>,
i.e come to dominate the whole world (individual vs. collective
takeoff)? Will AGIs FOOM<!--TODO: link-->, i.e growing unexpectedly
fast? And: will there be one or more discontinuity when AI systems
recursively self-improve? This text attempts to shine light on the last
question and highlight a (supposedly underappreciated) argument for one or
more discontinuities in AI takeoff based on a computational model of an AI
searching the space of possible optimization algorithms for stronger ones.</p>

<p>In the model, there are three possible ways of an AI improving its
intelligence:</p>

<ul>
<li>By working and investing a part of the money to buy more hardware (e.g. by a cloud provider). This should grow roughly exponentially, at a similar speed to the <a href="https://en.wikipedia.org/wiki/Gross_world_product#Recent_growth">Gross World Product</a> (although the model does not consider wall-clock time)</li>
<li>By performing a very simple <a href="https://en.wikipedia.org/wiki/Hill_climbing">hill-climbing</a> algorithm in the space of possible optimization algorithms, such as rewriting parts of its source code in <a href="https://en.wikipedia.org/wiki/Assembly_language">assembly language</a> or making trivial small performance improvements</li>
<li>By <a href="https://en.wikipedia.org/wiki/Brute-force_search">brute-force searching</a> the space of possible optimization algorithms, possibly in the vincinity of the current algorithm</li>
</ul>

<p><!--</p>

<h3>Existing Approaches</h3>

<p>TODO: collect &amp; write up.
--></p>

<h2>The Argument</h2>

<p>The argument presented has the following premises:</p>

<ul>
<li>There exists a property of algorithms called “<strong>optimization power</strong>”
<ul>
<li>Optimization power describes (among other things) the ability of an algorithm to search a search space quickly and efficiently</li>
</ul></li>
<li>There is a space of algorithms, and each algorithm in that space can be given a number according to its optimization power</li>
<li>The space of algorithms has certain properties
<ul>
<li>There are algorithms that have positive optimization power (&gt;0)</li>
<li>There are algorithms that have vastly more optimization power than others</li>
</ul></li>
</ul>

<p>If these premises are accepted, it is possible for some search spaces to
construct an algorithm that attempts to find the best algorithm according
to its optimization power:</p>

<ul>
<li>Start with an algorithm with optimization power &gt;0</li>
<li>Let the algorithm search part of the search space for a better algorithm
<ul>
<li>If it finds one, it modifies itself to become that better algorithm</li>
<li>Else it does nothing</li>
</ul></li>
<li>If it has searched the whole search space (or has proved that no
better algorithm can exist), it stops the search</li>
<li>The algorithm acquires some more resources such as computing power</li>
</ul>

<p>This can be visualized quite nicely when one imagines the search space
to be one-dimensional with arbitrary values (or perhaps its <a href="https://en.wikipedia.org/wiki/G%C3%B6del_numbering">Gödel
number</a>) on both axes
(which is of course not the case in reality):</p>

<p><img src="./img/toy_ai_takeoff_model/search_step_2.png" alt="A jagged graph with a maximum in the middle, a vertical red line at the minimum at the left, and a small horizontal orange line on top of it, T-shaped" title="A jagged graph with a maximum in the middle, a vertical red line at the minimum at the left, and a small horizontal orange line on top of it, T-shaped" /></p>

<p>At the first step in the optimization process, the algorithm is very
weak and can only search a small portion of the space.</p>

<p><img src="./img/toy_ai_takeoff_model/search_step_4.png" alt="The same graph with the two lines, and a lighter orange vertical line on top of the darker orange one. A wider horizontal yellow line on top of the light orange line" title="The same graph with the two lines, and a lighter orange vertical line on top of the darker orange one. A wider horizontal yellow line on top of the light orange line" /></p>

<p>At the second step in the optimization process, the algorithm is slightly
better than before and already defeating a local maximum.</p>

<p><img src="./img/toy_ai_takeoff_model/search_step_6.png" alt="The same graph with the lines as before, but now an even higher light green vertical, wider dark green line on top, the whole constellation forming a “stair-like” pattern" title="The same graph with the lines as before, but now an even higher light green vertical, wider dark green line on top, the whole constellation forming a “stair-like” pattern" /></p>

<p>The algorithm is now much stronger, and exploring a sizeable fraction
of the search space.</p>

<p><img src="./img/toy_ai_takeoff_model/search_step_7.png" alt="Same image as before, with a dark-green line on top of the highest horizontal one, now reaching the global maximum" title="Same image as before, with a dark-green line on top of the highest horizontal one, now reaching the global maximum" /></p>

<p>It has now practically found the global maximum.</p>

<p>The height of the vertical bar indicates the optimization power of
the current optimization process, while the width of the vertical bar
indicates the portion of the space searched by the current algorithm.
For simplicity, the current algorithm searches in its own vincinity,
which also might be a good heuristic (since similar algorithms might
have similar properties). The width of the horizontal bar increases as
the current optimization algorithm becomes stronger and stronger, which
leads to bigger subspaces being searched and in turn better algorithms
being found.</p>

<p>This argument might fail in many different ways, e.g. if
being more intelligent does not imply being more able to
search the space of optimization algorithms quickly (e.g. by
<a href="https://en.wikipedia.org/wiki/Data_compression">compression</a> and
searching the compressed data).</p>

<p>However, a conceptual argument is not sufficient here. It would be
advantageous to test whether a similarly explosive effect occurs in
higher-dimensional spaces with different characteristics.</p>

<h2>The Model</h2>

<p>A computational model can shed some light on whether the argument above
would actually bring about discontinuities in the recursive development
of artificial intelligence, and can also provide a more concrete ground
for disagreement (for by creating an opportunity for people to modify
the code and show that different search spaces, search algorithms and
external factors generate different conclusions.</p>

<p>On a high level, in
<a href="https://en.wikipedia.org/wiki/Pseudocode">pseudocode</a>, the model executes
a code similar to this:</p>

<pre><code>space=gen_space()
fill_space(space)

pos=random_pos()

factor=intelligence=1

for i=0, rounds
    print(intelligence)
    factor*=1.001
    intelligence=max(1, space[pos])*factor
    pos=climb(space, pos)
    pos=search_subpart(space, pos, intelligence)
</code></pre>

<p>First, the space is generated and filled with values. Then, the AI
repeatedly grows a little bit, does some hill-climbing, and brute-force
searches the neighbouring space.</p>

<h3>Generating the Search Space</h3>

<p>The search space the AI would be inquiring into here is the
<a href="https://en.wikipedia.org/wiki/Space_(mathematics)">space</a>
of all possible algorithms. While I'm not very knowledgeable
about the structure of the space of algorithms, it seems
plausible to me that it would be isomorphic to the perfect <a href="https://en.wikipedia.org/wiki/Binary_tree">binary
tree</a> with infinite depth (for
any given <a href="https://en.wikipedia.org/wiki/Turing_machine">turing machine</a>
with a binary tape).</p>

<p><!--At the nth level, the parents describe the preceding bits, and the
node describes whether the nth bit is 0 or 1.--></p>

<p><!--Perhaps trinary tree? 0/1/$ for bits and ending--></p>

<p>However, since I do not know algorithms that would assign
possible values to nodes in the tree, as well as fitting
search algorithms, I decided instead to use a <a href="https://en.wikipedia.org/wiki/Euclidean_space">Euclidean
space</a> to stand in for
the space of all algorithms. Specifically, the metric space was even
further simplified as an n-dimensional array with equal side-lengths:</p>

<pre><code>dim=5
size=33
space=np.zeros([size]*dim)
</code></pre>

<p>However, I might return to more accurate representations of the space
of possible algorithms.</p>

<p><!--TODO: actually do that--></p>

<h3>Filling the Search Space</h3>

<p>The most important decision in this model is how to fill the search space
(that is, what values to give the points in the search space).</p>

<p>Since I am very confused about what what a useful approximation of
the search space of intelligent algorithms could look like, I start by
generalizing the Diamond-Square algorithm to n dimensions.</p>

<p><!--
TODO:</p>

<h4>Desiderata for the Search Space</h4>

<ul>
<li>Sparse: Most algorithms don't really do any optimization work at all</li>
<li>"Spiky": Some optimization algorithms are vastly stronger than other, very similar algorithms
--></li>
</ul>

<h4>Diamond-Square</h4>

<p>The <a href="https://en.wikipedia.org/wiki/Diamond-square_algorithm">Diamond-Square
algorithm</a>
is a fractal algorithm originally developed for terrain generation.</p>

<p>The generated landscapes often resemble mountain-ranges, they show
relatively few abrupt local maxima. An example for a generated landscape:</p>

<p><img src="./img/toy_ai_takeoff_model/twodim.png" alt="Space generated by the algorithm in two dimensions" title="Space generated by the algorithm in two dimensions" /></p>

<p>The size of the space is restricted to dimensions of height/width/length
etc. <code>$2^{n}+1$</code>.</p>

<pre><code>space=create_space(dim, size, minval, maxval, extrfact)
</code></pre>

<p><code>create_space</code> is described in detail in <a href="./generalizing_diamond_square.html">Generalizing the Diamond-Square Algorithm to n Dimensions</a>.</p>

<h3>Searching the Space</h3>

<p>After generating the search space, it is searched for a number of times,
each time increasing the intelligence of the current search process by
a given factor.</p>

<pre><code>for i in range(0, rounds):
    factor*=growth
    intelligence=max(1, space[tuple(pos)])*factor
    f.write(str(space[tuple(pos)]) + "," + str(intelligence) + "\n")
</code></pre>

<p>To avoid algorithms of zero or negative intelligence, the floor of
intelligence is set to 1.</p>

<p>The space is searched in two different ways, starting from a random point:</p>

<pre><code>pos=[random.randint(0, size-1) for i in range(0, dim)]
pos=climb(space, pos, size, dim)
pos=search_around(space, pos, size, dim, intelligence)
</code></pre>

<h4>Hill Climbing</h4>

<p>First, the algorithm executes a very simple hill-climbing procedure. Here,
it examines the position next to it in every dimension (in the case of
two: in front of the current position, behind of the current position,
left to it and right to it), but not the corners, and chooses the
direction with the highest value. It then returns the position with
the highest value, if that value is greater than the one of the current
position.</p>

<pre><code>maxpos=np.array(pos)
for i in range(0, dim):
    pos[i]+=1
    if 0&lt;=pos[i]&lt;size:
        if space[tuple(pos)]&gt;space[tuple(maxpos)]:
            maxpos=np.array(pos)
    pos[i]-=2
    if 0&lt;=pos[i]&lt;size:
        if space[tuple(pos)]&gt;space[tuple(maxpos)]:
            maxpos=np.array(pos)
    pos[i]+=1
return maxpos
</code></pre>

<h4>Brute Force Search</h4>

<p>After hill-climbing, the model searches the neighbouring region of
the search space for better algorithms. The neighbouring region,
in this case, is a hypercube of dimension <code>$n$</code> and the size
<code>$1+2*\sqrt[n]{i^2}$</code>, with the current position being the center
of that cube (<code>$i$</code> is the current intelligence).</p>

<p>The choice of making the size of the hypercube cubic in the intelligence
is pretty much arbitrary. I will test with different approaches,
e.g. making it linear.<!--TODO: test around whether this makes any
difference--></p>

<pre><code>step=round(intelligence**(2/dim))
subpos=[slice(0,0)]*dim
for i in range(0, dim):
    subpos[i]=slice(max(0,pos[i]-step), min(size-1, pos[i]+step))
subsection=space[tuple(subpos)]
</code></pre>

<p>This subsection of the space is then brute-force searched for a maximum,
akin to the agent being able to reason about it and find near local
maxima.</p>

<pre><code>mp=np.where(subsection == np.amax(subsection))
pos=np.array([list(mp[i])[0]+subpos[i].start for i in range(0, dim)])
return pos
</code></pre>

<p>The position of the maximum found is then returned (often the current
position). A new maximum having been found is akin to the agent
discovering a more intelligent agent, and modifying itself to become
that agent.</p>

<p>This approach is not as efficient as it could be: If the agent is caught
at a local maximum, this approach leads to it searching parts of the
search space multiple times.</p>

<h3>External Intelligence Improvements</h3>

<p>Under this model, I assume an exponential growth as
a backdrop.  This exponential growth could be <a href="https://en.wikipedia.org/wiki/Moore%27s_law">Moore's
Law</a> or <a href="https://en.wikipedia.org/wiki/Gross_world_product">Gross World
Product</a> growth,
or another uninterrupted exponential growth mode.</p>

<p>This factor is currently set to 1.001 per timestep, or 0.1% growth.
If the backdrop is Moore's Law, with a doubling time of 18 months (or
540 days), then a timestep would be</p>

<div>
    $$\frac{540 \hbox{ days}}{\ln_{1.001}(2)} \approx 0.779 \hbox{ days}$$
</div>

<p>If one assumes GWP growth as a backdrop instead, one gets a
doubling every 20 years (…yet. growth mindset) (see <a href="https://www.openphilanthropy.org/blog/modeling-human-trajectory" title="Modeling the Human Trajectory">Roodman
2020</a>), which
works out to</p>

<div>
    $$\frac{7300 \hbox{ days}}{\ln_{1.001}(2)} \approx 10.52 \hbox{ days}$$
</div>

<p>per timestep.</p>

<p>Both assumptions seem not unreasonable to me (although I'm not an expert
on such things): A day seems enough time for an AI to design and deploy
an improvement to its own source code, although I acknowledge this might
change with different AI designs (especially more clean and algorithmic
designs might improve faster, while fuzzy &amp; big neural nets might take
much longer).</p>

<p><!--TODO: add analysis with growth mode of current ML models--></p>

<h2>Parameters</h2>

<p>I ran the model several times, varying the size and dimensionality of
the search space. The spaces used were <code>$\mathbb{F}_{8193}^{1}$</code>,
<code>$\mathbb{F}_{16385}^{1}$</code>,  <code>$\mathbb{F}_{32769}^{1}$</code>,
<code>$\mathbb{F}_{65537}^{1}$</code>,  <code>$\mathbb{F}_{1048577}^{1}$</code>,
<code>$\mathbb{F}_{16777217}^{1}$</code>,  <code>$\mathbb{F}_{4097}^{2}$</code>,
<code>$\mathbb{F}_{65}^{3}$</code>,  <code>$\mathbb{F}_{129}^{3}$</code>,
<code>$\mathbb{F}_{255}^{3}$</code>,  <code>$\mathbb{F}_{65}^{4}$</code>,
<code>$\mathbb{F}_{33}^{5}$</code>,  <code>$\mathbb{F}_{17}^{6}$</code> and
<code>$\mathbb{F}_{9}^{8}$</code> (<code>$\mathbb{F}_{a}^{b}$</code> being the <a href="https://en.wikipedia.org/wiki/Vector_space">vector
space</a> of dimensionality
<code>$b$</code> for the <a href="https://en.wikipedia.org/wiki/Finite_field">finite field</a>
with <code>$a$</code> elements).</p>

<p>Each iteration ran through 2048 timesteps, with a growth of 1.001.</p>

<p><!--TODO: add explanations for minval, maxval &amp; extrfact--></p>

<p>I ran the model only once with each set of parameters, since I discovered
that some parts of the model are very slow and take quite some time to
execute on my puny hardware (I left the model running through the night).</p>

<p>I would like to run the model with a bigger search space, and more
often than once, but unless I optimize the code to be faster, I expect
the easiest option would be for me to get access to a computer that
is more capable than my current laptop. If you have access to such
a computer and want to run the code with other parameters on it,
<a href="./about.html#Contact">contact me</a> or modify the code yourself
(<a href="./code/toy_ai_takeoff_model/takeoff.py">relevant file 1</a>, <a href="./code/toy_ai_takeoff_model/ndim_diamond_square.py">relevant
file 2</a>) and send
me the results.</p>

<h2>Results</h2>

<p><!--</p>

<h3>Uniform Values</h3>

<h3>Normal Values</h3>

<h3>Lognormal Values</h3>

<p>--></p>

<h2>Limitations</h2>

<h3>Search Space Wrong</h3>

<h3>No Hill Climbing</h3>

<h3>No Brute-Force Search</h3>

<h3>Additional Factors</h3>

<h2>Conclusion</h2>

<p><!--Articles:
* https://aiimpacts.org/historical-growth-trends/
* https://aiimpacts.org/likelihood-of-discontinuous-progress-around-the-development-of-agi/
* https://intelligence.org/ai-foom-debate/
* https://intelligence.org/files/IEM.pdf
* https://longtermrisk.org/the-future-of-growth-near-zero-growth-rates/
* https://sideways-view.com/2018/02/24/takeoff-speeds/
* https://www.lesswrong.com/posts/5WECpYABCT62TJrhY/will-ai-undergo-discontinuous-progress
* https://www.lesswrong.com/posts/66FKFkWAugS8diydF/modelling-continuous-progress
* https://www.lesswrong.com/posts/77xLbXs6vYQuhT8hq/why-ai-may-not-foom
* https://www.lesswrong.com/posts/CjW4axQDqLd2oDCGG/misconceptions-about-continuous-takeoff
* https://www.lesswrong.com/posts/JBadX7rwdcRFzGuju/recursive-self-improvement
* https://www.lesswrong.com/posts/YgNYA6pj2hPSDQiTE/distinguishing-definitions-of-takeoff
* https://www.lesswrong.com/posts/cxgtQXnH2uDGBJJGa/redefining-fast-takeoff
* https://www.lesswrong.com/posts/tjH8XPxAnr6JRbh7k/hard-takeoff
* https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ai-timelines
--></p>

<p><!--
https://en.wikipedia.org/wiki/Brownian<em>surface
https://en.wikipedia.org/wiki/Fractal</em>landscape
https://en.wikipedia.org/wiki/Fractional<em>Brownian</em>motion
https://en.wikipedia.org/wiki/Gradient<em>descent
https://en.wikipedia.org/wiki/Newton%27s</em>method
https://en.wikipedia.org/wiki/OpenSimplex<em>noise
https://en.wikipedia.org/wiki/Perlin</em>noise
https://en.wikipedia.org/wiki/Simplex_noise
https://github.com/buckinha/DiamondSquare
https://nullprogram.com/blog/2007/11/20/
--></p>
