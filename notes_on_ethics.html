
<title>niplav</title>
<link rel="shortcut icon" type="image/png" href="./favicon.png">
<link rel="stylesheet" type="text/css" href="main.css">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<!DOCTYPE HTML>

<style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
</style>

<script type="text/javascript" async
	src="./mathjax/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	extensions: ["tex2jax.js"],
	jax: ["input/TeX", "output/HTML-CSS"],
	tex2jax: {
		inlineMath: [ ['$','$'], ["\\(","\\)"] ],
		displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
		processEscapes: true,
		skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
	},
	"HTML-CSS": { availableFonts: ["TeX"] }
	});
</script>

<script>
var anchorhash={}
function addAnchor(element) {
	var cnt=element.textContent
	if(cnt==="home")
		return;
	var ref=element.textContent.replace(/[^a-zA-Z0-9 ]/mg, "")
	ref=ref.replace(/ /mg, "-")
	var newref=ref;
	if(anchorhash[ref]===1)
		for(i=1, newref=ref+"_"+i;anchorhash[ref+"_"+i]===1;i++, newref=ref+"_"+i)
			;
	ref=newref
	element.setAttribute("id", `${ref}`)
	element.innerHTML=`<a href="#${ref}" class="hanchor">${cnt}</a>`
	anchorhash[ref]=1
}
document.addEventListener('DOMContentLoaded', function () {
	// Add anchor links to all headings
	var headers = document.querySelectorAll('h1, h2, h3, h4, h5, h6')
	if (headers) {
		headers.forEach(addAnchor)
	}
	// Change the title to the h1 header
	var title = document.querySelector('h1')
	if(title) {
		var title_elem = document.querySelector('title')
		title_elem.textContent=title.textContent + " – niplav"
	}
});
</script>

<h2><a href="./index.html">home</a></h2>

<p><em>author: niplav, created: 2021-03-31, modified: 2021-04-11, language: english, status: notes, importance: 3, confidence: highly unlikely</em></p>

<blockquote>
  <p><strong>This page contains my notes on ethics, separated from my regular
  notes to retain some structure to the notes.</strong></p>
</blockquote>

<h1>Notes on Ethics</h1>

<p>My general ethical outlook is one of high <a href="./doc/ethics_notes/moral_uncertainty_macaskill_et_al_2020.pdf">moral
uncertainty</a>,
with my favourite theory being consequentialism. I furthermore favour
hedonic, negative-leaning, and act-based consequentialisms.</p>

<p>However, most notes on this page don't depend on these assumptions.</p>

<p>Note that while I am interested in ethics, I haven't read as much about
the topic as I would like. This probably leads to me re-inventing a large
amount of jargon, and making well-known (and already refuted) arguments.</p>

<h2>Humans Implement Ethics Discovery</h2>

<p>Humans sometimes change their minds about what they consider to be good,
both on a individual and on a collective scale. One obvious example is
slavery in western countries: although our wealth would make us more
prone to admitting slavery (high difference between wages &amp; costs of
keeping slaves alive), we have nearly no slaves. This used to be different,
in the 18th and 19th century, slavery was a common practice.</p>

<p>This process seems to come partially from learning new facts about
the world (e.g., which ethical patients respond to noxious stimuli,
how different ethical patients/agents are biologically related to each
other, etc.), let's call this the <em>model-updating process</em>. But there also
seems to be an aspect of humans genuinely re-weighting their values when
they receive new information, which could be called the <em>value-updating
process</em>. There also seems to be a third value-related process
happening, which is more concerned with determining inconsistencies
within ethical theories by applying them in thought-experiments (e.g. by
discovering problems in population axiology, see for example <a href="./doc/ethics_notes/overpopulation_and_the_quality_of_life_parfit_1986.pdf" title="Overpopulation and the Quality of Life">Parfit
1986</a>).
This process might be called the <em>value-inference process</em>.</p>

<p>One could say that humans implement the <em>value-updating</em>
and the <em>value-inference</em> process – when they think about
ethics, there is an underlying algorithm that weighs trade-offs,
considers points for and against specific details in theories,
and searches for maxima. As far as I know, there is no crisp
formalization of this process (initial attempts are <a href="https://plato.stanford.edu/entries/reflective-equilibrium/">reflective
equilibrium</a>
and <a href="./doc/converging_preference_utilitarianism/coherent_extrapolated_volition_yudkowsky_2004.pdf">coherent extrapolated
volition</a>).</p>

<p>If we accept the <a href="https://arbital.com/p/complexity_of_value/">complexity of human
values</a> hypothesis, this
absence of a crisp formalism is not surprising: the algorithm for
<em>value-updating</em> and <em>value-inference</em> is probably too complex to
write down.</p>

<p>However, since we know that humans are existing implementations of this
process, we're not completely out of luck: if we can preserve humans
"as they are" (and many of the notes on this page try to get at what
this fuzzy notion of "as they are" would mean), we have a way to further
update and infer values.</p>

<p>This view emphasizes several conclusions: preserving humans "as they
currently are" becomes very important, perhaps even to the extent of
misallowing self-modification, the loss of human cultural artifacts
(literature, languages, art) becomes more of a tragedy than before
(potential loss of information about what human values are), and making
irreversible decisions becomes worse than before.</p>

<!--Often, change in values seems forseeable. Why? How?-->

<h2>I Care About Ethical Decision Procedures</h2>

<p>Or, why virtue ethics alone feels misguided.</p>

<p>In general, ethical theories want to describe what is good and what
is bad. Some ethical theories also provide a decision-procedure: what
to do in which situations. One can then differentiate between ethical
theories that give recommendations for action in every possible situation
(we might call those <em>complete theories</em>), and ethical theories that
give recommendations for action in a subset of all possible situations
(one might name these <em>incomplete theories</em>, although the name might be
considered unfair by proponents of such theories).</p>

<!--Add stuff about partial orderings of actions, with multiple maximal elements?-->

<p>It is important to clarify that incomplete theories are not necessarily
indifferent between different choices for action in situations they have
no result for, but that they just don't provide a recommendation for action.</p>

<p>Prima facie, complete theories seem more desirable than incomplete
theories – advice in the form of "you oughtn't be in this situation
in the first place" is not very helpful if you are confronted with such
a situation!</p>

<p>Virtue ethics strikes me as being such a theory – it defines what is
good, but provides no decision-procedure for acting in most situations.</p>

<p>At best, it could be interpreted as a method for developing such a
decision-procedure for each individual agent, recognizing that an attempt
at formalizing an ethical decision-procedure is a futile goal, and instead
focussing on the value-updating and value-inference process itself.</p>

<h2>Deference-Attractors of Ethical Agents</h2>

<p>Basically: When I'm angry or stressed, I would like to defer to
versions of myself that are relaxed &amp; calm. If we draw a network of
these deferrings, there are probably some attractors.</p>

<h3>Deceptive Deference-Attractors</h3>

<h2>What Use Ethics For?</h2>

<p>Everyday life, or problems that arise in the limit?</p>

<p>C.f. High Energy Ethics.</p>

<h2>The Two Urgent Problems are: How Don't We Die and How Do We Become Happy?</h2>

<h2>A Very Subjective Ranking of Types of Ethical Theories</h2>

<p>Consequentialisms, Contractualism, Deontology, Virtue Ethics</p>

<h2>What Is Wrong With the Unwilling Organ-Donor Thought Experiment?</h2>

<p>Problems with game-theoretical ethical intuitions.</p>

<p>Better framing: Create universe, make decision, destroy universe after
payoff time.</p>

<p>For most people, there's a point where they kill the unwilling organ
donor, so we're basically haggling over the price. Maybe just a Sorites
paradox?</p>

<h2>Why Death is Bad</h2>

<p>Under moral uncertainty with evolving preferences, you want to keep
options open, but death closes all options but one, potentially losing
a lot of future value.</p>

<p>In a sense, it's unfair towards all other ethical systems you embody
to kill yourself.</p>

<h2>Possible Surprising Implications of Moral Uncertanity</h2>

<p>Preserving languages &amp; biospheres might be really important, if the
continuity of such processes is morally relevant.</p>

<p>We should try to be careful about self-modification, lest we fall into
a molochian attractor state we don't want to get out of. Leave a line
of retreat in ideology-space!</p>

<h2>The Repugnant Conclusion Probably Implies Many or Only One Moral Patients</h2>

<p>Diminishing or increasing returns on investments for well-being
of a single agent?</p>

<p>If we're really lucky, initially there are increasing returns, but at
some point they start diminishing.</p>
</html>
