<html><head></head><body><h2 id="home"><a href="./index.html">home</a></h2>


<title>niplav</title>
<link href="./favicon.png" rel="shortcut icon" type="image/png">
<link href="main.css" rel="stylesheet" type="text/css">
<meta content="text/html; charset=utf-8" http-equiv="Content-Type">
<style type="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
</style>
<script>
document.addEventListener('DOMContentLoaded', function () {
	// Change the title to the h1 header
	var title = document.querySelector('h1')
	if(title) {
		var title_elem = document.querySelector('title')
		title_elem.textContent=title.textContent + " – niplav"
	}
});
</script>
<p><em>author: niplav, created: 2025-12-06, modified: 2025-12-09, language: english, status: in progress, importance: 6, confidence: possible</em></p>
<blockquote>
<p><strong>.</strong></p>
</blockquote><div class="toc"><div class="toc-title">Contents</div><ul><li><a href="#How_Big_of_a_Problem_Is_It">How Big of a Problem Is It</a><ul><li><a href="#Current_Evidence">Current Evidence</a><ul></ul></li></ul></li><li><a href="#CounterArguments">Counter-Arguments</a><ul></ul></li><li><a href="#How_Malleable_Are_Human_Minds">How Malleable Are Human Minds</a><ul></ul></li><li><a href="#How_Might_We_Solve_It">How Might We Solve It</a><ul></ul></li><li><a href="#Related_Research">Related Research</a><ul></ul></li></ul></div>
<h1 id="Malleable_Human_Minds_and_AI"><a class="hanchor" href="#Malleable_Human_Minds_and_AI">Malleable Human Minds and AI</a></h1>
<p>Human values are physically instantiated, and those physical
instantiations can be changed by processes in the world around them. This
is a problem when you have systems around that can steer reality very
well, for example advanced AI systems.</p>
<p>This kind of issue sometimes appears in interactions
between humans, for example between <a href="https://en.wikipedia.org/wiki/Suffrage_for_Americans_with_Disabilities">mentally
disabled people and their caretakers in the context of
voting</a>.
The disabled person might lack the mental tools to form an opinion on
who to vote for on their own, and every action their caretaker takes to
help them form their opinion could steer the process the disabled person
takes to form their opinion by a large amount. In some sense it
might be under-defined what the "correct" way of trying them help
find a party to vote for is.</p>
<p>I think that advanced AI systems could be in a similar relation to
us as caretakers are to people with mental disabilities: They will
be able to easily<sup id="fnref1"><a href="#fn1" rel="footnote">1</a></sup> influence us by presenting us with <a href="https://www.lesswrong.com/s/uLEjM2ij5y3CXXW6c/p/fhJkQo34cYw6KqpH3">selected
information</a>,
find it hard to communicate the consequences of different decisions so
that we'd find them comprehensible, and see multiple equally valuable
pathways that involve human modification.</p>
<p>The problem of human malleability has several aspects, (1) direct value
modification, (2) addiction/superstimuli, (3) humans becoming unable to
understand AI reasoning, (4) the philosophical problem that "what humans
really want" might be underdefined. Worth disentangling and finding
different interventions?</p>
<p>The worry of human minds being malleable wrt the optimization
exerted by AIs would apply in multiple scenarios, both when AIs are
<a href="https://www.lesswrong.com/s/EmDuGeRw749sD3GKd/p/ZeE7EKHTFMBs8eMxn">intent-aligned</a>
vs. not, where they steer beliefs vs. values, whether they are attempting
take-over or they are not.</p>
<ol>
<li>In AI takeover scenarios, highly persuasive AIs might manipulate the developers or operators of their deployment environment and enable them to give those AIs more power.</li>
<li>Resulting in the world becoming optimized towards being a <a href="https://www.lesswrong.com/posts/nFv2buafNc9jSaxAH/siren-worlds-and-the-perils-of-over-optimised-search">siren world</a>, see also <a href="https://www.lesswrong.com/posts/c7kZKGswtLkjFbszg/balancing-exploration-and-resistance-to-memetic-threats">Neyman 2025</a></li>
<li>Intent alignment becomes fuzzy, much more complicated (see the <a href="https://arbital.com/p/dwim/">Do-What-I-Mean hierarchy</a>).</li>
<li>The future might be very addictive, similar to how modern humans are addicted to many superstimuli compared to hunter-gatherers.</li>
</ol>
<blockquote>
<p>Some people work makeshift government jobs; others collect a generous
basic income. Humanity could easily become a society of superconsumers,
spending our lives in an opium haze of amazing AI-provided luxuries
and entertainment.</p>
</blockquote>
<p><em>—Kokotajlo et al., <a href="https://ai-2027.com/slowdown#slowdown-2029-12-31">“AI 2027”</a>, 2025</em></p>
<p>I'll mostly focus on situations where AIs are more or less intent-aligned
to some humans, and have looked at non-intent-aligned AI persuasion risks
<a href="https://niplav.site/persuasion.html">elsewhere</a>.</p>
<h3 id="How_Big_of_a_Problem_Is_It"><a class="hanchor" href="#How_Big_of_a_Problem_Is_It">How Big of a Problem Is It</a></h3>
<p>Ignoring persuasion as a means that AIs could use for take-over, how
disvaluable would worlds be in which human preferences are being shaped
by well-intentioned AIs?</p>
<p>My intuition is that it wouldn't be as bad as the proverbial
<a href="https://arbital.com/p/paperclip_maximizer/">paperclip maximizer</a>
(to which one can assign a value of zero). Would it be better than
a world in which humans never build superintelligence? If humans
are too distracted by superstimuli to settle and use the <a href="https://arbital.com/p/cosmic_endowment/">cosmic
endowment</a>, then we'd be better
off not building AIs which are powerful enough to influence malleable
human minds.</p>
<p>Two considerations that flow into how much worse futures with these
kinds of influenced human minds would be are (1) how malleable human
minds are, and (2) the classic consideration about <a href="https://www.lesswrong.com/posts/xzFQp7bmkoKfnae9R">fragility of
values</a>.</p>
<p>In general, influenceable human minds are a risk factor for not reaching
near-optimal futures, and the standard questions on the ease of reaching
near-optimal futures apply, e.g. whether it is sufficient for a small
part of the universe to be near-optimal so that the whole future is
near-optimal.</p>
<p>If more time available: Try to build a
<a href="https://forum.effectivealtruism.org/posts/t6FA9kGsJsEQMDExt/what-is-estimational-programming-squiggle-in-context">squiggle</a>
model that tries to estimate the disvalue compared to (1) paperclipper,
(2) near-optimal future for a single human who has grabbed power, (3)
near-optimal future.</p>
<h4 id="Current_Evidence"><a class="hanchor" href="#Current_Evidence">Current Evidence</a></h4>
<p>Events that are somewhat indicative of strange human-AI interactions
that also somewhat steer the human:</p>
<ol>
<li>Recommender systems causing their users to be addicted or otherwise changed, see also <a href="https://forum.effectivealtruism.org/posts/xzjQvqDYahigHcwgQ/aligning-recommender-systems-as-cause-area">Aligning Recommender Systems as Cause Area (Ivan Vendrov/Jeremy Nixon, 2019)</a>
<ol>
<li>I haven't looked into this much, how overblown are e.g. polarization worries?</li>
</ol></li>
<li><a href="https://openai.com/index/sycophancy-in-gpt-4o/">Sycophancy in GPT-4o: what happened and what we’re doing about it (OpenAI, 2025)</a>, though it's unclear how much this is related to training on human feedback vs. mild misalignment on the side of the models. My guess is that the lack of continual learning by AIs also reduces the amount they change their users.
<ol>
<li>Plausibly related: <a href="https://www.lesswrong.com/posts/6ZnznCaTcbGYsCmqu/the-rise-of-parasitic-ai">The Rise of Parasitic AI (Adele Lopez, 2025)</a>.</li>
</ol></li>
<li><em>Anecdotal &amp; subjective evidence</em>: My guess is that my interactions with Claude over the past 1⅔ years have slightly changed the way I write, and maybe the way I interact with other people. (I think I've caught myself saying "You're absolutely right" a few times in conversations with people :-D). I'd guess that change has been small and positive.</li>
</ol>
<h3 id="CounterArguments"><a class="hanchor" href="#CounterArguments">Counter-Arguments</a></h3>
<p><strong>Intent-alignment is a strong enough attractor that this is solved
"automatically"</strong>: AIs will have learned a latent generator of what
ultimately endorsed and unendorsed modifications to human minds are,
and try to follow what these generators specify. Alternatively, at least
the modifications ultimately judged as very disvaluable are excluded,
and the remaining possible modifications to humans are benign enough to
still be very valuable.</p>
<p>This might not work if this generator can't be learned from the training
data because humans haven't made enough philosophical progress so that
it is present in the training data.</p>
<p>Reinforcement learning on human feedback won't learn which modifications
are acceptable and which aren't<sub>80%</sub>.</p>
<p><strong>The Good is already represented enough in the training data or the model
spec for this to be solved automatically</strong>: All the actions implied by
the model spec point at what is morally good, and AIs will enact that.
This is a stronger claim since it assumes some variant of moral realism?</p>
<p><strong>Value change≠Corruption</strong>: Humans already accept a lot of
modifications to their values, so it could be that most of the
modifications that will fine-ish. Similarly, it could be that
humans should be indifferent to most modifications to their values.</p>
<p><strong>Humans are already hardened against value corruption</strong>: Humans are
exposed to tons of propaganda, persuasion attempts, advertising,
with people very much trying to change their values, yet this doesn't
happen very often, especially with untargeted attempts.</p>
<p>Although: It seems to me that humans mostly do form their opinions
based on their long-term interactions with other humans, though maybe
this mostly happens during a formative period in the teens and twenties.</p>
<p>Question: When do humans form their values, do they "lock in" after some point in their life?</p>
<h3 id="How_Malleable_Are_Human_Minds"><a class="hanchor" href="#How_Malleable_Are_Human_Minds">How Malleable Are Human Minds</a></h3>
<p>Human preferences seem to be distributed around the brain (<a href="https://pubmed.ncbi.nlm.nih.gov/34060875/" title="The case against economic values in the orbitofrontal cortex (or anywhere else in the brain)">Hayden &amp; Niv
2021</a>)
and built using learned representations (<a href="https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX/p/CQAMdzA4MZEhNRtTp">(TurnTrout
2022)</a>).</p>
<p>The human brain <a href="https://en.wikipedia.org/wiki/List_of_animals_by_number_of_neurons">has 8.7×10¹⁰
neurons</a>,
<a href="https://aiimpacts.org/scale-of-the-human-brain/">10¹⁴ synapses</a>,
encoding ~26 distinguishable states per synapse (which then ballparks the
information content at <em>very roughly</em> to 10TB-100TB), takes in ~10⁷
bits per second (10⁷ of those visual, 10⁶ of those proprioceptive,
10⁴ auditory, the rest via other modalities), of which only 10-60 bits
go through attention (<a href="https://arxiv.org/pdf/2408.10234">Zheng &amp; Meister 2024</a>).</p>
<p>One can use this to estimate a very rough lower bound on how quickly
human brains could be overwritten (ignoring consolidation and many other
factors): [8*10¹³, 8*10¹⁴] bits/(10⁸ bits/second)≈[10, 100]
days.  If the information is limited to what passes through attention
it'd take [~250k, 2.5M] years.</p>
<p>Relevant factual questions:</p>
<ol>
<li>How much perceived information is consolidated how quickly?
<ol>
<li>Claude 4.5 Sonnet tells me we don't have a great idea of how much of the perceived information is consolidated per second, and most research on this is on how memory is consolidated, not the amount.</li>
</ol></li>
<li>Does most of the perceived information have to pass through attention to be consolidated, or is unconsciously perceived information consolidated as well? How much?</li>
<li>How much are different parts of brain "fire-walled" from having processed information be consolidated in them?</li>
<li>How uniformly throughout the brain are new patterns consolidated?</li>
</ol>
<h3 id="How_Might_We_Solve_It"><a class="hanchor" href="#How_Might_We_Solve_It">How Might We Solve It</a></h3>
<p>Ways to prevent these kinds of problems that arise when AIs interact
with malleable human minds:</p>
<ol>
<li><a href="https://www.lesswrong.com/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai#6__STEM_AI">STEM AI</a> that doesn't know much about humans/doesn't interact much with humans, and instead focuses purely on solving engineering problems with clear specifications. This way we'd have useful AIs that still don't need to interact with malleable human minds, humans reap many benefits of advanced AI, and humans take their time to engage in moral epistemology/value formation/philosophy on their own.
<ol>
<li>Downsides: Many useful applications such as therapy, geopolitics &amp; international coordination, augmentation with impact on psychological traits, everyday personal advice requires some knowledge of psychology and interaction with humans, and AI companies are less likely to leave those on the table (given how much they are currently focused on consumers and very interaction-heavy applications).</li>
<li>How much value would STEM AI leave on the table?</li>
<li>See also the <a href="https://www.aria.org.uk/programme-safeguarded-ai/">Safeguarded AI</a> program that also tries to address takeover risk.</li>
</ol></li>
<li>Solve moral epistemology? As in, find the correct reasoning procedure to discover what humans (should) want in the limit of reflection, in a form that is not itself malleable.
<ol>
<li>Downsides: Humans don't seem to have made enough progress in philosophy for a solution to moral epistemology, so this looks intractable.</li>
<li>See also the <a href="https://www.lesswrong.com/posts/52ygLry5KCdvxY6zn/essay-competition-on-the-automation-of-wisdom-and-philosophy">Automation of Philosophy and Wisdom</a> research agenda.</li>
</ol></li>
<li>Empower a principal similar to newer conceptualizations of <a href="https://niplav.site/doc/cs/ai/alignment/corrigibility/corrigibility_soares_et_al_2015.pdf">corrigibility</a> as I understand the newer research program by Max Harms to focus on. Focusing on such empowerment would at least prevent enfeeblement<sup id="fnref2"><a href="#fn2" rel="footnote">2</a></sup> situations.
<ol>
<li>Downsides: This may not prevent situations where empowering the principal modifies them and their values in ways that are not optimal (and plausibly not endorsed upon sufficient reflection by the unmodified principal).</li>
</ol></li>
<li>More focus on human augmentation, especially around <a href="https://tsvibt.blogspot.com/2024/10/overview-of-strong-human-intelligence.html">cognitive</a> and moral enhancement (if that is possible?).</li>
</ol>
<h3 id="Related_Research"><a class="hanchor" href="#Related_Research">Related Research</a></h3>
<ul>
<li><a href="https://arxiv.org/pdf/2405.17713?">AI Alignment with Changing and Influenceable Reward Functions (Micah Carroll/Davis Foote/Anand Siththaranjan/Stuart Russell/Anca Dragan, 2024)</a></li>
<li><a href="./doc/cs/ai/alignment/cirl/cooperative_inverse_reinforcement_learning_hadfield_menell_et_al_2016.pdf">Cooperative Inverse Reinforcement Learning (Dylan Hadfield-Menell/Anca Dragan/Pieter Abbeel/Stuart Russell, 2016)</a>, somewhat.</li>
<li><a href="https://niplav.site/doc/cs/ai/alignment/cev/coherent_extrapolated_volition_yudkowsky_2004.pdf">Coherent Extrapolated Volition (Eliezer Yudkowsky, 2004)</a></li>
</ul>
<div class="footnotes">
<hr>
<ol>
<li id="fn1">
<p>By some global yardstick, such as how many resources they would have to expend relative to how many they have available.&nbsp;<a href="#fnref1" rev="footnote">↩</a></p>
</li>
<li id="fn2">
<p>Term from “Human Compatible (Stuart Russell, 2019)”; describes situations in which humans have given up on influencing the future and let AIs take care of them.&nbsp;<a href="#fnref2" rev="footnote">↩</a></p>
</li>
</ol>
</div>
</body></html>