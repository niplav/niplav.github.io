<html><head><title>niplav</title>
<link href="./favicon.png" rel="shortcut icon" type="image/png"/>
<link href="main.css" rel="stylesheet" type="text/css"/>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<!DOCTYPE HTML>

<style type="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
</style>
<script async="" src="./mathjax/latest.js?config=TeX-MML-AM_CHTML" type="text/javascript">
</script>
<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	extensions: ["tex2jax.js"],
	jax: ["input/TeX", "output/HTML-CSS"],
	tex2jax: {
		inlineMath: [ ['$','$'], ["\\(","\\)"] ],
		displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
		processEscapes: true,
		skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
	},
	"HTML-CSS": { availableFonts: ["TeX"] }
	});
</script>
<script>
document.addEventListener('DOMContentLoaded', function () {
	// Change the title to the h1 header
	var title = document.querySelector('h1')
	if(title) {
		var title_elem = document.querySelector('title')
		title_elem.textContent=title.textContent + " – niplav"
	}
});
</script>
</head><body><h2 id="home"><a href="./index.html">home</a></h2>
<p><em>author: niplav, created: 2025-02-27, modified: 2025-03-21, language: english, status: notes, importance: 1, confidence: joke</em></p>
<blockquote>
<p><strong>New philosophical position/Effective Altruism cause area
dropped. Benefits of pursuing the creation of libertarian free will
likely not worth the costs, based on a Monte-Carlo estimate.</strong></p>
</blockquote><div class="toc"><div class="toc-title">Contents</div><ul><li><a href="#Fermi_CostBenefit_Estimate">Fermi Cost-Benefit Estimate</a><ul><li><a href="#Tractability">Tractability</a><ul></ul></li><li><a href="#Importance">Importance</a><ul></ul></li><li><a href="#Neglectedness">Neglectedness</a><ul></ul></li></ul></li><li><a href="#Risks">Risks</a><ul></ul></li><li><a href="#Other_Ontological_Entities_It_Would_Potentially_Be_Good_To_Create">Other Ontological Entities It Would Potentially Be Good To Create</a><ul></ul></li><li><a href="#See_Also">See Also</a><ul></ul></li></ul></div>
<h1 id="Creating_Libertarian_Free_Will"><a class="hanchor" href="#Creating_Libertarian_Free_Will">Creating Libertarian Free Will</a></h1>
<p>The philosophical literature is rich in discussions about <a href="https://en.wikipedia.org/wiki/Free-Will">free
will</a>: Whether beings (usually
humans<sup id="fnref1"><a href="#fn1" rel="footnote">1</a></sup>) can make choices that are not, in some sense, "completely
determined" by the causal forces of nature. I won't take
any position in that debate here, but instead will ask a
far less explored question:</p>
<p><strong>If <a href="https://en.wikipedia.org/wiki/Libertarian_free_will">libertarian free
will</a> <em>does not</em>
exist, would it be good to bring it into existence?</strong></p>
<p>Clearly, humans value freedom in various:
<a href="https://en.wikipedia.org/wiki/Libertarianism">Libertarianism</a> makes
it the central tenet of its political philosophy, mundane forms of
unfreedom (slavery, imprisonement, social pressure &amp;c) are usually
regarded as undesirable, at least in WEIRD morality; the <a href="https://en.wikipedia.org/wiki/Capability_Approach">capability
approach</a> and the
<a href="https://en.wikipedia.org/wiki/Will_To_Power">will to power</a> both take
as central object the value of expanded actions and some theories of
life and artificial intelligence take expanded empowerment/control over
the environment as <em>the</em> feature defining successful existence.</p>
<p>Therefore, it's just natural to extend the desire for an expanded
action space to the ontological level; if we could, would we
not want to be able to act <em>counter</em> to the laws of physics,
peskily constraining each of us to a single future? (<a href="https://en.wikipedia.org/wiki/Many-Worlds_Interpretation">Many
worlds</a>
notwithstanding, since it also produces a pre-defined multiverse, and
<a href="https://en.wikipedia.org/wiki/Quantum_randomness">quantum randomness</a>
(if it exists) also does not count, as outlined in the debate around
the existence of libertarian free will.)</p>
<p>One can call this the <strong>constructive axiological free will hypothesis</strong>
(CAFWH): "<em>If libertarian free will doesn't (yet) exist, it would be
good to create it and imbue humans with it</em>".</p>
<h2 id="Fermi_CostBenefit_Estimate"><a class="hanchor" href="#Fermi_CostBenefit_Estimate">Fermi Cost-Benefit Estimate</a></h2>
<p>The ITN framework<!--TODO: link--> is ready-to-hand for evaluating whether
working on the CAWFH is a good idea.</p>
<h3 id="Tractability"><a class="hanchor" href="#Tractability">Tractability</a></h3>
<p>Starting with the weakest point. It is currently not clear how to bring
about new ontological entities. One can weakly estimate the cost from
taking the number of basic ontological categories in existence and
dividing them by the work that was involved in bringing them about.</p>
<p>As for basic ontological categories,
my estimate ranges from 0 (<a href="https://en.wikipedia.org/wiki/Ontological_nihilism">ontological
nihilism</a>)
to ~10 (list: mathematical objects, God and/or gods, qualia,
<a href="https://en.wikipedia.org/wiki/Matter-energy_relation">matterenergy</a>,
<a href="https://en.wikipedia.org/wiki/Spacetime">spacetime</a>, <a href="https://en.wikipedia.org/wiki/Abstract_object">abstract
objects</a>,
<a href="https://en.wikipedia.org/wiki/Soul">souls</a>, <a href="https://en.wikipedia.org/wiki/Moral_Realism">moral
facts</a> and likely others
I have forgotten to consider).</p>
<p>The total work needed to bring those about is difficult to estimate, as
it may range from no work (in the case of ontological nihilism) up to
<a href="https://en.wikipedia.org/wiki/Absolute_Infinite">absolute infinity</a>
if supernatural beings created existence. I'll take as a mean the
matter-energy content of the observable universe multiplied by four
(since matter-energy and spacetime are only two of the eight basic
ontological categories listed above).</p>
<p>The amount of <a href="https://en.wikipedia.org/wiki/Baryonic_matter">baryonic
matter</a>
in the observable universe is <a href="https://en.wikipedia.org/wiki/Observable_Universe">estimated
at</a> ~<code>$10^{53}$</code> kg,
but since baryonic matter only makes up 4.5% of the total mass-energy
in the universe, I'll adjust the estimate (not leaving out dark energy &amp;
dark matter).</p>
<p>Using <a href="https://github.com/rethinkpriorities/squigglepy">squigglepy</a>:</p>
<pre><code>import squigglepy as sq
num_ontological_categories=sq.to(0, 10)
cost_new_entity=sq.lognorm(lognorm_mean=(1/0.045)*10**53, lognorm_sd=20)
prop_universe_entities=num_ontological_categories/sq.to(1, 3)
new_entity_cost=cost_new_entity/prop_universe_entities
</code></pre>
<p>Usually, a new ontological entity costs the mass-energy of <code>$10^{53}$</code> kg:</p>
<pre><code>&gt;&gt;&gt; sq.sample(new_entity_cost, 10)
array([2.96996124e+53, 7.18535888e+53, 2.98417566e+54, 1.11103257e+54,
       6.09266737e+53, 8.39602018e+53, 4.08215136e+53, 5.01485048e+53,
       4.63852243e+53, 3.93717114e+53])
</code></pre>
<h3 id="Importance"><a class="hanchor" href="#Importance">Importance</a></h3>
<p>It would likely be valuable to create libertarian free will. (Though
see the list of <a href="#Risks">possible risks</a> below.)</p>
<p>As a proxy, one can try to estimate how much time and energy humans
expand on broadening the list of possible choices available to them;
examples include education, migration to more democratic and liberal
countries, buying transport, many health interventions &amp;c. Philosophical
intuition points me to ~10% of human effort being spent on pure expansion
of mundane freedoms.</p>
<p>Assuming that libertarian free will would be ~5× more valuable, and a
~20% chance that humans already have libertarian free will, together with
a <a href="https://en.wikipedia.org/wiki/Gross_World_Product">gross world product</a>
of ~$100T, we can arrive at the value humanity should be willing to
pay for libertarian free will:</p>
<pre><code>gwp=sq.norm(mean=10**14, sd=1)
prop_spent_on_freedom=sq.beta(a=2, b=8)
real_freewill_mult=sq.to(4, 20)
chance_freewill_exists=sq.beta(a=1, b=4)
total_value=prop_spent_on_freedom*gwp*real_freewill_mult*(1-chance_freewill_exists)
</code></pre>
<p>Sampling, we usually get a few tens of trillions of dollars in value:</p>
<pre><code>&gt;&gt;&gt; sq.sample(total_value, 10)
array([9.96797785e+13, 4.23572584e+13, 8.24106658e+13, 4.32935165e+14,
       2.43818602e+14, 1.26017517e+14, 1.86109674e+14, 1.43918708e+14,
       4.01545018e+13, 2.40841431e+14])
</code></pre>
<h3 id="Neglectedness"><a class="hanchor" href="#Neglectedness">Neglectedness</a></h3>
<p>As far as I know, no being is working on creating libertarian free will
de novo, and philosophers have not yet discussed the possibility and
desirability<sup id="fnref2"><a href="#fn2" rel="footnote">2</a></sup>. It is possible that hidden supernatural entities are
engaged in the process, but that seems unlikely<sub>4%</sub>, and it's
not clear they would imbue humans specifically with the capability if
they created it.</p>
<hr/>
<p>Unfortunately, it seems like the cost for new ontological basic entities
is too high: It is implausible that we will be able to bring up the
equivalent of ~<code>$10^{53}-10^{54}$</code> kg of massenergy with an investment
of ~$<code>$10^{13}-10^{14}$</code>.</p>
<p>However, due to the small amount of thought that has gone into ontological
engineering, there is the potential that creating new ontological
categories is much cheaper than estimated here, and the neglectedness
leaves space for a few experimental philosophers.</p>
<h2 id="Risks"><a class="hanchor" href="#Risks">Risks</a></h2>
<p>Drastic changes to the structure of existence are not without their
risks. Creating libertarian free will may induce multiple hazards:</p>
<ol>
<li><em>Choosing evil</em>: Imbuing humans with free will may cause them to deliberately choose evil (instead of simply being compelled to by the laws of physics), leaving them uniquely culpable of their harmful deeds.</li>
<li><em>Making the world less predictable</em>: Free agents interacting with the world may make it less stable and predictable, leading to overall lower welfare.</li>
<li><em>Being smitten by God for choosing evil</em>: If God or other supernatural beings have a plan that involves Him or them keeping control over which entities have free will, or if entities are only smitten if they actively choose evil, then giving humans free will may cause God or gods to smite humans, potentially even as a collective. Thus introducing libertarian free will poses a small existential risk.</li>
</ol>
<h2 id="Other_Ontological_Entities_It_Would_Potentially_Be_Good_To_Create"><a class="hanchor" href="#Other_Ontological_Entities_It_Would_Potentially_Be_Good_To_Create">Other Ontological Entities It Would Potentially Be Good To Create</a></h2>
<ul>
<li>Moral Facts (…)</li>
<li>God, which would pose novel alignment challenges.</li>
<li><a href="https://en.wikipedia.org/wiki/Dasein">Dasein</a></li>
<li>Qualia</li>
<li><a href="https://en.wikipedia.org/wiki/Hypercomputation">Hypercomputation</a>, if it is not already possible via <a href="https://en.wikipedia.org/wiki/Malament-Hogarth_spacetime">Malament-Hogarth spacetimes</a>.</li>
</ul>
<h2 id="See_Also"><a class="hanchor" href="#See_Also">See Also</a></h2>
<ul>
<li><a href="http://bactra.org/notebooks/some-meta-ethical-positions.html">Some Unattractive Meta-Ethical Positions, Free to a Good Home (Cosma Shalizi, 2024)</a></li>
<li><a href="https://schwitzsplinters.blogspot.com/2025/02/zombie-is-to-human-as-human-is-to-xxx_26.html">Zombie is to Human as Human is to XXX? (Eric Schwitzgebel, 2025)</a></li>
</ul>
<div class="footnotes">
<hr/>
<ol>
<li id="fn1">
<p>Though even here we may consider possibilities similar to the ones <a href="http://bactra.org/notebooks/some-meta-ethical-positions.html">Shalizi</a> discusses: What if free will is possible, but only a few species in the past had it, or only a few species in the future will have it, but not <em>homo sapiens</em>? What if there are inanimate objects that <em>have</em> free will, but choose not to exercise it? What if some humans<!--TODO: link Aaronson anecdote--> have free will, but others don't, or if all humans have free will as a latent ability, but have failed to notice and deploy it? How about the hypothesis that figuring out whether something/someone has free will is <a href="https://en.wikipedia.org/wiki/Undecidable">undecidable</a>, or at least <a href="https://en.wikipedia.org/wiki/EXPTIME-complete">EXPTIME-complete</a>? Humans may not have free will themselves, but be theoretically able to create beings with free will… it's just that, by the way the universe is structured, we never will. <a href="#fnref1" rev="footnote">↩</a></p>
</li>
<li id="fn2">
<p>Based on a short websearch using Google Scholar &amp; Perplexity, and a conversation with ChatGPT with web search enabled. <a href="#fnref2" rev="footnote">↩</a></p>
</li>
</ol>
</div>
</body></html>
