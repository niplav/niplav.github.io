
<title>niplav</title>
<link rel="shortcut icon" type="image/png" href="./favicon.png">
<link rel="stylesheet" type="text/css" href="main.css">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<!DOCTYPE HTML>

<style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
</style>

<script type="text/javascript" async
	src="./mathjax/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	extensions: ["tex2jax.js"],
	jax: ["input/TeX", "output/HTML-CSS"],
	tex2jax: {
		inlineMath: [ ['$','$'], ["\\(","\\)"] ],
		displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
		processEscapes: true,
		skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
	},
	"HTML-CSS": { availableFonts: ["TeX"] }
	});
</script>

<script>
var anchorhash={}
function addAnchor(element) {
	var cnt=element.textContent
	if(cnt==="home")
		return;
	var ref=element.textContent.replace(/[^a-zA-Z0-9 ]/mg, "")
	ref=ref.replace(/ /mg, "-")
	var newref=ref;
	if(anchorhash[ref]===1)
		for(i=1, newref=ref+"_"+i;anchorhash[ref+"_"+i]===1;i++, newref=ref+"_"+i)
			;
	ref=newref
	element.setAttribute("id", `${ref}`)
	element.innerHTML=`<a href="#${ref}" class="hanchor">${cnt}</a>`
	anchorhash[ref]=1
}
document.addEventListener('DOMContentLoaded', function () {
	// Add anchor links to all headings
	var headers = document.querySelectorAll('h1, h2, h3, h4, h5, h6')
	if (headers) {
		headers.forEach(addAnchor)
	}
	// Change the title to the h1 header
	var title = document.querySelector('h1')
	if(title) {
		var title_elem = document.querySelector('title')
		title_elem.textContent=title.textContent + " – niplav"
	}
});
</script>

<h2><a href="./index.html">home</a></h2>

<p><em>author: niplav, created: 2021-08-17, modified: 2021-08-21, language: english, status: draft, importance: 6, confidence: possible</em></p>

<blockquote>
  <p><strong>.</strong></p>
</blockquote>

<h1>Brain-Computer Interfaces and AI Alignment</h1>

<h2>Epistemic Status</h2>

<p>I am neither a neuroscientist nor an AI alignment researcher (although I
have read some blogposts about the latter), and I know very little about
brain-computer interfaces (from now on abbreviated as “BCIs”). I have
done a cursory internet search for a resource laying out the case for the
utility of BCIs in AI alignment, but haven't been able to find anything
that satisfies my standards (I have also asked on the LessWrong open
thread and the AI alignment channel on the Eleuther AI discord channel,
and not gotten any answers that provide such a resource (although I was
told some useful arguments)).</p>

<p>I have tried to make the best case for and against BCIs,
stating some tree of arguments that I think many AI alignment
researchers tacitly believe, mostly taking the Bostrom/Yudkowsky
story of AI risk (although it might be generalizable to a
<a href="https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like
" title="What failure looks like">Christiano-like</a> story; I don't know enough about CAIS or
ARCHES to make a judgment about the applicability of the arguments).</p>

<h2>Arguments For the Utility of Brain-Computer Interfaces in AI Alignment</h2>

<h3>Improving Human Cognition</h3>

<p>Just as writing or computers have improved the quality and speed of
human cognition, BCIs could improve the quality and the speed of human
thinking. These advantages could arise out of several different advantages
of BCIs over traditional perception:</p>

<ul>
<li>Quick lookup of facts (e.g. querying Wikipedia while in a conversation)</li>
<li>Augmented long-term memory (with more reliable and resilient memory freeing up capacity for thought)</li>
<li>Augmented working memory (i.e. holding 11±2 instead of 7±2 items in mind at the same time) (thanks to janus#0150 for this point)</li>
<li>Exchange of mental models (instead of explaining a complicated model, one would be able to simply “send” the model to another person, saving a lot of time explaining)</li>
<li>Outsourcing simple cognitive tasks to external computers</li>
</ul>

<!--TODO: how much more intelligent? How much faster?-->

<h3>Understanding the Human Brain</h3>

<p>Neuroscience seems to be blocked by not having good access to human
brains while they are alive, and would benefit from shorter feedback
loops and better data. A better understanding of the human brain might
be quite useful in e.g. finding the location of human values in the brain
(even though it seems like there is no one such location <!--TODO: link to
the downloaded paper-->). Similarly, a better understanding of the human
brain might aid in better understanding and interpreting neural networks.</p>

<h4>Path Towards Whole-Brain Emulation or Human Imitation</h4>

<p>Whole-brain emulation (henceforth WBE) (with the emulations being faster
or cheaper to run than physical humans) would likely be useful for AI
alignment if used differentially for alignment over capabilities – human
WBEs would to a large part share human values, and could subjectively
slow down timelines while searching for AI alignment solutions. Fast
progress in BCIs could make WBEs more likely before an AI <a href="https://www.lesswrong.com/posts/JPan54R525D68NoEt/the-date-of-ai-takeover-is-not-the-day-the-ai-takes-over" title="The date of AI Takeover is not the day the AI takes over">point of no
return</a>
by improving the understanding of the human brain.</p>

<p>A similar but weaker argument would apply to
<a href="https://www.alignmentforum.org/posts/LTFaD96D9kWuTibWr/just-imitate-humans" title="Just Imitate Humans?">Ai systems that imitate human behavior</a>.</p>

<h3>“Merging” AI Systems With Humans</h3>

<p>A notion often brought forward in the context of BCIs and AI alignment
is the one of “merging” humans and AI systems<!--TODO: [citation
neeeded]-->.</p>

<p>Unfortunately, a clearer explanation of how exactly this would work or
help with making AI go well is usually not provided (at least I haven't
managed to find any clear explanation). There are different possible
ways of conceiving of humans “merging” with AI systems: using human
values/cognition/policies as partial input to the AI system.</p>

<h4>Input of Values</h4>

<p>The most straightforward method of merging AI systems and humans could be
to use humans outfitted with BCIs as part of the reward function of an AI
system. In this case, a human would be presented with a set of outcomes
by an AI system, and would then signal how desirable that outcome would
be from the human's perspective. The AI would then search for ways to
reach the states rated highest by the human with the largest probability.</p>

<p>If one were able to find parts of the human brain that hold the human
utility function, one could use these directly as parts of the AI systems.
However, it seems unlikely that the human brain has a clear notion of
terminal values distinct from instrumental values and policies<!--TODO:
link the case against economic values in the brain--> in a form that
could be used by an AI system.</p>

<h5>Easier Approval-Directed AI Systems</h5>

<p>Additionally, a human connected to an AI system via a BCI would
have an easier time evaluating the cognition of <a href="https://www.lesswrong.com/s/EmDuGeRw749sD3GKd/p/7Hr8t6xwuuxBTqADK
" title="Approval-directed agents">approval-directed
agents</a>, since they might be able
to follow he cognition of the AI system in real-time,
and spot undesirable thought processes (akin to <a href="https://arbital.com/p/cognitive_steganography/" title="Cognitive steganography">cognitive
steganography</a>).</p>

<h4>Input of Cognition</h4>

<p>Related to the aspect of augmenting humans using BCIs by outsourcing
parts of cognition, the inverse is also possible: identifying modules
of AI systems that are most likely to be misaligned to humans or produce
such misalignment, and replacing them with human cognition.</p>

<p>For example the part of the AI system that formulates long-term plans
could be most likely to be engaged in formulating misaligned plans,
and the AI system could be made more myopic by replacing the long-term
planning modules by humans, while short-term planning would be left to
AI systems.</p>

<p>Alternatively, if humanity decides it wants to prevent AI systems from
forming <a href="https://www.lesswrong.com/posts/BKjJJH2cRpJcAnP7T">human models</a>,
modeling humans &amp; societies could be outsourced to actual humans, whose
human models would be used by the AI systems.</p>

<h4>Input of Policies</h4>

<h3>Side-note: A Spectrum from Humans to Human Imitations</h3>

<p>There seems to be a spectrum from biological humans to human imitations,
roughly along the axes of integration with digital systems/speed/:
Biological humans – humans with BCIs – whole-brain emulations –
human imitations. This spectrum also partially tracks how aligned
these human-like systems can be expected to act: a human imitation
off-distribution seems much less trustworthy than a whole-brain emulation
of a human.</p>

<h2>Arguments Against the Utility of Brain-Computer Interfaces in AI Alignment</h2>

<h3>Possibly Unaligned Systems Have Direct Access to Human Brains</h3>

<h3>Opportunity Cost</h3>

<p>If work on BCIs is net-positive in expectation for making AI go well,
it might be the case that other approaches are more promising, and that
focusing on BCIS might leave those approaches underdeveloped.</p>

<p>For example, one can posit neural network interpretability as the
<a href="https://www.givedirectly.org/">GiveDirectly</a> of AI alignment: reasonably
tractable, likely helpful in a large class of scenarios, with basically
unlimited scaling and only slowly diminishing returns. And just as any
new EA cause area must pass the first test of being more promising than
GiveDirectly, so every alignment approach could be viewed as a competitor
to interpretability work.</p>

<h3>“Merging” is Just Faster Interaction</h3>

<h3>Problems Arise with Superhuman Systems</h3>

<h4>Performance Problems</h4>

<h4>Transparency Problems</h4>

<h3>Speed Differences are Problematic</h3>

<h3>Successful Alignment via BCIs Depends on Knowing Details</h3>

<h2>Subjective Conclusion</h2>
</html>
