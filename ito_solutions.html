<html><head><title>niplav</title>
<link href="./favicon.png" rel="shortcut icon" type="image/png"/>
<link href="main.css" rel="stylesheet" type="text/css"/>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<!DOCTYPE HTML>

<style type="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
</style>
<script async="" src="./mathjax/latest.js?config=TeX-MML-AM_CHTML" type="text/javascript">
</script>
<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	extensions: ["tex2jax.js"],
	jax: ["input/TeX", "output/HTML-CSS"],
	tex2jax: {
		inlineMath: [ ['$','$'], ["\\(","\\)"] ],
		displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
		processEscapes: true,
		skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
	},
	"HTML-CSS": { availableFonts: ["TeX"] }
	});
</script>
<script>
document.addEventListener('DOMContentLoaded', function () {
	// Change the title to the h1 header
	var title = document.querySelector('h1')
	if(title) {
		var title_elem = document.querySelector('title')
		title_elem.textContent=title.textContent + " – niplav"
	}
});
</script>
</head><body><h2><a href="./index.html">home</a></h2>
<p><em>author: niplav, created: 2022-10-19, modified: 2022-11-08, language: english, status: in progress, importance: 2, confidence: likely</em></p>
<blockquote>
<p><strong>Solutions to the textbook “Introduction to Optimization”.</strong></p>
</blockquote>
<h1 id="Solutions_to_Introduction_to_Optimization"><a class="hanchor" href="#Solutions_to_Introduction_to_Optimization">Solutions to “Introduction to Optimization”</a></h1>
<h3 id="401"><a class="hanchor" href="#401">4.0.1</a></h3>
<p>See <a href="./mfis_solutions.html#24">here</a>.</p>
<h3 id="411"><a class="hanchor" href="#411">4.11</a></h3>
<h4 id="i"><a class="hanchor" href="#i">i)</a></h4>
<p>Some standard numpy code:</p>
<pre><code>import numpy as np

def givec(x, c=10):
    n=len(x)
    iexp=np.array(range(0,n))
    cdiag=c**(iexp/((n-1)*np.ones(n)))
    C=np.diag(cdiag)
    return C

def fsq(x, c=10):
    C=givec(x, c=c)
    return x.T@C@x

def fhole(x, c=10, a=0.1):
    retval=fsq(x, c=c)
    return retval/(a**2+retval)
</code></pre>
<p>The derivative of <code>$f_{\text{sq}}=x^{\top}Cx$</code> is <code>$(C+C^{\top})x$</code>
(as per identity 81 from the Matrix Cookbook (Petersen &amp; Pedersen
2012))<!--TODO: link-->.</p>
<p>Which is excellent, since the identity 2.3 of Maths for Intelligent
Systems (Toussaint 2022) gives
<code>$\frac{\partial}{\partial x} x^{\top}Cx=x^{\top}C \frac{\partial}{\partial x}x+x^{\top} C^{\top} \frac{\partial}{\partial x} x$</code>
which would be <code>$x^{\top}C+x^{\top}C^{\top}=C^{\top}x+Cx=(C^{\top}+C)x$</code>
and therefore the same as above (and in identity 97 from the Matrix
Cookbook).</p>
<p>So then</p>
<pre><code>def fsqgrad(x, c=10):
    C=givec(x, c=c)
    return (C.T+C)@x
</code></pre>
<p>Since <code>$f_{\text{sq}}$</code> and <code>$f_{\text{hole}}$</code> return a scalar, one can
use a simple <a href="https://en.wikipedia.org/wiki/Quotient_rule">quotient rule</a>:</p>
<div>
        $$f_{\text{hole}}(x)=\\
        \frac{(C^{\top}+C)x\cdot(a^2+x^{\top}Cx)-x^{\top}Cx\cdot (C^{\top}+C)x)}{(a^2+x^{\top}Cx)^2}= \\
         \frac{(C^{\top}+C)x\cdot(a^2+x^{\top}Cx-x^{\top}Cx)}{(a^2+x^{\top}Cx)^2}= \\
         \frac{a^2 \cdot (C^{\top}+C)x}{(a^2+x^{\top}Cx)^2}$$
</div>
<p>The implementation is straightforwardly</p>
<pre><code>def fholegrad(x, c=10, a=0.1):
    retval_fsq=fsq(x, c=c)
    retval_fsqgrad=fsqgrad(x, c=c, a=a)
    return (retval_fsqgrad*(a**2+retval_fsq)-retval_fsq*retval_fsqgrad)/((a**2+retval_fsq))**2
</code></pre>
<h4 id="ii"><a class="hanchor" href="#ii">ii)</a></h4>
<p>The function can be pretty directly translated from the pseudocode in
the book to Python (no surprise here):</p>
<pre><code>def graddesc(f, fgrad, alpha=0.001, theta=10e-20, x0=[1,1], c=10, a=0.1):
    x=np.array(x0)
    i=0
    prevcost=-math.inf
    curcost=f(x)
    while abs(prevcost-curcost)&gt;theta:
        print("#iteration: ", i)
        print("current cost: ", curcost)
        prevcost=curcost
        x=x-alpha*fgrad(x)
        curcost=f(x)
        i=i+1
    print("solution: ", x)
    print("iterations: ", i)
    print("cost: ", f(x))
    return x
</code></pre>
<p>Executing <code>graddesc</code> with <code>fsq</code>/<code>fsqgrad</code> and <code>fhole</code>/<code>fholegrad</code> gives
~1000 iterations for finding a local minimum with <code>fsq</code>, and takes ~185k
iterations for <code>fhole</code>. But it finds the minimum:</p>
<pre><code>print("fsq:")
graddesc(fsq, fsqgrad)
print("fhole:")
graddesc(fhole, fholegrad)

fsq:
solution:  [4.98354923e-09 1.65118900e-84]
iterations:  9549
cost:  2.4835762963261225e-17
fhole:
solution:  [ 4.13240000e-11 -7.98149419e-19]
iterations:  184068
cost:  1.7076729729684466e-19
</code></pre>
<h4 id="iii"><a class="hanchor" href="#iii">iii)</a></h4>
<pre><code>def backtrack_graddesc(f, fgrad, alpha=0.05, theta=10e-15, x0=[1,1], c=10, a=0.1, rho_ls=0.01, rho_plus=1.2, rho_minus=0.5, delta_max=math.inf):
    x=np.array(x0)
    i=0
    prevcost=-math.inf
    curcost=f(x)
    delta=-fgrad(x)/np.linalg.norm(fgrad(x))
    while np.linalg.norm(alpha*delta)&gt;=theta:
        while f(x+alpha*delta)&gt;f(x)+rho_ls*fgrad(x)@(alpha*delta).T:
            alpha=rho_minus*alpha
        prevcost=curcost
        x=x+alpha*delta
        alpha=min(rho_plus*alpha, delta_max)
        curcost=f(x)
        i=i+1
    return x
</code></pre>
<h4 id="iv"><a class="hanchor" href="#iv">iv)</a></h4>
<p>First we compute the derivative of <code>$\underset{β}{\text{argmin}}||y-Xβ||^2+λ||β||^2$</code>:</p>
<div>
    $$\frac{\partial ||y-Xβ||^2+λ||β||^2}{\partial β}= \\
    \frac{\partial ||y-Xβ||^2}{\partial β} + \frac{\partial λ||β||^2}{\partial β}= \\
    \frac{\partial ||-Xβ-(-y)||^2}{\partial β}+λ2β=\\
    2 \cdot \frac{-Xβ-(-y)}{||-Xβ-(-y)||}+λ2β$$
</div>
</body></html>
