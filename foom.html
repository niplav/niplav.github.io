<html><head><title>niplav</title>
<link href="./favicon.png" rel="shortcut icon" type="image/png"/>
<link href="main.css" rel="stylesheet" type="text/css"/>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<!DOCTYPE HTML>

<style type="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
</style>
<script async="" src="./mathjax/latest.js?config=TeX-MML-AM_CHTML" type="text/javascript">
</script>
<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	extensions: ["tex2jax.js"],
	jax: ["input/TeX", "output/HTML-CSS"],
	tex2jax: {
		inlineMath: [ ['$','$'], ["\\(","\\)"] ],
		displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
		processEscapes: true,
		skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
	},
	"HTML-CSS": { availableFonts: ["TeX"] }
	});
</script>
<script>
document.addEventListener('DOMContentLoaded', function () {
	// Change the title to the h1 header
	var title = document.querySelector('h1')
	if(title) {
		var title_elem = document.querySelector('title')
		title_elem.textContent=title.textContent + " – niplav"
	}
});
</script>
</head><body><h2 id="home"><a href="./index.html">home</a></h2>
<p><em>author: niplav, created: 2024-01-30, modified: 2024-01-30, language: english, status: in progress, importance: 3, confidence: certain</em></p>
<blockquote>
<p><strong>Some thoughts on discontinuous and/or fast takeoff during TAI
development, in response to a twitter question.</strong></p>
</blockquote>
<h1 id="On_Discontinuous_and_Fast_Takeoff"><a class="hanchor" href="#On_Discontinuous_and_Fast_Takeoff">On Discontinuous and Fast Takeoff</a></h1>
<p>The most fleshed-out model of AI takeoff is <a href="https://niplav.site/doc/cs/ai/alignment/takeoff/a_compute_centric_framework_of_takeoff_speeds_davidson_2023.pdf">Davidson
2023</a>,
which makes the median prediction of 20% automation to 100% automation
in ~3 years (10th percentile: 0.8 years, 90th percentile: 12.5 years).</p>
<p>Along the axes of {fast, slow}×{continuous, discountinuous}, that feels
quite fast to me, even if it isn't very discountinuous.</p>
<p>The other reasons make me move towards "but it might be a lot faster
than that". One reason is that the Davidson model assumes that the human
brain performs 10¹⁵ FLOP/s, and that the AI systems will be at most
that efficient or slightly less efficient. So a lot of my disagreement is
around: <strong>how much the ceiling of cognitive algorithms is above humans</strong>
(my belief: very high<sub>80%</sub>), and the rest of the disagreement
is <strong>how quickly can AI systems move towards that ceiling</strong> (my belief:
not sure, but potentially within days<sub>40%</sub>).</p>
<h2 id="Better_Algorithms_Exist"><a class="hanchor" href="#Better_Algorithms_Exist">Better Algorithms Exist</a></h2>
<p>One reason is that human brains don't seem like the optimal substrate
for performing cognition: Warm &amp; wet, very low information transmission
speed (signals on neurons are limited to at most 200 m/s) <a href="https://www.lesswrong.com/posts/HhWhaSzQr6xmBki8F/birds-brains-planes-and-ai-against-appeals-to-the-complexity">Kokotajlo
2021</a>,
needing to self-construct and self-repair — and
still brains are incredibly sample-efficient! And I
suspect that, if anything, humans are at best at a <a href="https://www.lesswrong.com/posts/yuP4D4Pz79uyPS9KW">subspace
optimum</a> of cognitive
algorithms.</p>
<p>Then there's the <em>power of error-corrected/discrete/serial computation</em>:
Digital computers can make very long inferences in discrete domains
without problems, and when I introspect, I have the strong intuition
that my system 2 tries to approximate this, especially when trying
to enumerate options in a decision, recursively decompose a plan
into its components (which gets much easier <a href="https://bmk.sh/2020/08/17/Building-AGI-Using-Language-Models/">once you have a world
model</a>),
perform abstraction (while caching which parts of the
abstraction are tight and which are leaky)—but my system 2 only has
<a href="https://en.wikipedia.org/wiki/The_Magical_Number_Seven,_Plus_or_Minus_Two">7±2</a>
(or maybe <a href="https://en.wikipedia.org/wiki/Working_Memory#Capacity">actually just
4</a>?) usable
slots. And unless the limit is due to combinatorial explosion (which
might be handleable by careful pruning, or prioritized search), AI
systems could have larger (perhaps vastly larger?) working memories.</p>
<p>The standard rejoinder here is that evolution has optimized human
brains really really hard, and our current technology is usually 2-6
orders of magnitude worse than what evolution has come up with<!--TODO:
find Christiano investigation into this-->. But if we believe that
error-corrected computation is quite rare in biology, then this opens
up a new niche to make progress in, similar to how there are no plants
in space because they couldn't evolve rocket-like tech and transparent
shells that were resistant enough in vacuum.</p>
<p>This points at an intuition I have: There is a bunch of α left
in combining error-corrected/discrete/serial computation (which
computers are good at) with error-resistant/continuous/parallel
computation (à la neural networks or brains). And especially if
I think about cognition through the lens of algorithms, it feels
like there's a <em>deep mine of algorithms</em>: The space of possible
algorithms is vast, and even in <em>very</em> simple problem domains we have
found surprising innovations (such as going from the <a href="https://en.wikipedia.org/wiki/Karatsuba_algorithm">Karatsuba
algorithm</a>
to the <a href="https://en.wikipedia.org/wiki/Sch%C3%B6nhage-Strassen_algorithm">Schönhage-Strassen
algorithm</a>,
or from the naive algorithm for the <a href="https://en.wikipedia.org/wiki/Maximum_Subarray_problem">maximum subarray
problem</a>
to Kadane's algorithm). My "optimism" here has been hindered
somewhat by some evidence on how well <a href="https://www.lesswrong.com/posts/J6gktpSgYoyq5q3Au/benchmarking-an-old-chess-engine-on-new-hardware">old chess algorithms perform on new
hardware</a>,
and the observation that the surprising algorithms we find are
usually galactic (such as in the case of the <a href="https://en.wikipedia.org/wiki/Computational_complexity_of_matrix_multiplication#Matrix_multiplication_exponent">decreasing shrinking
rate of the best-case exponent in the computational complexity of matrix
multiplication</a>—where
yet we still only use <a href="https://en.wikipedia.org/wiki/Strassen's_algorithm">Strassen's
algorithm</a>).</p>
<p>Additionally, there's some domains of computation of which we have
made little use, <em>because</em> our minds are limited in a way that makes
it difficult to think about them. As the adage goes, programming is
divided into four levels of difficulty: <code>if</code> statements, <code>while</code>
loops, <a href="https://en.wikipedia.org/wiki/Recursion">recursion</a> and
<a href="https://en.wikipedia.org/wiki/Parallelism_(computing)">parallelism</a>;
but what about domains like <a href="https://en.wikipedia.org/wiki/Self-modifying_code">self-modifying
code</a> (where, except
maybe <a href="https://en.wikipedia.org/wiki/G%C3%B6del_machine">Gödel machines</a>,
there is no respectable theory, and except <a href="https://en.wikipedia.org/wiki/Alexia_Massalin">Alexia
Massalin's</a>
<a href="https://en.wikipedia.org/wiki/Superoptimization">superoptimization</a> there
isn't really any application)? Although, to be fair, <a href="https://en.wikipedia.org/wiki/Neural_architecture_search">neural architecture
search</a> might
be getting there, sometime.</p>
<!--TODO: Additionally, people seem to have *forgotten about thinking*:-->
<p>My view on better algorithms existing is <em>not</em>
informed very much by <a href="https://www.lesswrong.com/posts/hvz9qjWyv8cLX9JJR/evolution-provides-no-evidence-for-the-sharp-left-turn">specific observations about
evolution</a>.</p>
<h2 id="Better_Algorithms_are_Quickly_Reachable"><a class="hanchor" href="#Better_Algorithms_are_Quickly_Reachable">Better Algorithms are Quickly Reachable</a></h2>
<p>As in the section about better algorithms existing, many of my intuitions
here come from algorithm design and/or regular software engineering.</p>
<p>One argument against discountinuous takeoff is a response
to the hypothesis of recursive self-improvement, in
which AI systems start finding improvements to their own
architectures more and more quickly (which I try to model
<a href="./toy_ai_takeoff_model.html">here</a>). The counterargument says that
before there will be AI systems that are really good at self-improvement,
there <a href="https://sideways-view.com/2018/02/24/takeoff-speeds/">will be systems that are first crappy and then merely okay at
self-improvement</a>.<!--TODO:
link page that collects examples of these in current ML?--></p>
<p>But usually, with algorithms, having a 99%-finished implementation of the
algorithm doesn't give you 99% of the benefit, nor does it give you 50%
or even 1% of the benefit. It simply doesn't work. And here intuitions
collide: I find it plausible that, in this case, the <a href="https://www.lesswrong.com/posts/xkRtegmqL2iyhtDB3/the-gods-of-straight-lines">The Gods of Straight
Lines</a>
do not interfere, and instead something far stranger is afoot, but
the machine learning intuition tells people that everything in neural
networks is continuous, so why wouldn't there be a continous path to a
TAI architecture?<!--TODO: link continuity assumption post by Kulveit?--></p>
</body></html>
