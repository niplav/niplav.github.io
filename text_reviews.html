<html><head><title>niplav</title>
<link href="./favicon.png" rel="shortcut icon" type="image/png"/>
<link href="main.css" rel="stylesheet" type="text/css"/>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<!DOCTYPE HTML>

<style type="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
</style>
<script async="" src="./mathjax/latest.js?config=TeX-MML-AM_CHTML" type="text/javascript">
</script>
<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	extensions: ["tex2jax.js"],
	jax: ["input/TeX", "output/HTML-CSS"],
	tex2jax: {
		inlineMath: [ ['$','$'], ["\\(","\\)"] ],
		displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
		processEscapes: true,
		skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
	},
	"HTML-CSS": { availableFonts: ["TeX"] }
	});
</script>
<script>
document.addEventListener('DOMContentLoaded', function () {
	// Change the title to the h1 header
	var title = document.querySelector('h1')
	if(title) {
		var title_elem = document.querySelector('title')
		title_elem.textContent=title.textContent + " – niplav"
	}
});
</script>
</head><body><h2 id="home"><a href="./index.html">home</a></h2>
<p><em>author: niplav, created: 2020-12-30, modified: 2023-07-23, language: english, status: in progress, importance: 3, confidence: opinion</em></p>
<blockquote>
<p><strong>Reading a text is sometimes a big time investment, but people
don't approach their reading in a structured manner, e.g. by keeping
notes, making flashcards, doing exercises, writing reviews or making
summaries. I have been taking notes on the books I read since mid 2017,
but have neglected writing reviews or summaries that might be useful to
others. This is my attempt at salvaging that oversight.</strong></p>
</blockquote>
<h1 id="Text_Reviews"><a class="hanchor" href="#Text_Reviews">Text Reviews</a></h1>
<h2 id="Book_Reviews"><a class="hanchor" href="#Book_Reviews">Book Reviews</a></h2>
<h3 id="Compassion_by_the_Pound_F_Bailey_NorwoodJayson_L_Lusk_2011"><a class="hanchor" href="#Compassion_by_the_Pound_F_Bailey_NorwoodJayson_L_Lusk_2011">Compassion, by the Pound (F. Bailey Norwood/Jayson L. Lusk, 2011)</a></h3>
<p>“Compassion, by the Pound” by the economists F. Bailey Norwood and
Jayson L. Lusk is one of those books that are excellent in their first
half, and somewhat (but not utterly) disappointing in their second
half. The two economists, spurred by their own research and their
perceived lack of good information on the topic of farm animal welfare,
start off with a historical overview of animal agriculture and animal
welfare activism, proceed to talking about the sentience of animals,
give an enormous overview of standard animal agriculture praxis, push in
two completely unnecessary chapters about how philosophers and economists
see animal agriculture (the first one being massively oversimplifying,
the second one being annoyed), present a model for consumers to use for
deciding what to eat (although the links have fallen victim to linkrot),
copy-paste one of their papers into the book, and finish with those
kinds of general closing statements that are as so often too vacuous to
be interesting.</p>
<p>At its best, the book is just a delicious heap of information about
animal agriculture praxis. From detailed lists of surgeries performed
on animals without anaesthetics (dehorning, beak trimming, castration,
teeth clipping and tail docking) to the behaviour of cows in big pens
(they huddle together in a corner, and don't use up all the space)
to the hierarchical behavior in chickens (much more strongly than in
cows, actually a major factor of injury in cage-free egg production),
the book presents an industrial-scale mountain of interesting facts
about animal agriculture. The best parts of the book pretty much scream
to be flashcardized.</p>
<p>Norwood's &amp; Lusk's judgement seems well informed and not particularly
strongly clouded by bias, and presented in a empathetic, but also
neutral tone (except in the case of them mentioning in a side remark
that surgeries such as castration on animals are nearly always performed
without anaesthetics, seemingly regarding this as completely acceptable).</p>
<p>However, not everything is golden under the sun. The chapter on
philosophy is especially painful (or might this just be my Gell-Mann
amnesia speaking?) – they seem dismissive of philosophers' arguments,
present them in short and watered-down form, and even state in a footnote:</p>
<blockquote>
<p>If there is one thing we have learned from reading the
works of ethical philosophers; it is that no one ever, ever wins
the debate</p>
</blockquote>
<p><em>— F. Bailey Norwood/Jayson L. Lusk, “Compassion, by the Pound” p. 388, 2011</em></p>
<p>The chapter on Talking with Economists is better, but plagued with
the eternal problem of economics: people don't like it, and the same
debate about the very basic tenets of economics needs to be rehashed
over and over again. As it happens here, much to my own disappointment
(“Yes, sure, I agree that things have a price, that regulation is often
nonsensical and consumers change their minds when presented with the same
scenario, worded slightly differently. Can we get back to fascinating
in-depth descriptions of animal agriculture now, please?”).</p>
<p>Chapter 9, Consumer Expressions, is not <em>bad</em> per se, but still sloppy: It
is abundantly clear that the chapter is simply a paper by the two authors
copy-pasted into the book. The experiments they perform are interesting
and scientifically sophisticated, but the chapter is nonetheless jarring
to the reader – clearly somewhat out of place in the rest of the book.</p>
<p>Two things stand out to me from this book:</p>
<ol>
<li>They mention Brian Tomasik's early writings on wild-animal suffering in
a very positive tone, remarking that "It is one of the most interesting
and well researched narratives that is not officially published by any
organization."</li>
<li>After reading it, I remain mostly unshaken in my
vegetarianism. However, I have stopped eating eggs as a result of reading
this book, and I now assign a much higher probability to the hypothesis
that beef cows' lives on factory farms are actually net positive, although
I wouldn't go so far as to give it the majority of my probability mass.</li>
</ol>
<p>“Compassion, by the Pound” is sometimes clearly a product of annoyance
– an annoyance at animal advocates who allegedly spread misinformation
about farming practices, annoyance at people who <em>just don't understand
economics</em> (which I get, yes, it's frustrating), and yes, sometimes
also annoyance at the horrifying conditions many farm animals have to
live under. Hopefully both economists and animal advocates won't have
to be annoyed as much in the future, but for the time being, we're still
killing and eating animals.</p>
<p><strong>6/10</strong></p>
<h3 id="The_Human_Predicament_David_Benatar_2017"><a class="hanchor" href="#The_Human_Predicament_David_Benatar_2017">The Human Predicament (David Benatar, 2017)</a></h3>
<p>“The Human Predicament” is a book about life philosophy, written
by the pessimistic analytic philosopher David Benatar. In it, Benatar
describes what he calls the human predicament (hence the title), which
consists of the fact that human lives are usually bad, and much worse
than people themselves think. In his view, human lives lack cosmic (and
sometimes terrestrial) meaning, are bad because they're much shorter
than they could be, much more filled with pain and discomfort than
humans think, and full of ignorance, unfulfilled desires and physical
deterioration during the course of one's lifetime.</p>
<p>However, according to Benatar, all alternatives are also bad: death,
because it often deprives of life, and annihilates the person dying;
and suicide, for much the same reasons, unless it annihilates a life
that is awful enough to justify death. Life extension, under Benatar's
view, is extremely unlikely, and even if achieved, would only prolong
the misery of human existence.</p>
<p>The only positive option is to not come into existence at all–or at
least not make others come into existence, even though one desires to.
He alludes several times to one of his other books, Better Never To Have
Been, in which he advocates for antinatalism.</p>
<p>Reading this book felt a little
bit pointless to me. Since <a href="https://www.gwern.net/Research-criticism#beliefs-are-for-actions">beliefs are for
actions</a>,
and Benatar is just applying a linear transformation to all
available options (if everything's bad, nothing is), you
act exactly the same. Although I had a phase where I believed
antinatalism quite strongly, and still don't plan on having kids
(although I know that this attitude might change with increasing
age), but overall antinatalism does not strike me as a <a href="https://reducing-suffering.org/strategic-considerations-moral-antinatalists/" title="Strategic Considerations for Moral Antinatalists">pragmatic
policy</a>,
me instead adopting an
<a href="https://qualiacomputing.com/2017/12/20/the-universal-plot-part-i-consciousness-vs-pure-replicators/" title="The Universal Plot: Part I – Consciousness vs. Pure Replicators">anti-pure-replicator</a>
strategy.</p>
<p>Especially the chapter on meaning felt irrelevant: I don't have an
internal experience of meaning (or the lack thereof), and oscillate
between believing it to be a subtype of high-valence qualia and believing
it to be a mechanism for the mind to do things that are in themselves
not enjoyable (a "second reward signal" next to pleasure).</p>
<p>Benatar mentions cryonics, life extension technology and
transhumanism in general, and while his treatment of these topics
is more respectful than most, he dismisses them fairly quickly. I
disagree with his underlying skepticism on these the feasibility of radically altering the human condition through technology, given that it seems that humanity
can expect to find itself in a period of <a href="https://sideways-view.com/2017/10/04/hyperbolic-growth/" title="Hyperbolic growth">hyperbolic economic
growth</a>
(see also <a href="https://www.openphilanthropy.org/blog/modeling-human-trajectory" title="Modeling the Human Trajectory">Roodman
2020</a>).</p>
<p>I am also not a fan of the pessimism-optimism distinction. Benatar himself
touches on this:</p>
<blockquote>
<p>that a view is pessimistic should, in itself, neither
count in its favor nor against it. (The same, of course, is true
of an optimistic view.)</p>
</blockquote>
<p><em>— <a href="https://en.wikipedia.org/wiki/David_Benatar">David Benatar</a>, “The Human Predicament” p. 225, 2017</em></p>
<p>It seems to me that humans can believe very bad things to be the case and
still be happier than most other humans in their lives (I know this is
at least true for one human, myself). This, combined with the fact that
Benatar simply shifts the utility function downwards, makes me inclined
to rejecting much of his worldview as simply a matter of emotional tone
on the same facts everyone else also believes.</p>
<p>Finally, I want to accuse Benatar of insufficient pessimism (on
his own criteria): The most likely outcome for humanity (and for
life in general) seems not to be total extinction, but instead a
universe filled with beings most capable of copying themselves, the
whole cosmos teeming with <a href="https://slatestarcodex.com/2014/07/13/growing-children-for-bostroms-disneyland/" title="Growing Children For Bostrom’s Disneyland">subsistence-level beings with very boring
conscious experiences</a> until the stars go out. (Or even worse
scenarios from anti-aligned artificial intelligences, see <a href="https://reducing-suffering.org/near-miss/" title="Astronomical suffering from slightly misaligned artificial intelligence">Tomasik
2019</a>).</p>
<p>Overall, the book had some interesting points about suicide, the quality of
life and meaning, but felt rather pointless.</p>
<p><strong>3/10</strong></p>
<h3 id="Right_Concentration_Leigh_Brasington_2015"><a class="hanchor" href="#Right_Concentration_Leigh_Brasington_2015">Right Concentration (Leigh Brasington, 2015)</a></h3>
<p>Illustrates the theory-practice gap, but in the other direction:
excellently practical first half (which helped me get into the first
jhāna (briefly) during a long retreat (the hard part is getting
the access concentration good enough, which the book doesn't spend
enough time on, in my opinion—only a short appendix (at least there's
recommendations for other books)). The anecdotes from his students and
their problems with entering the jhānas are fascinating (pīti that
doesn't go away? jhānas contraindicated with seizures?), as are his
reports of deep concentration states on long retreats (the visual field
turning white in the fourth jhāna, and reports about the the nimitta,
make me wonder what goes on in the visual cortex during absorption
meditation).</p>
<p>But Brasington just <em>wants</em> to believe that the Suttas are basically
infallible, <strong>especially</strong> when they report what the Buddha said
(Brasington has remarked on podcasts that we know that the Buddha knew
what he was talking about, which I don't get—even if he was a great
meditator and thinker, he could just have been <em>wrong</em> sometimes):
Expecting the Suttas to accurately and coherently reflect reality
in all its aspects is a bit too optimistic for me. But Brasington
goes full memetic immune disorder on the Suttas, and the result is
just…uninteresting?</p>
<p><strong>6.5/10</strong></p>
<h3 id="What_We_Owe_The_Future_William_MacAskill_2022"><a class="hanchor" href="#What_We_Owe_The_Future_William_MacAskill_2022">What We Owe The Future (William MacAskill, 2022)</a></h3>
<p>Preceded by a <a href="https://www.goodreads.com/book/show/50485582-the-precipice">superior book with the same
topic</a>;
this one is sleeker, less filled with random interesting facts, less
scientific, less exuberant in its prose. I enjoyed the introduction of the
<a href="https://forum.effectivealtruism.org/topics/spc-framework">SPC framework</a>
(though that might be also a flaw, unlike with
<a href="https://forum.effectivealtruism.org/topics/itn-framework">ITN</a> I haven't
even seen anyone else pay lip service to SPC…), found the alleged
first popular introduction to population axiology cute, and liked the
chapters on stagnation.</p>
<p>But honestly? I enjoyed the research that led to those chapters
more than the chapters in the book themselves (especially <a href="https://forum.effectivealtruism.org/s/HSA8wsaYiqdt4ouNF/p/pMsnCieusmYqGW26W">Rodriguez
2019</a>
and <a href="https://forum.effectivealtruism.org/posts/GsjmufaebreiaivF7/what-is-the-likelihood-that-civilizational-collapse-would">Rodriguez
2020</a>),
and I think the team that made The Precipice would've done a nicer job
at exposition.</p>
<p>Similarly, I was not a huge fan of the chapter on risks from artificial
intelligence. Too conservative, which might've been warranted before
GPT-3, but mid-2022? Bad timing to be all "could be soon or bad, or both,
or not, idk". (Although apparently other reviewers have the opposite
issue, so perhaps a good compromise was struck).</p>
<p>I am unsure about the value lock-in frame. On the one hand, it's a very
rough description of some of the danger with AI x-risk, but not all
danger fits in that format: What if AI systems don't lock in any specific
value, but kill off humanity and then go on to explore the space of all
possible value? This framing also invites endless bickering about "who
gets to control the AIs values" and "democracy" and "social solutions",
and the <em>completely separate</em> issue of stable totalitarianism.</p>
<p>Finally: Who <em>the hell</em> decided this was a good way to do endnotes? In
general the best policy is to <a href="https://entirelyuseless.com/2015/07/11/are-hyperlinks-a-bad-idea/">under no circumstances use endnotes, ever,
<strong>why</strong></a>.
But WWOTF makes it 10x worse: I usually read endnotes,
because I'm unusually curious and bad at priorization,
but WWOTF only has ~25% substantive endnotes, with the rest
being just incomplete references (which can be accessed on <a href="https://whatweowethefuture.com/bibliography/">the
website</a>)—so I found
myself flipping back and forth, only to be disappointed most of the
time. Surely there must be a better way of distinguishing between
citations and endnotes.</p>
<p>Maybe I should've avoided it: Pop philosophy that is already in my
own groundwater.</p>
<p>If you're reading this site, read The Precipice instead. (Not a <em>full</em>
condemnation of WWOTF).</p>
<p><strong>5.5/10</strong></p>
<h3 id="Attention_Span_Gloria_Mark_2023"><a class="hanchor" href="#Attention_Span_Gloria_Mark_2023">Attention Span (Gloria Mark, 2023)</a></h3>
<blockquote>
<p>Curiosity is the drug of the internet.</p>
</blockquote>
<p><em>—Gloria Mark, “Attention Span” p. 114, 2023</em></p>
<p>Read this while <a href="./spans.html">researching attention spans</a>, I
did not find what I was looking for (remaining mostly unconvinced
that the reported statistics are strong enough to justify the
claim that attention spans have been declining). Otherwise
acceptable; and in some parts genuinely novel to me, giving a
plethora of ways of measuring attention (<a href="https://en.wikipedia.org/wiki/Transcranial_Doppler">transcranial Doppler
sonography</a>,
<a href="https://en.wikipedia.org/wiki/Functional_near-infrared_spectroscopy">functional near-infrared
spectroscopy</a>,
facial <a href="https://en.wikipedia.org/wiki/Thermography">thermography</a>
to measure cognitive effort, <a href="https://en.wikipedia.org/wiki/Hemodynamics#Velocity">blood
velocity</a>…),
claims that the Pomodoro technique hasn't been experimentally tested
(big if true!). Apparently people often <em>self-interrupt</em> while on a task,
which I've noticed myself doing more &amp; more. The Big 5 relate to how
humans perform tasks:</p>
<blockquote>
<p>Those who score high in Neuroticism in personality tests also tend
to perform worse on selective attention tasks where they have to pay
attention to some things and ignore distracting stimuli,²⁰ much like
the Stroop task.</p>
</blockquote>
<p><em>—Gloria Mark, “Attention Span” p. 154, 2023</em></p>
<blockquote>
<p>We expected that conscientious people would be more likely to be
continuous email checkers, and that is exactly what we found. In fact,
it explained their email checking behavior to a striking extent […]
we found that people who score higher on the personality trait
of Openness perform better in environments with interruptions.</p>
</blockquote>
<p><em>—Gloria Mark, “Attention Span” p. 156, 2023</em></p>
<p>This leads to conscientious people being more exhausted if possible
low-effort interruptions are taken away from them, they just work
continuously until exhaustion.</p>
<p>Mark's background in art gives some entertaining anecdotes and
statistics, I especially enjoyed learning about <a href="https://en.wikipedia.org/wiki/Soviet_montage_theory">dialectical
montage</a> and
decreasing shot-lengths in movies, series and advertisements.</p>
<p>Apparently people want to use this as a self-help book‽ Bizarre.</p>
<p>Satisfactory.</p>
<p><strong>6.5/10</strong></p>
<h3 id="Human_Compatible_Stuart_Russell_2019"><a class="hanchor" href="#Human_Compatible_Stuart_Russell_2019">Human Compatible (Stuart Russell, 2019)</a></h3>
<blockquote>
<p>It sounds odd to say that happiness should be an engineering discipline,
but that seems to be the inevitable conclusion.</p>
</blockquote>
<p><em>—Stuart Russell, “Human Compatible” p. 123, 2019</em></p>
<p><a href="#What_We_Owe_The_Future_William_MacAskill_2022">Another</a> book with an
orange cover, and another popularization of a thing I spend a lot of
time thinking and reading about. But I like this one much more!</p>
<p>Thoroughly enjoyed the many tidbits from AI history, and the stories
about semi-successful systems, as well as a preference-utilitarian
definition of "sadism, envy, resentment and malice", a naive approach
to meta-reasoning ("just reason about a thing if the expected value of
reasoning is positive", without talking about the obvious boots-trapping
problems…but still), learning about the Baldwin effect and the quotes
about risks from artificial intelligence from Butler's Erewhon.</p>
<p>Skeptical about transformative AI soon, and about the <a href="https://www.gwern.net/Scaling-hypothesis">scaling
hypothesis</a>, but probably for
reasons I can't understand. Also this was written before GPT-3, so he
might've changed his mind since then.</p>
<p>The book <em>does</em> assume that <a href="https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX/p/pdaGN6pQyQarFHXF4">reward is the optimization
target</a>,
and doesn't mention <a href="https://www.lesswrong.com/posts/FkgsxrGf3QxhfLWHG">inner
optimizers</a>, but your
popularization of alignment can only do so much. I should really read
into the whole CIRL/corrigibility debate, any day now.</p>
<p>The book <em>did</em> have endnotes, which I hate, but less so than with <a href="#What_We_Owe_The_Future_William_MacAskill_2022">What We
Owe The Future</a>—perhaps
because I got to read the titles of the papers and not just a naked "Foo
et al. 2010", perhaps because there was just more content per footnote.</p>
<blockquote>
<p>The task is, fortunately, not the following: given a machine that
possesses a high degree of intelligence, work out how to contol it. If
that were the task, we would be toast. A machine viewed as a black box,
a <em>fait accompli</em>, might as well have arrived from outer space. And
our chances of controlling a superintelligent entity from outer space
are roughly zero. Similar arguments apply to the methods of creating AI
systems that guarantee we won't understand how they work; these methods
include <em>whole-brain emulation</em>¹—creating souped-up electronic copies
of human brains—as well as methods based on simulated evolutions of
programs.² I won't say more about these proposals because they are so
obviously a bad idea.</p>
</blockquote>
<p><em>—Stuart Russell, “Human Compatible” p. 171, 2019</em></p>
<p><strong>7.5/10</strong></p>
<h2 id="LessWrong_Annual_Reviews"><a class="hanchor" href="#LessWrong_Annual_Reviews">LessWrong Annual Reviews</a></h2>
<h3 id="2019"><a class="hanchor" href="#2019">2019</a></h3>
<p>These were written for the <a href="https://www.lesswrong.com/posts/QFBEjjAvT6KbaA3dY/the-lesswrong-2019-review">2019 LessWrong Review</a>.</p>
<h4 id="What_failure_looks_like_Paul_Christiano_2019"><a class="hanchor" href="#What_failure_looks_like_Paul_Christiano_2019">What failure looks like (Paul Christiano, 2019)</a></h4>
<p><a href="https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like">Original Post</a>.</p>
<p>I read <a href="https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like">this
post</a>
only half a year ago after seeing it being referenced in several different
places, mostly as a newer, better alternative to the existing FOOM-type
failure scenarios. I also didn't follow the comments on this post when
it came out.</p>
<p>This post makes a lot of sense in Christiano's worldview, where we have a
relatively continuous, somewhat multipolar takeoff which to a large extent
inherits the problem in our current world. This is especially applies to
part I: we already have many different instances of scenarios where humans
follow measured incentives and produce unintended outcomes. <a href="https://en.wikipedia.org/wiki/Goodhart%27s_law">Goodhart's
law</a> is a thing. Part
I ties in especially well with Wei Dai's concern that</p>
<blockquote>
<p>AI-powered memetic warfare makes all humans effectively insane.</p>
</blockquote>
<p>While I haven't done research on this, I have a medium
strength intuition that this is already happening. Many
people I know are at least somewhat addicted to the internet,
having lost a lot of attention due to having their motivational
system hijacked, which is worrying because <a href="https://www.lesswrong.com/posts/aDtzAZf3LnwYvmBP7/attention-is-your-scarcest-resource">Attention is your scarcest
resource</a>.
I believe investigating the amount to which attention has
deteriorated (or has been monopolized by different actors)
would be valuable, as well as thinking about which incentives
will start when AI technologies become more powerful (<a href="https://www.lesswrong.com/users/daniel-kokotajlo">Daniel
Kokotajlo</a> has been
writing especially interesting essays on this kind of problem).</p>
<p>As for part II, I'm a bit more skeptical. I would summarize "going
out with a bang" as a "collective treacherous turn", which would
demand somewhat high levels of coordination between agents of various
different levels of intelligence (agents would be incentivized to turn
early because of first-mover-advantages, but this would increase the
probability of humans doing something about it), as well as agents
knowing very early that they want to perform a treacherous turn to
influence-seeking behavior. I'd like to think about how the frequency of
premature treacherous turns relates to the intelligence of agents. Would
that be continuous or discontinuous? Unrelated to Christiano's post,
this seems like an important consideration (maybe work has gone into
this and I just haven't seen it yet).</p>
<p>Still, part II holds up pretty well, especially since we can expect AI
systems to cooperate effectively via merging utility functions, and we
can see systems in the real world that fail regularly, but not much is
being done about them (especially social structures that sort-of work).</p>
<p>I have referenced this post numerous times, mostly in connection with a
short explanation of how I think current attention-grabbing systems are
a variant of what is described in part I. I think it's pretty good, and
someone (not me) should flesh the idea out a bit more, perhaps connecting
it to existing systems (I remember the story about the recommender system
manipulating its users into political extremism to increase viewing time,
but I can't find a link right now).</p>
<p>The one thing I would like to see improved is at least some links to
prior existing work. Christiano writes that</p>
<blockquote>
<p>(None of the concerns in this post are novel.)</p>
</blockquote>
<p>but it isn't clear whether he is just summarizing things he has thought
about, which are implicit knowledge in his social web, or whether he is
summarizing existing texts. I think part I would have benefitted from a
link to Goodhart's law (or an explanation why it is something different).</p>
<h4 id="1960_The_Year_The_Singularity_Was_Cancelled_Scott_Alexander_2019"><a class="hanchor" href="#1960_The_Year_The_Singularity_Was_Cancelled_Scott_Alexander_2019">1960: The Year The Singularity Was Cancelled (Scott Alexander, 2019)</a></h4>
<p><a href="https://www.lesswrong.com/posts/bYrF8rXFYwPqnfxTp/1960-the-year-the-singularity-was-cancelled">Original Post</a>.</p>
<p>I believe this is an important
<a href="https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding">gears-level</a>
addition to posts like <a href="https://sideways-view.com/2017/10/04/hyperbolic-growth/">hyperbolic
growth</a>,
<a href="http://mason.gmu.edu/~rhanson/longgrow.html">long-term growth as a sequence of exponential
modes</a> and an old Yudkowsky
post I am unable to find at the moment.<!--TODO: capitalization of Yudkowsky is wrong still on LW--></p>
<p>I don't know how closely these
texts are connected, but <a href="https://www.openphilanthropy.org/blog/modeling-human-trajectory">Modeling the Human
Trajectory</a>
picks up one year later, creating two technical models: one
stochastically fitting and extrapolating GDP growth; the other providing
a deterministic outlook, considering labor, capital, human capital,
technology and production (and, in one case, natural resources). Roodman
arrives at somewhat similar conclusions, too: The industrial revolution
was a <em>very</em> big deal, and something happened around 1960 that has
slowed the previous strong growth (as far as I remember, it doesn't
provide an explicit reason for this).</p>
<p>A point in this post that I found especially interesting
was the speculation about the back plague being the spark
that ignited the industrial revolution. The reason given is
a good example of <a href="https://www.lesswrong.com/posts/GZSzMqr8hAB2dR8pk/studies-on-slack">slack catapulting a system out of a local
maximum</a>,
in this case a malthusian europe into the industrial revolution.</p>
<p>Interestingly, both this text and Roodman don't consider individual
intelligence as an important factor in global productivity. Despite the
well-known <a href="https://en.wikipedia.org/wiki/Flynn_effect">Flynn-Effect</a>
that has mostly continued since 1960 (caveat caveat), no extraordinary
change in global productivity has occurred. This makes some sense: a
rise of less than 1 standard deviation might be appreciable, but not
groundbreaking. But the relation to artificial intelligence makes it
interesting: the purported (economic) advantage of AI systems is that
they can copy themselves, thereby making population growth not the
most constraining variable in this growth model. I don't believe this
is particularly anticipation-constraining, though: this could mean that
either the post-singularity ("singularity") world is multipolar, or the
singleton controlling everything has created many sub-agents.</p>
<p>I appreciate this post. I have referenced it a couple of
times in conversations. Together with the investigation
by OpenPhil it makes a solid case that <a href="https://slatestarcodex.com/2018/11/26/is-science-slowing-down-2/">the gods of straight
lines</a>
have decided to throw us into <a href="https://forum.effectivealtruism.org/posts/XXLf6FmWujkxna3E6/are-we-living-at-the-most-influential-time-in-history-1">the most important century of
history</a>.
May the <a href="https://www.lesswrong.com/posts/MFNJ7kQttCuCXHp8P/the-goddess-of-everything-else">godess of everything
else</a>
be merciful with us.</p>
<h3 id="2020"><a class="hanchor" href="#2020">2020</a></h3>
<p>These were written for the <a href="https://www.lesswrong.com/posts/M9kDqF2fn3WH44nrv/the-2020-review-preliminary-voting">2020 LessWrong
Review</a>.</p>
<h4 id="AntiAging_State_of_the_Art_JackH_2020"><a class="hanchor" href="#AntiAging_State_of_the_Art_JackH_2020">Anti-Aging: State of the Art (JackH, 2020)</a></h4>
<p><a href="https://www.lesswrong.com/posts/RcifQCKkRc9XTjxC2/anti-aging-state-of-the-art">Original post</a>.</p>
<p>I read this post at the same time as reading <a href="https://forum.effectivealtruism.org/s/8QHbqGpXnzcQxiAis">Ascani
2019</a>
and <a href="https://nintil.com/longevity">Ricón 2021</a> in an
attempt to get clear about anti-aging research. Comparing
these three texts against each other, I would classify <a href="https://forum.effectivealtruism.org/s/8QHbqGpXnzcQxiAis">Ascani
2019</a> as
trying to figure out whether focusing on anti-aging research is a
good idea, <a href="https://nintil.com/longevity">Ricón 2021</a> trying to give
a gearsy overview of the field (objective unlocked: get Nintil posts
cross-posted to LessWrong), and this text as showing what has already
been accomplished.</p>
<p>In that regard it succeeds perfectly well: The structure of Part V is
so clean I suspect that it sweeps a bunch of complexity and alternative
approaches under the rug, and the results described seriously impressed
me and some of the people I was reading this text with at the time (We
can <em>reverse arthritis and cataracts in mice</em>‽ We can <em>double their
maximum lifespan</em>‽). It is excellent science propaganda: Inspiring
awe at what has been accomplished, desire to accomplish more, and
hope that this is possible.</p>
<p>While the post shines in parts III, IV and V, I have some
quibbles and complaints about the introduction, part I
and part II. First, I disliked the jab against cryonics
in the first paragraph <a href="https://niplav.github.io/considerations_on_cryonics.html">without considering the costs and
benefits</a>,
which rightly received some pushback in the comments (the strongest
counter-observation being that barring <a href="https://www.gwern.net/Longevity">some practical suggestions
for slowing down aging right now</a>,
cryonics and anti-aging research occupy very different parts of the
strategy for life-extension, and can be pursued in parallel). Part II
disappointed me because it was pro-longevity advocacy under the veneer
of a factual question: Has anybody actually tried to think through how a
world without aging might <em>actually look like</em>, instead of re-treading the
same pro-aging trance and anti-aging science arguments? That seems like
a question that is both interesting and pretty relevant, even when you
believe that ending aging is important enough that it should definitely
be done, if just to prepare for weird second- and third-order effects.</p>
<p>(Part I felt like I was a choir being preached to, which isn't <em>that bad</em>,
but still…)</p>
<p>I really liked learning a bunch of new facts about aging (as for example
the list of species that don't age, that aging is responsible for 30%
of lost DALYs, and distinction between gerontology, engineering and
geriatrics). Factposts are underrated.</p>
<p>The comments on this post were often very good, and had some nice
discussion about whether the advice in section VII was to be focused on.</p>
<p>I've been overly negative in this review, but overall I still like this
post, and have voted a 4 on it (which I might change to a 1). The parts
III-V are excellent, and I have only minor problems with the preceding
parts. This is the kind of post I would give a science-interested
skeptic of anti-aging research. I'd like to have this post in the review,
because it represents something some part of the core to the LessWrong
transhumanist aesthetic that often gets overlooked.</p>
<h4 id="Persuasion_Tools_AI_takeover_without_AGI_or_agency_Daniel_Kokotajlo_2020"><a class="hanchor" href="#Persuasion_Tools_AI_takeover_without_AGI_or_agency_Daniel_Kokotajlo_2020">Persuasion Tools: AI takeover without AGI or agency? (Daniel Kokotajlo, 2020)</a></h4>
<p><a href="https://www.lesswrong.com/posts/qKvn7rxP2mzJbKfcA/persuasion-tools-ai-takeover-without-agi-or-agency">Original post</a>.</p>
<p>The problem outlined in this post results from two major concerns on
lesswrong: risks from advanced AI systems and irrationality due to
parasitic memes.</p>
<p>It presents the problem of persuasion tools as continuous with the
problems humanity has had with virulent ideologies and sticky memes,
exacerbated by the increasing capability of narrowly intelligent machine
learning systems to exploit biases in human thought. It provides (but
doesn't explore) two examples from history to support its hypothesis:
the printing press as a partial cause of the 30 years war, and the radio
as a partial cause of 20th century totalitarianism.</p>
<p>Especially those two concerns reminded me of <a href="https://www.lesswrong.com/posts/YicoiQurNBxSp7a65">Is Clickbait
Destroying Our General Intelligence? (Eliezer Yudkowsky,
2018)</a>, which could
be situated in this series of events:</p>
<blockquote>
<p>I suspect some culturally transmitted parts of the general intelligence
software got damaged by radio, television, and the Internet, with a key
causal step being an increased hypercompetition of ideas compared to
earlier years.</p>
</blockquote>
<p>Kokotajlo also briefly considers the hypothesis that
epistemic conditions might have become better through the
internet, but rejects it (for reasons that are not spelled
out, but the answers to <a href="https://www.lesswrong.com/posts/PjfsbKrK5MnJDDoFr/have-epistemic-conditions-always-been-this-bad">Have epistemic conditions always been this
bad? (Wei Dai, 2021)</a>
might be illuminating). (Survivorship bias probably plays a large
role here: epistemically unsound information is less likely to survive
long-term trials for truth, especially in an environment where memes on
the less truth-oriented side of the spectrum are in a harsher competition
than memes on the more truth-oriented side).</p>
<p>This post was written a year ago, and didn't make any concrete
predictions (for a vignette of the future by the author,
see <a href="https://www.lesswrong.com/posts/6Xgy6CAf2jqHhynHL/what-2026-looks-like-daniel-s-median-future">What 2026 looks like (Daniel's Median Future) (Daniel Kokotajlo,
2021)</a>).
My personal implied predictions under this worldview are something
like this:</p>
<ol>
<li>A large number (&gt;100 mio.) of people in the western world (USA &amp; EU) will interact with chatbots on a regular basis (e.g. more than once a week).
<ul>
<li>I think this isn't yet the case: I've encountered chatbots mainly in the context of customer service, and don't know anyone personally who has used a chatbot for entertainment for more than an afternoon (Western Europe). If we count automated personal assistants such as Alexa or Siri, this might be true.</li>
</ul></li>
<li>It is revealed that a government spent a substantial amount of money (&gt;$1B) on automating propaganda creation.
<ul>
<li>As far as I know, there hasn't been any reveal of such a large-scale automated propaganda campaign (the wikipedia pages on <a href="https://en.wikipedia.org/wiki/Propaganda_in_China">Propaganda in China</a> and <a href="https://en.wikipedia.org/wiki/Propaganda_in_the_US">in the US</a> mention no such operations.</li>
</ul></li>
<li>Online ideological conflicts spill over into the real world more often.
<ul>
<li>As I haven't been following the news closely, I don't have many examples here, but the <a href="https://en.wikipedia.org/wiki/2020-21_United_States_election_protests">2020–21 United States election protests</a> come to mind.</li>
</ul></li>
<li>The internet becomes more fractured, into discrete walled gardens (e.g. into a Chinese Internet, <a href="https://slatestarcodex.com/2014/09/30/i-can-tolerate-anything-except-the-outgroup/">US Blue Internet, and US Red Internet</a>).
<ul>
<li>This seems to become more and more true, with sites such as Gab and the Fediverse gaining in popularity. However, it doesn't seem like the US Red Internet has the technological capabilities to automate propaganda or build a complete walled garden, to the extent that the US Blue Internet or the Chinese internet do.</li>
</ul></li>
</ol>
<p>I found the text quite relevant both to thinking about possible alternative
stories about the way in which AI could go wrong, and also to my
personal life.</p>
<p>In the domain of AI safety, I became more convinced of the importance
of aligning recommender systems to human values (also mentioned in the
post), if they pose larger risk than commonly assumed, and provide
a good ground for experimentation on alignment techniques. Whether
<a href="https://arxiv.org/abs/2107.10939v1">aligning recommender systems</a> is more
important than aligning large language models seems like an important crux
here: Are the short-term/long-term risks higher for recommender systems
(i.e. reinforcement learners) larger than the risks from large language
models? Which route appears more fruitful when trying to align more
generally capable systems? As far as I can see, the alignment community
is more interested in attempts to align large language models, compared
to recommender systems, probably due to recent progress in that area
and because it's easier to test alignment in language models (?).</p>
<p>The scenarios in which AI powered memetic warfare significantly harm
humanity can also be tied into research on the malicious use of AI,
e.g. <a href="https://www.eff.org/files/2018/02/20/malicious_ai_report_final.pdf">The Malicious Use of Artificial Intelligence: Forecasting,
Prevention, and Mitigation (Brundage et al. 2018)</a>. Policy tools from
diplomacy with regard to biological, chemical and nuclear warfare could
be applied to memetic and psychologcial warfare.</p>
<p>The text explicitely positions the dangers of persuasion tools as a risk
factor, but more speculatively, they might also pose an existential risk
in themselves, in two different scenarios:</p>
<ul>
<li>If humans are very easy to manipulate by AI systems that are narrowly superhuman in the domain of human psychology, a scenario similar to <a href="https://www.lesswrong.com/rationality/evolving-to-extinction">Evolving to Extinction (Eliezer Yudkowsky, 2007)</a> might occur: nearly everybody goes <a href="https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like?commentId=CB8ieALcHfSSuAYYJ#CB8ieALcHfSSuAYYJ">effectively insane</a> at approximately the same time, resulting in the collapse of civilization.</li>
<li>Humans might become insane enough that further progress along relevant axes is halted, but not insane enough that civilization collapses. We get stuck oscillating around some technological level, until another existential catastrophe like nuclear war and resulting nuclear winter finishes us off.</li>
</ul>
<p>On the personal side, after being fooled by <a href="https://nitter.net/lxrjl/status/1376830454075129858">people using GPT-3 to
generate tweets</a>
and seeing at least one instance of observing
someone <a href="https://old.reddit.com/r/TheMotte/comments/qzfptl/culture_war_roundup_for_the_week_of_november_22/hmhnkzg/">asking a commenter for the MD5 hashsum of a
string</a>
to verify that the commenter was human (and the commenter failing that
challenge), along with observing the increasingly negative effects of
internet usage on my attention span, I decided to separate my place for
sleeping &amp; eating from the place where I use internet, with a ~10 minute
commute between those two. I also decided to pay less attention to news
stories/reddit/twitter, especially from sources affiliated with large
governments, downloaded my favourite websites.</p>
<p>This post was relevant to my thoughts about alternative AI risk scenarios
as well as drastic personal decisions, and I expect to give it a 1 or
(more likely) a 4 in the final vote.</p>
</body></html>
